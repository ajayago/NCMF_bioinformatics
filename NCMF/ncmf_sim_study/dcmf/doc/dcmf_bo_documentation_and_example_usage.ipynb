{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dCMF + Bayesian Optimization(BO) based hyperparameter search\n",
    "Example of running the \"dcmf_bo\" module with a mix of BO searched and user provided (hyper)parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import time\n",
    "import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dcmf_bo import dcmf_bo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the sample dataset\n",
    "\n",
    "This directory contains a sample synthetic dataset generated for the augmented setting of Fig 1(c) in the [paper](https://arxiv.org/abs/1811.11427).\n",
    "You can download the sample data from [here](https://drive.google.com/open?id=1EFF_kuOIg2aYyOGZY_peX3NziqCSxxP1) and unzip it to the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/sample_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from data_dir:  ../data/sample_data/\n"
     ]
    }
   ],
   "source": [
    "#Loads the dataset into a dict\n",
    "#Note: This dataset contains 5-folds for the matrix X_12 (matrix R below)\n",
    "num_folds = 1\n",
    "#\n",
    "pp = pprint.PrettyPrinter()\n",
    "print(\"Loading data from data_dir: \",data_dir)\n",
    "U1 = pkl.load(open(data_dir+\"X_13.pkl\",'rb'))\n",
    "U2 = pkl.load(open(data_dir+\"X_14.pkl\",'rb'))\n",
    "V1 = pkl.load(open(data_dir+\"X_26.pkl\",'rb'))\n",
    "W1 = pkl.load(open(data_dir+\"X_53.pkl\",'rb'))\n",
    "R_temp_dict = {}\n",
    "for fold_num in np.arange(1,num_folds+1):\n",
    "    Rtrain = pkl.load(open(data_dir+'/X_12_train_fold_'+str(fold_num)+'.pkl','rb'))\n",
    "    Rtrain = Rtrain\n",
    "    Rtrain_idx = pkl.load(open(data_dir+'/X_12_train_idx_'+str(fold_num)+'.pkl','rb')) \n",
    "    Rtest = pkl.load(open(data_dir+'/X_12_test_fold_'+str(fold_num)+'.pkl','rb'))\n",
    "    Rtest_idx = pkl.load(open(data_dir+'/X_12_test_idx_'+str(fold_num)+'.pkl','rb'))\n",
    "    Rdoublets = pkl.load(open(data_dir+'/R_doublets_'+str(fold_num)+'.pkl','rb'))\n",
    "    R_temp_dict[fold_num] = {\"Rtrain\":Rtrain,\"Rtrain_idx\":Rtrain_idx,\"Rtest\":Rtest,\"Rtest_idx\":Rtest_idx,\"Rdoublets\":Rdoublets}\n",
    "#\n",
    "data_dict = {\"U1\":U1,\"U2\":U2,\"V1\":V1,\"W1\":W1,\"R\":R_temp_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U1.shape:  (1000, 20)\n",
      "U2.shape:  (1000, 150)\n",
      "V1.shape:  (2000, 250)\n",
      "W1.shape:  (300, 20)\n",
      "R.shape:  (1000, 2000)\n"
     ]
    }
   ],
   "source": [
    "print(\"U1.shape: \",U1.shape)\n",
    "print(\"U2.shape: \",U2.shape)\n",
    "print(\"V1.shape: \",V1.shape)\n",
    "print(\"W1.shape: \",W1.shape)\n",
    "print(\"R.shape: \",data_dict['R'][1]['Rtrain'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the required data structures\n",
    "\n",
    "Here we construct the data structures required as input to the dcmf API\n",
    "\n",
    "#### *entity matrix relationship graph *\n",
    "\n",
    "- **G**: dict, keys are entity IDs and values are lists of associated matrix IDs\n",
    "\n",
    "#### * training data*\n",
    "- **X_data**: dict, keys are matrix IDs and values are (1) np.array, or (2) dict, (if this matrix is in validation set **X_val**) with validation set IDs as keys & values as np.array\n",
    "- **X_meta**: dict, keys are matrix IDs and values are lists of the 2 associated entity IDs\n",
    "\n",
    "#### *validation data*\n",
    "- **X_val**: dict, keys are IDs of the matrices that are part of validation set and values are dict with validation set IDs as keys and values are (1) scipy.sparse matrix, or (2) list of triplets corresponding to the validation entries (if you would like to perform classification and measure AUC)  \n",
    "**Note**: To perform K folds cross validation, use K validation sets for the corresponsing matrix/matrices. In the example below, we used a single validation set with ID \"1\" for the matrix with ID \"X1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = {\n",
    "    \"e1\":[\"X1\",\"X2\",\"X3\"],\\\n",
    "    \"e2\":[\"X1\",\"X4\"],\\\n",
    "    \"e3\":[\"X2\",\"X5\"],\\\n",
    "    \"e4\":[\"X3\"],\\\n",
    "    \"e5\":[\"X5\"],\\\n",
    "    \"e6\":[\"X4\"]}\n",
    "    #\"e6\":[\"X4\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = {\n",
    "    \"X1\":{\"1\":data_dict['R'][1][\"Rtrain\"]},\\\n",
    "    \"X2\":U1,\\\n",
    "    \"X3\":U2,\\\n",
    "    \"X4\":V1,\\\n",
    "    \"X5\":W1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_meta = {\n",
    "    \"X1\":[\"e1\",\"e2\"],\\\n",
    "    \"X2\":[\"e1\",\"e3\"],\\\n",
    "    \"X3\":[\"e1\",\"e4\"],\\\n",
    "    \"X4\":[\"e2\",\"e6\"],\\\n",
    "    \"X5\":[\"e5\",\"e3\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rtest_triplets = [[1,1,1],[3,3,0],[1,2,0],[0,1,0],[0,2,0],[0,3,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = {\n",
    "    \"X1\":{\"1\":Rtest}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *dCMF network construction - hyperparameters*\n",
    "\n",
    "- **kf**: float, in the range (0,1) \n",
    "- **k**: int, entity representation or encoding size. Refer Appendix A in the [paper](https://arxiv.org/abs/1811.11427) for info about how k and kf are used in the dCMF network construction. \n",
    "- **e_actf**: str, autoencoder's encoding activation function.\n",
    "- **d_actf**: str, autoencoder's decoding activation function. Supported functions are \"tanh\",\"sigma\",\"relu\",\"lrelu\"\n",
    "- **is_linear_last_enc_layer**: bool, True to set linear activation for the bottleneck/encoding generation layer \n",
    "- **is_linear_last_dec_layer**: bool, True to set linear activation for the output/decoding generation layer \n",
    "- **num_chunks**: int, number of training batches to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = 0.5\n",
    "k = 100\n",
    "e_actf = \"tanh\"\n",
    "d_actf = \"tanh\"\n",
    "is_linear_last_enc_layer = False\n",
    "is_linear_last_dec_layer = False\n",
    "num_chunks = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Optimization/training - hyperparamteres*\n",
    "\n",
    "- **learning_rate**: float, Adam optimizer's learning rate\n",
    "- **weight_decay**: float, Adam optimizers's weight decay (L2 penalty)\n",
    "- **max_epochs**: int, maximum number of training epochs at which the training stops \n",
    "- **convg_thres**: float, convergence threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.05\n",
    "max_epochs = 5\n",
    "convg_thres = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Hyperparamteres related to pre-training*\n",
    "\n",
    "- **is_pretrain**: bool, True for pretraining \n",
    "- **pretrain_thres**: bool, pre-training convergence thresholsd\n",
    "- **max_pretrain_epochs**: int, maximum number of pre-training epochs at which the training stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_pretrain=True\n",
    "pretrain_thres= 0.1\n",
    "max_pretrain_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Parameters related to validation*\n",
    "\n",
    "- **val_metric**: str, Validation performance metric. Supported metrics: [\"rmse\",\"r@k\",\"p@k\",\"auc\"]. Where,  \n",
    "     *rmse* - Root [mean square error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)  \n",
    "     *r@k* - Recall@k. Refer section 5.2's sub-section \"Evaluation metric\" in the [paper](https://arxiv.org/abs/1811.11427)      \n",
    "     *p@k* - Probability@k. Refer section 5.3's sub-section \"Evaluation metric\" in the [paper](https://arxiv.org/abs/1811.11427)      \n",
    "     *auc* - [Area under the curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)\n",
    "    \n",
    "- **is_val_transpose**: bool, True if the reconstructed matrix has to be transposed before computing the validation performance\n",
    "- **at_k**: int, the value of k if the **val_metric** is either \"r@k\" or \"p@k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_metric = \"rmse\"\n",
    "is_val_transpose = False\n",
    "at_k = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *GPU - parameters *\n",
    "\n",
    "- **is_gpu**: bool, True if pytorch tensors storage and operations has to be done in GPU\n",
    "- **gpu_ids**: str, Comma separated string of CUDA GPU ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_gpu = False\n",
    "gpu_ids = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *BO hyperparameter search and related parameters*\n",
    "\n",
    "Following are the list of hyperparameters that can be searched through BO.\n",
    "- \"learning_rate\"\n",
    "- \"convg_thres\"\n",
    "- \"weight_decay\"\n",
    "- \"kf\"\n",
    "- \"k\"\n",
    "- \"num_chunks\"\n",
    "- \"pretrain_thres\"\n",
    "\n",
    "To enable BO based search for any of the above parameters set them as '*None*' while initializing dcmf_bo. If not None, then the used provided values will be used. \n",
    "\n",
    "The config file *\"bo_config.py\"* helps you control the domain in which each of the above hyperparameters have to be searched. Below is an excerpt from the file. The *type* can be discrete/continuous/categorical and *domain* has to be defined accordingly. Refer to GPyOpt documentation [here](http://nbviewer.jupyter.org/github/SheffieldML/GPyOpt/blob/master/manual/GPyOpt_mixed_domain.ipynb) for more details. **Note**: *the order of hyperparameters in the config file should not be changed. You may however modify the domain/type as required. Do not remove the specification of an hyperparameter even if you do not want to perform BO search for it*\n",
    "  \n",
    "*{\"name\": \"kf\", \"type\": \"continuous\", \"domain\": (0.1,0.5)}*  \n",
    "  \n",
    "Here are the parameters related to BO \n",
    "- **num_bo_steps**: Number of BO steps to run\n",
    "- **initial_design_size**: Number of BO steps to run with random hyperparameter samples\n",
    "- **best_criterion**: str, criterion for selecting best hyperparameter set from the **num_bo_steps** runs. Supported criteria: [\"loss\",\"val\"], where,  \n",
    "    \"loss\" - select the hyperparameter set which resulted in minimum loss.  \n",
    "    \"val\" - select the hyperparamter set which resulted in maximum validation performance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_criterion = \"loss\" \n",
    "num_bo_steps = 5\n",
    "initial_design_size = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Instantiating the dCMF model...*\n",
    "- Initializes dCMF after validating the input data and the (hyper)parameters\n",
    "- Here we perform BO based search for hyperparamters \"learning_rate\", \"weight_decay\" and \"convg_thres\" and the rest were initialized with the provided values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "dcmf_bo.__init__ - start\n",
      "dCMF + BO:\n",
      "---\n",
      "Input Hyperparameters:-\n",
      "max_epochs:  5\n",
      "isPretrain:  True\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  2\n",
      "k:  100\n",
      "kf:  0.5\n",
      "d_actf:  tanh\n",
      "d_actf:  tanh\n",
      "---\n",
      "Hyperparameters to be set using BO:-\n",
      "learning_rate\n",
      "convg_thres\n",
      "weight_decay\n",
      "---\n",
      "val:-\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  1\n",
      "best_criterion:  loss\n",
      "val_metric (used only if X_val #matrices > 0):  rmse\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  False\n",
      "---\n",
      "Others:-\n",
      "is_gpu:  False\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "---\n",
      "dcmf_bo.__init__ - end\n"
     ]
    }
   ],
   "source": [
    "dcmf_bo_model = dcmf_bo(G, X_data, X_meta,\\\n",
    "            num_chunks=num_chunks,k=k, kf=kf, e_actf=e_actf, d_actf=d_actf,\\\n",
    "            learning_rate=None, weight_decay=None, convg_thres=None, max_epochs=max_epochs,\\\n",
    "            is_gpu=is_gpu,gpu_ids=gpu_ids,is_pretrain=is_pretrain, pretrain_thres=pretrain_thres,\\\n",
    "            max_pretrain_epochs=max_pretrain_epochs,X_val=X_val,val_metric=val_metric,\\\n",
    "            is_val_transpose=is_val_transpose, at_k=at_k, best_criterion=best_criterion,\\\n",
    "            is_linear_last_enc_layer=is_linear_last_enc_layer,is_linear_last_dec_layer=is_linear_last_dec_layer,num_val_sets=num_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Fitting... *\n",
    "- For each step of BO\n",
    "    - Performs the input transformation and network construction\n",
    "    - (Pre-trains and) trains the model to obtain the entity representations\n",
    "    - Reconstruct the input matrices using the entity representations obtained\n",
    "- Refer Algo 1 and Algo 2 in the [paper](https://arxiv.org/abs/1811.11427) for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dcmf_bo.fit - start\n",
      "#BO - max_iter: 2\n",
      "Models printout after each iteration is only available for GP and GP_MCMC models\n",
      "#BO - max_iter: 2\n",
      "#BO - max_time: inf\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "#\n",
      "dCMF:\n",
      "---\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  4.361453979173315e-05\n",
      "weight_decay:  0.25995186097511425\n",
      "convg_thres:  6.968342245572056e-05\n",
      "max_epochs:  5\n",
      "isPretrain:  True\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  2\n",
      "k:  100\n",
      "kf:  0.5\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  False\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  1\n",
      "val_metric (used only if X_val #matrices > 0):  rmse\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  False\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "## fold_num:  1  ##\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  4.361453979173315e-05\n",
      "weight_decay:  0.25995186097511425\n",
      "convg_thres:  6.968342245572056e-05\n",
      "max_epochs:  5\n",
      "isPretrain:  True\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  2\n",
      "k:  100\n",
      "kf:  0.5\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  False\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  1\n",
      "val_metric (used only if X_val #matrices > 0):  rmse\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  False\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "dcmf - model construction - start\n",
      "__input_transformation - start\n",
      "#\n",
      "concatenated-matrix construction...\n",
      "e_id:  e1\n",
      "X_id_list:  ['X1', 'X2', 'X3']\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (1000, 2000)\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (1000, 20)\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (1000, 150)\n",
      "C_dict[e].shape:  torch.Size([1000, 2170])\n",
      "---\n",
      "e_id:  e2\n",
      "X_id_list:  ['X1', 'X4']\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (1000, 2000)\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (2000, 250)\n",
      "C_dict[e].shape:  torch.Size([2000, 1250])\n",
      "---\n",
      "e_id:  e3\n",
      "X_id_list:  ['X2', 'X5']\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (1000, 20)\n",
      "X_id:  X5\n",
      "X[X_id].shape:  (300, 20)\n",
      "C_dict[e].shape:  torch.Size([20, 1300])\n",
      "---\n",
      "e_id:  e4\n",
      "X_id_list:  ['X3']\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (1000, 150)\n",
      "C_dict[e].shape:  torch.Size([150, 1000])\n",
      "---\n",
      "e_id:  e5\n",
      "X_id_list:  ['X5']\n",
      "X_id:  X5\n",
      "X[X_id].shape:  (300, 20)\n",
      "C_dict[e].shape:  torch.Size([300, 20])\n",
      "---\n",
      "e_id:  e6\n",
      "X_id_list:  ['X4']\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (2000, 250)\n",
      "C_dict[e].shape:  torch.Size([250, 2000])\n",
      "---\n",
      "#\n",
      "concatenated-matrix chunking...\n",
      "WARNING: Entity with ID: e5 has minimum feature size 20 in the setting. The encoding length k 100 is larger than minimim entity feature size.\n",
      "#\n",
      "e_id:  e3 , min_num_datapoints:  20 , num_chunks:  2\n",
      "e_id:  e5 , min_features:  20 , k:  100\n",
      "#\n",
      "e_id:  e1  C_dict[e_id].shape:  torch.Size([1000, 2170])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([500, 2170])\n",
      "---\n",
      "e_id:  e2  C_dict[e_id].shape:  torch.Size([2000, 1250])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([1000, 1250])\n",
      "---\n",
      "e_id:  e3  C_dict[e_id].shape:  torch.Size([20, 1300])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([10, 1300])\n",
      "---\n",
      "e_id:  e4  C_dict[e_id].shape:  torch.Size([150, 1000])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([75, 1000])\n",
      "---\n",
      "e_id:  e5  C_dict[e_id].shape:  torch.Size([300, 20])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([150, 20])\n",
      "---\n",
      "e_id:  e6  C_dict[e_id].shape:  torch.Size([250, 2000])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([125, 2000])\n",
      "---\n",
      "#\n",
      "creating pytorch variables of input matrices...\n",
      "#\n",
      "__input_transformation - end\n",
      "__network_construction - start\n",
      "__network_construction - end\n",
      "dcmf - model construction - end\n",
      "#\n",
      "__pretrain - start\n",
      "pretrain epoch:  1  total loss L:  5.77337384223938  Took  1.1  secs.\n",
      "pretrain epoch:  2  total loss L:  5.762561798095703  Took  1.0  secs.\n",
      "**pretrain converged**\n",
      "__pretrain - end\n",
      "#\n",
      "dcmf.fit - start\n",
      "epoch:  1  total loss L:  9.002478122711182  Took  1.1  secs.\n",
      "epoch:  2  total loss L:  7.403848886489868  Took  1.0  secs.\n",
      "epoch:  3  total loss L:  6.2908241748809814  Took  1.0  secs.\n",
      "epoch:  4  total loss L:  5.919672012329102  Took  1.0  secs.\n",
      "epoch:  5  total loss L:  5.8883936405181885  Took  1.0  secs.\n",
      "Computing RMSE.\n",
      "Rpred.shape:  (1000, 2000)\n",
      "Rtest.shape:  (1000, 2000)\n",
      "nz_list_pred.shape:  25000\n",
      "nz_list_test.shape:  25000\n",
      "rmse:  0.10377505267855172\n",
      "---\n",
      "#\n",
      "dcmf.fit - end\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "#\n",
      "dCMF:\n",
      "---\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  7.75906309603412e-05\n",
      "weight_decay:  0.13638972753881876\n",
      "convg_thres:  7.404244553417574e-05\n",
      "max_epochs:  5\n",
      "isPretrain:  True\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  2\n",
      "k:  100\n",
      "kf:  0.5\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  False\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  1\n",
      "val_metric (used only if X_val #matrices > 0):  rmse\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  False\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "## fold_num:  1  ##\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  7.75906309603412e-05\n",
      "weight_decay:  0.13638972753881876\n",
      "convg_thres:  7.404244553417574e-05\n",
      "max_epochs:  5\n",
      "isPretrain:  True\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  2\n",
      "k:  100\n",
      "kf:  0.5\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  False\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  1\n",
      "val_metric (used only if X_val #matrices > 0):  rmse\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  False\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "dcmf - model construction - start\n",
      "__input_transformation - start\n",
      "#\n",
      "concatenated-matrix construction...\n",
      "e_id:  e1\n",
      "X_id_list:  ['X1', 'X2', 'X3']\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (1000, 2000)\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (1000, 20)\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (1000, 150)\n",
      "C_dict[e].shape:  torch.Size([1000, 2170])\n",
      "---\n",
      "e_id:  e2\n",
      "X_id_list:  ['X1', 'X4']\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (1000, 2000)\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (2000, 250)\n",
      "C_dict[e].shape:  torch.Size([2000, 1250])\n",
      "---\n",
      "e_id:  e3\n",
      "X_id_list:  ['X2', 'X5']\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (1000, 20)\n",
      "X_id:  X5\n",
      "X[X_id].shape:  (300, 20)\n",
      "C_dict[e].shape:  torch.Size([20, 1300])\n",
      "---\n",
      "e_id:  e4\n",
      "X_id_list:  ['X3']\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (1000, 150)\n",
      "C_dict[e].shape:  torch.Size([150, 1000])\n",
      "---\n",
      "e_id:  e5\n",
      "X_id_list:  ['X5']\n",
      "X_id:  X5\n",
      "X[X_id].shape:  (300, 20)\n",
      "C_dict[e].shape:  torch.Size([300, 20])\n",
      "---\n",
      "e_id:  e6\n",
      "X_id_list:  ['X4']\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (2000, 250)\n",
      "C_dict[e].shape:  torch.Size([250, 2000])\n",
      "---\n",
      "#\n",
      "concatenated-matrix chunking...\n",
      "WARNING: Entity with ID: e5 has minimum feature size 20 in the setting. The encoding length k 100 is larger than minimim entity feature size.\n",
      "#\n",
      "e_id:  e3 , min_num_datapoints:  20 , num_chunks:  2\n",
      "e_id:  e5 , min_features:  20 , k:  100\n",
      "#\n",
      "e_id:  e1  C_dict[e_id].shape:  torch.Size([1000, 2170])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([500, 2170])\n",
      "---\n",
      "e_id:  e2  C_dict[e_id].shape:  torch.Size([2000, 1250])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([1000, 1250])\n",
      "---\n",
      "e_id:  e3  C_dict[e_id].shape:  torch.Size([20, 1300])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([10, 1300])\n",
      "---\n",
      "e_id:  e4  C_dict[e_id].shape:  torch.Size([150, 1000])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([75, 1000])\n",
      "---\n",
      "e_id:  e5  C_dict[e_id].shape:  torch.Size([300, 20])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([150, 20])\n",
      "---\n",
      "e_id:  e6  C_dict[e_id].shape:  torch.Size([250, 2000])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([125, 2000])\n",
      "---\n",
      "#\n",
      "creating pytorch variables of input matrices...\n",
      "#\n",
      "__input_transformation - end\n",
      "__network_construction - start\n",
      "__network_construction - end\n",
      "dcmf - model construction - end\n",
      "#\n",
      "__pretrain - start\n",
      "pretrain epoch:  1  total loss L:  6.251332998275757  Took  1.0  secs.\n",
      "pretrain epoch:  2  total loss L:  6.225248336791992  Took  1.0  secs.\n",
      "**pretrain converged**\n",
      "__pretrain - end\n",
      "#\n",
      "dcmf.fit - start\n",
      "epoch:  1  total loss L:  9.840073585510254  Took  1.0  secs.\n",
      "epoch:  2  total loss L:  7.5749616622924805  Took  1.0  secs.\n",
      "epoch:  3  total loss L:  6.58198881149292  Took  1.0  secs.\n",
      "epoch:  4  total loss L:  6.863657712936401  Took  1.0  secs.\n",
      "epoch:  5  total loss L:  6.457305669784546  Took  1.0  secs.\n",
      "Computing RMSE.\n",
      "Rpred.shape:  (1000, 2000)\n",
      "Rtest.shape:  (1000, 2000)\n",
      "nz_list_pred.shape:  25000\n",
      "nz_list_test.shape:  25000\n",
      "rmse:  0.07106720714779179\n",
      "---\n",
      "#\n",
      "dcmf.fit - end\n",
      "#BO starting..\n",
      "#BO - cur_iter:  1\n",
      "updateModel: \n",
      "X_all.shape:  (2, 7)\n",
      "Y_all.shape:  (2, 11)\n",
      "self.X.shape:  (2, 7)\n",
      "self.Y.shape:  (2, 11)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "#\n",
      "dCMF:\n",
      "---\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  4.7153669528578657e-05\n",
      "weight_decay:  0.26347917695876444\n",
      "convg_thres:  1.968883558354767e-05\n",
      "max_epochs:  5\n",
      "isPretrain:  True\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  2\n",
      "k:  100\n",
      "kf:  0.5\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  False\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  1\n",
      "val_metric (used only if X_val #matrices > 0):  rmse\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  False\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "## fold_num:  1  ##\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  4.7153669528578657e-05\n",
      "weight_decay:  0.26347917695876444\n",
      "convg_thres:  1.968883558354767e-05\n",
      "max_epochs:  5\n",
      "isPretrain:  True\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  2\n",
      "k:  100\n",
      "kf:  0.5\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  False\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  1\n",
      "val_metric (used only if X_val #matrices > 0):  rmse\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  False\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "dcmf - model construction - start\n",
      "__input_transformation - start\n",
      "#\n",
      "concatenated-matrix construction...\n",
      "e_id:  e1\n",
      "X_id_list:  ['X1', 'X2', 'X3']\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (1000, 2000)\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (1000, 20)\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (1000, 150)\n",
      "C_dict[e].shape:  torch.Size([1000, 2170])\n",
      "---\n",
      "e_id:  e2\n",
      "X_id_list:  ['X1', 'X4']\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (1000, 2000)\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (2000, 250)\n",
      "C_dict[e].shape:  torch.Size([2000, 1250])\n",
      "---\n",
      "e_id:  e3\n",
      "X_id_list:  ['X2', 'X5']\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (1000, 20)\n",
      "X_id:  X5\n",
      "X[X_id].shape:  (300, 20)\n",
      "C_dict[e].shape:  torch.Size([20, 1300])\n",
      "---\n",
      "e_id:  e4\n",
      "X_id_list:  ['X3']\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (1000, 150)\n",
      "C_dict[e].shape:  torch.Size([150, 1000])\n",
      "---\n",
      "e_id:  e5\n",
      "X_id_list:  ['X5']\n",
      "X_id:  X5\n",
      "X[X_id].shape:  (300, 20)\n",
      "C_dict[e].shape:  torch.Size([300, 20])\n",
      "---\n",
      "e_id:  e6\n",
      "X_id_list:  ['X4']\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (2000, 250)\n",
      "C_dict[e].shape:  torch.Size([250, 2000])\n",
      "---\n",
      "#\n",
      "concatenated-matrix chunking...\n",
      "WARNING: Entity with ID: e5 has minimum feature size 20 in the setting. The encoding length k 100 is larger than minimim entity feature size.\n",
      "#\n",
      "e_id:  e3 , min_num_datapoints:  20 , num_chunks:  2\n",
      "e_id:  e5 , min_features:  20 , k:  100\n",
      "#\n",
      "e_id:  e1  C_dict[e_id].shape:  torch.Size([1000, 2170])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([500, 2170])\n",
      "---\n",
      "e_id:  e2  C_dict[e_id].shape:  torch.Size([2000, 1250])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([1000, 1250])\n",
      "---\n",
      "e_id:  e3  C_dict[e_id].shape:  torch.Size([20, 1300])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([10, 1300])\n",
      "---\n",
      "e_id:  e4  C_dict[e_id].shape:  torch.Size([150, 1000])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([75, 1000])\n",
      "---\n",
      "e_id:  e5  C_dict[e_id].shape:  torch.Size([300, 20])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([150, 20])\n",
      "---\n",
      "e_id:  e6  C_dict[e_id].shape:  torch.Size([250, 2000])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([125, 2000])\n",
      "---\n",
      "#\n",
      "creating pytorch variables of input matrices...\n",
      "#\n",
      "__input_transformation - end\n",
      "__network_construction - start\n",
      "__network_construction - end\n",
      "dcmf - model construction - end\n",
      "#\n",
      "__pretrain - start\n",
      "pretrain epoch:  1  total loss L:  6.185364484786987  Took  1.0  secs.\n",
      "pretrain epoch:  2  total loss L:  6.172137975692749  Took  1.0  secs.\n",
      "**pretrain converged**\n",
      "__pretrain - end\n",
      "#\n",
      "dcmf.fit - start\n",
      "epoch:  1  total loss L:  11.680742263793945  Took  1.1  secs.\n",
      "epoch:  2  total loss L:  8.518387794494629  Took  1.0  secs.\n",
      "epoch:  3  total loss L:  8.150197982788086  Took  1.0  secs.\n",
      "epoch:  4  total loss L:  6.732640266418457  Took  1.0  secs.\n",
      "epoch:  5  total loss L:  6.293523073196411  Took  1.0  secs.\n",
      "Computing RMSE.\n",
      "Rpred.shape:  (1000, 2000)\n",
      "Rtest.shape:  (1000, 2000)\n",
      "nz_list_pred.shape:  25000\n",
      "nz_list_test.shape:  25000\n",
      "rmse:  0.09780870456750396\n",
      "---\n",
      "#\n",
      "dcmf.fit - end\n",
      "num acquisition: 1, time elapsed: 7.90s\n",
      "#BO - cur_iter:  2\n",
      "updateModel: \n",
      "X_all.shape:  (3, 7)\n",
      "Y_all.shape:  (3, 11)\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "#\n",
      "dCMF:\n",
      "---\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  5.59685064547585e-05\n",
      "weight_decay:  0.05\n",
      "convg_thres:  1.68009245504548e-05\n",
      "max_epochs:  5\n",
      "isPretrain:  True\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  2\n",
      "k:  100\n",
      "kf:  0.5\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  False\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  1\n",
      "val_metric (used only if X_val #matrices > 0):  rmse\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  False\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "## fold_num:  1  ##\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  5.59685064547585e-05\n",
      "weight_decay:  0.05\n",
      "convg_thres:  1.68009245504548e-05\n",
      "max_epochs:  5\n",
      "isPretrain:  True\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  2\n",
      "k:  100\n",
      "kf:  0.5\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  False\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  1\n",
      "val_metric (used only if X_val #matrices > 0):  rmse\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  False\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "dcmf - model construction - start\n",
      "__input_transformation - start\n",
      "#\n",
      "concatenated-matrix construction...\n",
      "e_id:  e1\n",
      "X_id_list:  ['X1', 'X2', 'X3']\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (1000, 2000)\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (1000, 20)\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (1000, 150)\n",
      "C_dict[e].shape:  torch.Size([1000, 2170])\n",
      "---\n",
      "e_id:  e2\n",
      "X_id_list:  ['X1', 'X4']\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (1000, 2000)\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (2000, 250)\n",
      "C_dict[e].shape:  torch.Size([2000, 1250])\n",
      "---\n",
      "e_id:  e3\n",
      "X_id_list:  ['X2', 'X5']\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (1000, 20)\n",
      "X_id:  X5\n",
      "X[X_id].shape:  (300, 20)\n",
      "C_dict[e].shape:  torch.Size([20, 1300])\n",
      "---\n",
      "e_id:  e4\n",
      "X_id_list:  ['X3']\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (1000, 150)\n",
      "C_dict[e].shape:  torch.Size([150, 1000])\n",
      "---\n",
      "e_id:  e5\n",
      "X_id_list:  ['X5']\n",
      "X_id:  X5\n",
      "X[X_id].shape:  (300, 20)\n",
      "C_dict[e].shape:  torch.Size([300, 20])\n",
      "---\n",
      "e_id:  e6\n",
      "X_id_list:  ['X4']\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (2000, 250)\n",
      "C_dict[e].shape:  torch.Size([250, 2000])\n",
      "---\n",
      "#\n",
      "concatenated-matrix chunking...\n",
      "WARNING: Entity with ID: e5 has minimum feature size 20 in the setting. The encoding length k 100 is larger than minimim entity feature size.\n",
      "#\n",
      "e_id:  e3 , min_num_datapoints:  20 , num_chunks:  2\n",
      "e_id:  e5 , min_features:  20 , k:  100\n",
      "#\n",
      "e_id:  e1  C_dict[e_id].shape:  torch.Size([1000, 2170])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([500, 2170])\n",
      "---\n",
      "e_id:  e2  C_dict[e_id].shape:  torch.Size([2000, 1250])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([1000, 1250])\n",
      "---\n",
      "e_id:  e3  C_dict[e_id].shape:  torch.Size([20, 1300])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([10, 1300])\n",
      "---\n",
      "e_id:  e4  C_dict[e_id].shape:  torch.Size([150, 1000])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([75, 1000])\n",
      "---\n",
      "e_id:  e5  C_dict[e_id].shape:  torch.Size([300, 20])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([150, 20])\n",
      "---\n",
      "e_id:  e6  C_dict[e_id].shape:  torch.Size([250, 2000])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([125, 2000])\n",
      "---\n",
      "#\n",
      "creating pytorch variables of input matrices...\n",
      "#\n",
      "__input_transformation - end\n",
      "__network_construction - start\n",
      "__network_construction - end\n",
      "dcmf - model construction - end\n",
      "#\n",
      "__pretrain - start\n",
      "pretrain epoch:  1  total loss L:  5.893256187438965  Took  1.0  secs.\n",
      "pretrain epoch:  2  total loss L:  5.872880220413208  Took  1.1  secs.\n",
      "**pretrain converged**\n",
      "__pretrain - end\n",
      "#\n",
      "dcmf.fit - start\n",
      "epoch:  1  total loss L:  11.073211669921875  Took  1.0  secs.\n",
      "epoch:  2  total loss L:  7.127645015716553  Took  1.0  secs.\n",
      "epoch:  3  total loss L:  7.225202560424805  Took  1.0  secs.\n",
      "epoch:  4  total loss L:  6.611584424972534  Took  1.0  secs.\n",
      "epoch:  5  total loss L:  6.104795455932617  Took  1.0  secs.\n",
      "Computing RMSE.\n",
      "Rpred.shape:  (1000, 2000)\n",
      "Rtest.shape:  (1000, 2000)\n",
      "nz_list_pred.shape:  25000\n",
      "nz_list_test.shape:  25000\n",
      "rmse:  0.08937467888797047\n",
      "---\n",
      "#\n",
      "dcmf.fit - end\n",
      "num acquisition: 2, time elapsed: 15.91s\n",
      "#BO - cur_iter:  3\n",
      "updateModel: \n",
      "X_all.shape:  (4, 7)\n",
      "Y_all.shape:  (4, 11)\n",
      "#BO Exit conditions:\n",
      "self.num_acquisitions >= self.max_iter:  True  | self.num_acquisitions:  2  |  self.max_iter:  2\n",
      "len(self.X) > 1:  True  |  4\n",
      "self._distance_last_evaluations() <= self.eps:  False  | self._distance_last_evaluations():  90.0007073053035  | self.eps:  0\n",
      "#\n",
      "dcmf_bo.fit - end\n"
     ]
    }
   ],
   "source": [
    "dcmf_bo_model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Result attributes:*\n",
    "\n",
    "- **out_dict_p_hash_info**: dict, keys are loss/validation performance attributes and values are corresponding results for the best parameter set\n",
    "- **out_list_D**: list of dicts, info dicts for all the BO steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'E': 6,\n",
       " 'M': 5,\n",
       " 'best_criterion': 'loss',\n",
       " 'list_bo_hyperparams': ['learning_rate', 'convg_thres', 'weight_decay'],\n",
       " 'loss_all_folds': {'1': [0.6861676573753357,\n",
       "   0.7225450873374939,\n",
       "   1.0877848267555237,\n",
       "   0.9674782156944275,\n",
       "   1.304173469543457,\n",
       "   0.9413672089576721,\n",
       "   0.05440543219447136,\n",
       "   0.05987958237528801,\n",
       "   0.04063842073082924,\n",
       "   0.01538105309009552,\n",
       "   0.008572279708459973]},\n",
       " 'loss_all_folds_avg_sum': 5.888393233763054,\n",
       " 'loss_all_folds_avg_tuple': [0.6861676573753357,\n",
       "  0.7225450873374939,\n",
       "  1.0877848267555237,\n",
       "  0.9674782156944275,\n",
       "  1.304173469543457,\n",
       "  0.9413672089576721,\n",
       "  0.05440543219447136,\n",
       "  0.05987958237528801,\n",
       "  0.04063842073082924,\n",
       "  0.01538105309009552,\n",
       "  0.008572279708459973],\n",
       " 'num_val_sets': 1,\n",
       " 'params': {'convg_thres': 6.968342245572056e-05,\n",
       "  'd_actf': 'tanh',\n",
       "  'e_actf': 'tanh',\n",
       "  'is_linear_last_dec_layer': False,\n",
       "  'is_linear_last_enc_layer': False,\n",
       "  'is_pretrain': True,\n",
       "  'k': 100,\n",
       "  'kf': 0.5,\n",
       "  'learning_rate': 4.361453979173315e-05,\n",
       "  'max_epochs': 5,\n",
       "  'max_pretrain_epochs': 2,\n",
       "  'num_chunks': 2,\n",
       "  'pretrain_thres': 0.1,\n",
       "  'weight_decay': 0.25995186097511425},\n",
       " 'val_metric': 'rmse',\n",
       " 'val_perf_all_folds': {'1': {'X1': 0.10377505267855172}},\n",
       " 'val_perf_all_folds_avg': {'X1': 0.10377505267855172},\n",
       " 'val_perf_all_folds_total': {'1': 0.10377505267855172},\n",
       " 'val_perf_all_folds_total_avg': 0.10377505267855172}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcmf_bo_model.out_dict_p_hash_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'E': 6,\n",
       "  'M': 5,\n",
       "  'best_criterion': 'loss',\n",
       "  'list_bo_hyperparams': ['learning_rate', 'convg_thres', 'weight_decay'],\n",
       "  'loss_all_folds': {'1': [0.6861676573753357,\n",
       "    0.7225450873374939,\n",
       "    1.0877848267555237,\n",
       "    0.9674782156944275,\n",
       "    1.304173469543457,\n",
       "    0.9413672089576721,\n",
       "    0.05440543219447136,\n",
       "    0.05987958237528801,\n",
       "    0.04063842073082924,\n",
       "    0.01538105309009552,\n",
       "    0.008572279708459973]},\n",
       "  'loss_all_folds_avg_sum': 5.888393233763054,\n",
       "  'loss_all_folds_avg_tuple': [0.6861676573753357,\n",
       "   0.7225450873374939,\n",
       "   1.0877848267555237,\n",
       "   0.9674782156944275,\n",
       "   1.304173469543457,\n",
       "   0.9413672089576721,\n",
       "   0.05440543219447136,\n",
       "   0.05987958237528801,\n",
       "   0.04063842073082924,\n",
       "   0.01538105309009552,\n",
       "   0.008572279708459973],\n",
       "  'num_val_sets': 1,\n",
       "  'params': {'convg_thres': 6.968342245572056e-05,\n",
       "   'd_actf': 'tanh',\n",
       "   'e_actf': 'tanh',\n",
       "   'is_linear_last_dec_layer': False,\n",
       "   'is_linear_last_enc_layer': False,\n",
       "   'is_pretrain': True,\n",
       "   'k': 100,\n",
       "   'kf': 0.5,\n",
       "   'learning_rate': 4.361453979173315e-05,\n",
       "   'max_epochs': 5,\n",
       "   'max_pretrain_epochs': 2,\n",
       "   'num_chunks': 2,\n",
       "   'pretrain_thres': 0.1,\n",
       "   'weight_decay': 0.25995186097511425},\n",
       "  'val_metric': 'rmse',\n",
       "  'val_perf_all_folds': {'1': {'X1': 0.10377505267855172}},\n",
       "  'val_perf_all_folds_avg': {'X1': 0.10377505267855172},\n",
       "  'val_perf_all_folds_total': {'1': 0.10377505267855172},\n",
       "  'val_perf_all_folds_total_avg': 0.10377505267855172},\n",
       " {'E': 6,\n",
       "  'M': 5,\n",
       "  'list_bo_hyperparams': ['learning_rate', 'convg_thres', 'weight_decay'],\n",
       "  'loss_all_folds': {'1': [0.687395453453064,\n",
       "    0.7208608388900757,\n",
       "    1.0851430296897888,\n",
       "    0.9737319648265839,\n",
       "    1.6988522410392761,\n",
       "    0.9438808262348175,\n",
       "    0.05085761100053787,\n",
       "    0.10031270980834961,\n",
       "    0.045417243614792824,\n",
       "    0.03983781021088362,\n",
       "    0.11101560480892658]},\n",
       "  'loss_all_folds_avg_sum': 6.4573053335770965,\n",
       "  'loss_all_folds_avg_tuple': [0.687395453453064,\n",
       "   0.7208608388900757,\n",
       "   1.0851430296897888,\n",
       "   0.9737319648265839,\n",
       "   1.6988522410392761,\n",
       "   0.9438808262348175,\n",
       "   0.05085761100053787,\n",
       "   0.10031270980834961,\n",
       "   0.045417243614792824,\n",
       "   0.03983781021088362,\n",
       "   0.11101560480892658],\n",
       "  'num_val_sets': 1,\n",
       "  'params': {'convg_thres': 7.404244553417574e-05,\n",
       "   'd_actf': 'tanh',\n",
       "   'e_actf': 'tanh',\n",
       "   'is_linear_last_dec_layer': False,\n",
       "   'is_linear_last_enc_layer': False,\n",
       "   'is_pretrain': True,\n",
       "   'k': 100,\n",
       "   'kf': 0.5,\n",
       "   'learning_rate': 7.75906309603412e-05,\n",
       "   'max_epochs': 5,\n",
       "   'max_pretrain_epochs': 2,\n",
       "   'num_chunks': 2,\n",
       "   'pretrain_thres': 0.1,\n",
       "   'weight_decay': 0.13638972753881876},\n",
       "  'val_metric': 'rmse',\n",
       "  'val_perf_all_folds': {'1': {'X1': 0.07106720714779179}},\n",
       "  'val_perf_all_folds_avg': {'X1': 0.07106720714779179},\n",
       "  'val_perf_all_folds_total': {'1': 0.07106720714779179},\n",
       "  'val_perf_all_folds_total_avg': 0.07106720714779179},\n",
       " {'E': 6,\n",
       "  'M': 5,\n",
       "  'list_bo_hyperparams': ['learning_rate', 'convg_thres', 'weight_decay'],\n",
       "  'loss_all_folds': {'1': [0.6863579750061035,\n",
       "    0.7162083089351654,\n",
       "    1.0818336009979248,\n",
       "    0.9680303633213043,\n",
       "    1.715064287185669,\n",
       "    0.9447867572307587,\n",
       "    0.05276443436741829,\n",
       "    0.029509572312235832,\n",
       "    0.04162974376231432,\n",
       "    0.010077390819787979,\n",
       "    0.04726079688407481]},\n",
       "  'loss_all_folds_avg_sum': 6.293523230822757,\n",
       "  'loss_all_folds_avg_tuple': [0.6863579750061035,\n",
       "   0.7162083089351654,\n",
       "   1.0818336009979248,\n",
       "   0.9680303633213043,\n",
       "   1.715064287185669,\n",
       "   0.9447867572307587,\n",
       "   0.05276443436741829,\n",
       "   0.029509572312235832,\n",
       "   0.04162974376231432,\n",
       "   0.010077390819787979,\n",
       "   0.04726079688407481],\n",
       "  'num_val_sets': 1,\n",
       "  'params': {'convg_thres': 1.968883558354767e-05,\n",
       "   'd_actf': 'tanh',\n",
       "   'e_actf': 'tanh',\n",
       "   'is_linear_last_dec_layer': False,\n",
       "   'is_linear_last_enc_layer': False,\n",
       "   'is_pretrain': True,\n",
       "   'k': 100,\n",
       "   'kf': 0.5,\n",
       "   'learning_rate': 4.7153669528578657e-05,\n",
       "   'max_epochs': 5,\n",
       "   'max_pretrain_epochs': 2,\n",
       "   'num_chunks': 2,\n",
       "   'pretrain_thres': 0.1,\n",
       "   'weight_decay': 0.26347917695876444},\n",
       "  'val_metric': 'rmse',\n",
       "  'val_perf_all_folds': {'1': {'X1': 0.09780870456750396}},\n",
       "  'val_perf_all_folds_avg': {'X1': 0.09780870456750396},\n",
       "  'val_perf_all_folds_total': {'1': 0.09780870456750396},\n",
       "  'val_perf_all_folds_total_avg': 0.09780870456750396},\n",
       " {'E': 6,\n",
       "  'M': 5,\n",
       "  'list_bo_hyperparams': ['learning_rate', 'convg_thres', 'weight_decay'],\n",
       "  'loss_all_folds': {'1': [0.6874182522296906,\n",
       "    0.721558690071106,\n",
       "    1.079846739768982,\n",
       "    0.9720847010612488,\n",
       "    1.3910327553749084,\n",
       "    0.9425188302993774,\n",
       "    0.05937027186155319,\n",
       "    0.1276453696191311,\n",
       "    0.050385523587465286,\n",
       "    0.03322018310427666,\n",
       "    0.03971369261853397]},\n",
       "  'loss_all_folds_avg_sum': 6.104795009596273,\n",
       "  'loss_all_folds_avg_tuple': [0.6874182522296906,\n",
       "   0.721558690071106,\n",
       "   1.079846739768982,\n",
       "   0.9720847010612488,\n",
       "   1.3910327553749084,\n",
       "   0.9425188302993774,\n",
       "   0.05937027186155319,\n",
       "   0.1276453696191311,\n",
       "   0.050385523587465286,\n",
       "   0.03322018310427666,\n",
       "   0.03971369261853397],\n",
       "  'num_val_sets': 1,\n",
       "  'params': {'convg_thres': 1.68009245504548e-05,\n",
       "   'd_actf': 'tanh',\n",
       "   'e_actf': 'tanh',\n",
       "   'is_linear_last_dec_layer': False,\n",
       "   'is_linear_last_enc_layer': False,\n",
       "   'is_pretrain': True,\n",
       "   'k': 100,\n",
       "   'kf': 0.5,\n",
       "   'learning_rate': 5.59685064547585e-05,\n",
       "   'max_epochs': 5,\n",
       "   'max_pretrain_epochs': 2,\n",
       "   'num_chunks': 2,\n",
       "   'pretrain_thres': 0.1,\n",
       "   'weight_decay': 0.05},\n",
       "  'val_metric': 'rmse',\n",
       "  'val_perf_all_folds': {'1': {'X1': 0.08937467888797047}},\n",
       "  'val_perf_all_folds_avg': {'X1': 0.08937467888797047},\n",
       "  'val_perf_all_folds_total': {'1': 0.08937467888797047},\n",
       "  'val_perf_all_folds_total_avg': 0.08937467888797047}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcmf_bo_model.out_list_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
