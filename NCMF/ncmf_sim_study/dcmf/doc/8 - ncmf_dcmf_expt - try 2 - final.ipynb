{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import os\n",
    "#\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import time\n",
    "import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dcmf import dcmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dname = \"./../../ncmf_sim_data/\"\n",
    "in_dir = base_dname + \"cmf/\"\n",
    "out_dir_base = base_dname + \"dcmf/out/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_name:  dt1\n",
      "#\n",
      "X0.shape:  (200, 500)\n",
      "X1.shape:  (200, 300)\n",
      "X2.shape:  (200, 400)\n",
      "X3.shape:  (700, 500)\n",
      "X4.shape:  (600, 300)\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "WARNING: The following parameters are unused since no validation data provided.\n",
      "val_metric:  auc\n",
      "at_k:  10\n",
      "is_val_transpose:  True\n",
      "#\n",
      "dCMF:\n",
      "---\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  0.0001\n",
      "weight_decay:  0.01\n",
      "convg_thres:  -0.1\n",
      "max_epochs:  100\n",
      "isPretrain:  False\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  10\n",
      "k:  100\n",
      "kf:  0.0005\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  True\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  0\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "## fold_num:  1  ##\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "WARNING: The following parameters are unused since no validation data provided.\n",
      "val_metric:  auc\n",
      "at_k:  10\n",
      "is_val_transpose:  True\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  0.0001\n",
      "weight_decay:  0.01\n",
      "convg_thres:  -0.1\n",
      "max_epochs:  100\n",
      "isPretrain:  False\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  10\n",
      "k:  100\n",
      "kf:  0.0005\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  True\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  0\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "dcmf - model construction - start\n",
      "__input_transformation - start\n",
      "#\n",
      "concatenated-matrix construction...\n",
      "e_id:  e0\n",
      "X_id_list:  ['X0', 'X1', 'X2']\n",
      "X_id:  X0\n",
      "X[X_id].shape:  (200, 500)\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (200, 300)\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (200, 400)\n",
      "C_dict[e].shape:  torch.Size([200, 1200])\n",
      "---\n",
      "e_id:  e1\n",
      "X_id_list:  ['X0', 'X3']\n",
      "X_id:  X0\n",
      "X[X_id].shape:  (200, 500)\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (700, 500)\n",
      "C_dict[e].shape:  torch.Size([500, 900])\n",
      "---\n",
      "e_id:  e2\n",
      "X_id_list:  ['X1', 'X4']\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (200, 300)\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (600, 300)\n",
      "C_dict[e].shape:  torch.Size([300, 800])\n",
      "---\n",
      "e_id:  e3\n",
      "X_id_list:  ['X2']\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (200, 400)\n",
      "C_dict[e].shape:  torch.Size([400, 200])\n",
      "---\n",
      "e_id:  e4\n",
      "X_id_list:  ['X3']\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (700, 500)\n",
      "C_dict[e].shape:  torch.Size([700, 500])\n",
      "---\n",
      "e_id:  e5\n",
      "X_id_list:  ['X4']\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (600, 300)\n",
      "C_dict[e].shape:  torch.Size([600, 300])\n",
      "---\n",
      "#\n",
      "concatenated-matrix chunking...\n",
      "#\n",
      "e_id:  e0 , min_num_datapoints:  200 , num_chunks:  10\n",
      "e_id:  e3 , min_features:  200 , k:  100\n",
      "#\n",
      "e_id:  e0  C_dict[e_id].shape:  torch.Size([200, 1200])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([20, 1200])\n",
      "---\n",
      "e_id:  e1  C_dict[e_id].shape:  torch.Size([500, 900])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([50, 900])\n",
      "---\n",
      "e_id:  e2  C_dict[e_id].shape:  torch.Size([300, 800])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([30, 800])\n",
      "---\n",
      "e_id:  e3  C_dict[e_id].shape:  torch.Size([400, 200])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([40, 200])\n",
      "---\n",
      "e_id:  e4  C_dict[e_id].shape:  torch.Size([700, 500])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([70, 500])\n",
      "---\n",
      "e_id:  e5  C_dict[e_id].shape:  torch.Size([600, 300])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([60, 300])\n",
      "---\n",
      "#\n",
      "creating pytorch variables of input matrices...\n",
      "#\n",
      "__input_transformation - end\n",
      "__network_construction - start\n",
      "__network_construction - end\n",
      "dcmf - model construction - end\n",
      "#\n",
      "#\n",
      "dcmf.fit - start\n",
      "epoch:  1  total loss L:  123.07052898406982  Took  0.6  secs.\n",
      "epoch:  2  total loss L:  67.95838832855225  Took  0.1  secs.\n",
      "epoch:  3  total loss L:  55.93578910827637  Took  0.1  secs.\n",
      "epoch:  4  total loss L:  51.08290767669678  Took  0.1  secs.\n",
      "epoch:  5  total loss L:  47.457496643066406  Took  0.1  secs.\n",
      "epoch:  6  total loss L:  45.29337501525879  Took  0.1  secs.\n",
      "epoch:  7  total loss L:  43.708740234375  Took  0.1  secs.\n",
      "epoch:  8  total loss L:  42.3167986869812  Took  0.1  secs.\n",
      "epoch:  9  total loss L:  41.1111946105957  Took  0.1  secs.\n",
      "epoch:  10  total loss L:  40.44743037223816  Took  0.1  secs.\n",
      "epoch:  11  total loss L:  39.34563684463501  Took  0.1  secs.\n",
      "epoch:  12  total loss L:  38.646265506744385  Took  0.1  secs.\n",
      "epoch:  13  total loss L:  38.05079698562622  Took  0.1  secs.\n",
      "epoch:  14  total loss L:  38.64036798477173  Took  0.2  secs.\n",
      "epoch:  15  total loss L:  37.6492018699646  Took  0.1  secs.\n",
      "epoch:  16  total loss L:  37.400230884552  Took  0.1  secs.\n",
      "epoch:  17  total loss L:  35.621633529663086  Took  0.1  secs.\n",
      "epoch:  18  total loss L:  34.445093870162964  Took  0.1  secs.\n",
      "epoch:  19  total loss L:  33.59289073944092  Took  0.1  secs.\n",
      "epoch:  20  total loss L:  32.4924373626709  Took  0.1  secs.\n",
      "epoch:  21  total loss L:  31.887429237365723  Took  0.2  secs.\n",
      "epoch:  22  total loss L:  30.964646577835083  Took  0.1  secs.\n",
      "epoch:  23  total loss L:  30.451085805892944  Took  0.1  secs.\n",
      "epoch:  24  total loss L:  29.83263063430786  Took  0.1  secs.\n",
      "epoch:  25  total loss L:  29.331292629241943  Took  0.1  secs.\n",
      "epoch:  26  total loss L:  29.242319345474243  Took  0.1  secs.\n",
      "epoch:  27  total loss L:  29.379243850708008  Took  0.1  secs.\n",
      "epoch:  28  total loss L:  29.30562734603882  Took  0.1  secs.\n",
      "epoch:  29  total loss L:  30.788459300994873  Took  0.1  secs.\n",
      "epoch:  30  total loss L:  32.158852338790894  Took  0.1  secs.\n",
      "epoch:  31  total loss L:  30.81061053276062  Took  0.1  secs.\n",
      "epoch:  32  total loss L:  28.286383628845215  Took  0.1  secs.\n",
      "epoch:  33  total loss L:  27.435775756835938  Took  0.1  secs.\n",
      "epoch:  34  total loss L:  26.460845232009888  Took  0.1  secs.\n",
      "epoch:  35  total loss L:  25.905593395233154  Took  0.1  secs.\n",
      "epoch:  36  total loss L:  25.63486361503601  Took  0.1  secs.\n",
      "epoch:  37  total loss L:  25.434686422348022  Took  0.1  secs.\n",
      "epoch:  38  total loss L:  25.30066967010498  Took  0.2  secs.\n",
      "epoch:  39  total loss L:  26.0232036113739  Took  0.1  secs.\n",
      "epoch:  40  total loss L:  26.15801739692688  Took  0.2  secs.\n",
      "epoch:  41  total loss L:  25.779624462127686  Took  0.1  secs.\n",
      "epoch:  42  total loss L:  26.293986558914185  Took  0.2  secs.\n",
      "epoch:  43  total loss L:  24.86643934249878  Took  0.1  secs.\n",
      "epoch:  44  total loss L:  24.067390203475952  Took  0.1  secs.\n",
      "epoch:  45  total loss L:  23.848286151885986  Took  0.1  secs.\n",
      "epoch:  46  total loss L:  23.637524127960205  Took  0.2  secs.\n",
      "epoch:  47  total loss L:  24.85008215904236  Took  0.1  secs.\n",
      "epoch:  48  total loss L:  27.514029264450073  Took  0.1  secs.\n",
      "epoch:  49  total loss L:  28.120805501937866  Took  0.1  secs.\n",
      "epoch:  50  total loss L:  29.627901554107666  Took  0.1  secs.\n",
      "epoch:  51  total loss L:  25.04784870147705  Took  0.1  secs.\n",
      "epoch:  52  total loss L:  23.737990379333496  Took  0.1  secs.\n",
      "epoch:  53  total loss L:  23.585582971572876  Took  0.1  secs.\n",
      "epoch:  54  total loss L:  23.12191867828369  Took  0.1  secs.\n",
      "epoch:  55  total loss L:  23.017136096954346  Took  0.1  secs.\n",
      "epoch:  56  total loss L:  23.807124614715576  Took  0.1  secs.\n",
      "epoch:  57  total loss L:  23.165218114852905  Took  0.1  secs.\n",
      "epoch:  58  total loss L:  24.056779861450195  Took  0.1  secs.\n",
      "epoch:  59  total loss L:  22.995339155197144  Took  0.1  secs.\n",
      "epoch:  60  total loss L:  23.597252130508423  Took  0.1  secs.\n",
      "epoch:  61  total loss L:  22.023294687271118  Took  0.1  secs.\n",
      "epoch:  62  total loss L:  21.928014516830444  Took  0.1  secs.\n",
      "epoch:  63  total loss L:  21.6525239944458  Took  0.1  secs.\n",
      "epoch:  64  total loss L:  21.307823181152344  Took  0.1  secs.\n",
      "epoch:  65  total loss L:  20.861881971359253  Took  0.1  secs.\n",
      "epoch:  66  total loss L:  21.43996548652649  Took  0.1  secs.\n",
      "epoch:  67  total loss L:  21.354623794555664  Took  0.1  secs.\n",
      "epoch:  68  total loss L:  20.93554127216339  Took  0.1  secs.\n",
      "epoch:  69  total loss L:  21.612435817718506  Took  0.1  secs.\n",
      "epoch:  70  total loss L:  22.08994221687317  Took  0.1  secs.\n",
      "epoch:  71  total loss L:  22.83476209640503  Took  0.1  secs.\n",
      "epoch:  72  total loss L:  24.3371422290802  Took  0.2  secs.\n",
      "epoch:  73  total loss L:  23.922240734100342  Took  0.1  secs.\n",
      "epoch:  74  total loss L:  25.487680912017822  Took  0.1  secs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  75  total loss L:  28.212308645248413  Took  0.1  secs.\n",
      "epoch:  76  total loss L:  31.09899067878723  Took  0.1  secs.\n",
      "epoch:  77  total loss L:  37.004623889923096  Took  0.1  secs.\n",
      "epoch:  78  total loss L:  48.0832941532135  Took  0.1  secs.\n",
      "epoch:  79  total loss L:  63.55009961128235  Took  0.1  secs.\n",
      "epoch:  80  total loss L:  37.173906087875366  Took  0.1  secs.\n",
      "epoch:  81  total loss L:  32.3003568649292  Took  0.1  secs.\n",
      "epoch:  82  total loss L:  25.92588520050049  Took  0.1  secs.\n",
      "epoch:  83  total loss L:  23.433659315109253  Took  0.1  secs.\n",
      "epoch:  84  total loss L:  22.216071128845215  Took  0.1  secs.\n",
      "epoch:  85  total loss L:  22.003260612487793  Took  0.1  secs.\n",
      "epoch:  86  total loss L:  21.83461046218872  Took  0.1  secs.\n",
      "epoch:  87  total loss L:  21.881680965423584  Took  0.1  secs.\n",
      "epoch:  88  total loss L:  21.292073249816895  Took  0.2  secs.\n",
      "epoch:  89  total loss L:  20.80760931968689  Took  0.1  secs.\n",
      "epoch:  90  total loss L:  20.337180495262146  Took  0.1  secs.\n",
      "epoch:  91  total loss L:  20.25967049598694  Took  0.1  secs.\n",
      "epoch:  92  total loss L:  20.06131887435913  Took  0.1  secs.\n",
      "epoch:  93  total loss L:  20.086225152015686  Took  0.1  secs.\n",
      "epoch:  94  total loss L:  20.485780596733093  Took  0.1  secs.\n",
      "epoch:  95  total loss L:  20.11263120174408  Took  0.1  secs.\n",
      "epoch:  96  total loss L:  21.32381844520569  Took  0.2  secs.\n",
      "epoch:  97  total loss L:  21.557475924491882  Took  0.1  secs.\n",
      "epoch:  98  total loss L:  21.718939661979675  Took  0.1  secs.\n",
      "epoch:  99  total loss L:  22.362179040908813  Took  0.1  secs.\n",
      "epoch:  100  total loss L:  21.2283798456192  Took  0.1  secs.\n",
      "#\n",
      "dcmf.fit - end\n",
      "dataset_name:  ds1\n",
      "#\n",
      "X0.shape:  (200, 500)\n",
      "X1.shape:  (200, 300)\n",
      "X2.shape:  (200, 400)\n",
      "X3.shape:  (700, 500)\n",
      "X4.shape:  (600, 300)\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "WARNING: The following parameters are unused since no validation data provided.\n",
      "val_metric:  auc\n",
      "at_k:  10\n",
      "is_val_transpose:  True\n",
      "#\n",
      "dCMF:\n",
      "---\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  0.0001\n",
      "weight_decay:  0.01\n",
      "convg_thres:  -0.1\n",
      "max_epochs:  100\n",
      "isPretrain:  False\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  10\n",
      "k:  100\n",
      "kf:  0.0005\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  True\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  0\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "## fold_num:  1  ##\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "WARNING: The following parameters are unused since no validation data provided.\n",
      "val_metric:  auc\n",
      "at_k:  10\n",
      "is_val_transpose:  True\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  0.0001\n",
      "weight_decay:  0.01\n",
      "convg_thres:  -0.1\n",
      "max_epochs:  100\n",
      "isPretrain:  False\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  10\n",
      "k:  100\n",
      "kf:  0.0005\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  True\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  0\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "dcmf - model construction - start\n",
      "__input_transformation - start\n",
      "#\n",
      "concatenated-matrix construction...\n",
      "e_id:  e0\n",
      "X_id_list:  ['X0', 'X1', 'X2']\n",
      "X_id:  X0\n",
      "X[X_id].shape:  (200, 500)\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (200, 300)\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (200, 400)\n",
      "C_dict[e].shape:  torch.Size([200, 1200])\n",
      "---\n",
      "e_id:  e1\n",
      "X_id_list:  ['X0', 'X3']\n",
      "X_id:  X0\n",
      "X[X_id].shape:  (200, 500)\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (700, 500)\n",
      "C_dict[e].shape:  torch.Size([500, 900])\n",
      "---\n",
      "e_id:  e2\n",
      "X_id_list:  ['X1', 'X4']\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (200, 300)\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (600, 300)\n",
      "C_dict[e].shape:  torch.Size([300, 800])\n",
      "---\n",
      "e_id:  e3\n",
      "X_id_list:  ['X2']\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (200, 400)\n",
      "C_dict[e].shape:  torch.Size([400, 200])\n",
      "---\n",
      "e_id:  e4\n",
      "X_id_list:  ['X3']\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (700, 500)\n",
      "C_dict[e].shape:  torch.Size([700, 500])\n",
      "---\n",
      "e_id:  e5\n",
      "X_id_list:  ['X4']\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (600, 300)\n",
      "C_dict[e].shape:  torch.Size([600, 300])\n",
      "---\n",
      "#\n",
      "concatenated-matrix chunking...\n",
      "#\n",
      "e_id:  e0 , min_num_datapoints:  200 , num_chunks:  10\n",
      "e_id:  e3 , min_features:  200 , k:  100\n",
      "#\n",
      "e_id:  e0  C_dict[e_id].shape:  torch.Size([200, 1200])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([20, 1200])\n",
      "---\n",
      "e_id:  e1  C_dict[e_id].shape:  torch.Size([500, 900])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([50, 900])\n",
      "---\n",
      "e_id:  e2  C_dict[e_id].shape:  torch.Size([300, 800])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([30, 800])\n",
      "---\n",
      "e_id:  e3  C_dict[e_id].shape:  torch.Size([400, 200])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([40, 200])\n",
      "---\n",
      "e_id:  e4  C_dict[e_id].shape:  torch.Size([700, 500])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([70, 500])\n",
      "---\n",
      "e_id:  e5  C_dict[e_id].shape:  torch.Size([600, 300])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([60, 300])\n",
      "---\n",
      "#\n",
      "creating pytorch variables of input matrices...\n",
      "#\n",
      "__input_transformation - end\n",
      "__network_construction - start\n",
      "__network_construction - end\n",
      "dcmf - model construction - end\n",
      "#\n",
      "#\n",
      "dcmf.fit - start\n",
      "epoch:  1  total loss L:  55.35438680648804  Took  0.1  secs.\n",
      "epoch:  2  total loss L:  43.31899881362915  Took  0.1  secs.\n",
      "epoch:  3  total loss L:  40.2747700214386  Took  0.1  secs.\n",
      "epoch:  4  total loss L:  37.32643103599548  Took  0.1  secs.\n",
      "epoch:  5  total loss L:  34.99308156967163  Took  0.1  secs.\n",
      "epoch:  6  total loss L:  33.231425046920776  Took  0.1  secs.\n",
      "epoch:  7  total loss L:  32.087557315826416  Took  0.2  secs.\n",
      "epoch:  8  total loss L:  31.031537294387817  Took  0.1  secs.\n",
      "epoch:  9  total loss L:  30.11591625213623  Took  0.1  secs.\n",
      "epoch:  10  total loss L:  29.256916761398315  Took  0.1  secs.\n",
      "epoch:  11  total loss L:  28.438143968582153  Took  0.1  secs.\n",
      "epoch:  12  total loss L:  27.657484769821167  Took  0.1  secs.\n",
      "epoch:  13  total loss L:  26.907371997833252  Took  0.1  secs.\n",
      "epoch:  14  total loss L:  26.178702116012573  Took  0.1  secs.\n",
      "epoch:  15  total loss L:  25.4706974029541  Took  0.1  secs.\n",
      "epoch:  16  total loss L:  24.783284187316895  Took  0.1  secs.\n",
      "epoch:  17  total loss L:  24.116819858551025  Took  0.1  secs.\n",
      "epoch:  18  total loss L:  23.4714298248291  Took  0.1  secs.\n",
      "epoch:  19  total loss L:  22.847361087799072  Took  0.1  secs.\n",
      "epoch:  20  total loss L:  22.24483895301819  Took  0.1  secs.\n",
      "epoch:  21  total loss L:  21.663516998291016  Took  0.1  secs.\n",
      "epoch:  22  total loss L:  21.104317784309387  Took  0.1  secs.\n",
      "epoch:  23  total loss L:  20.569690823554993  Took  0.1  secs.\n",
      "epoch:  24  total loss L:  20.06187617778778  Took  0.1  secs.\n",
      "epoch:  25  total loss L:  19.588090181350708  Took  0.1  secs.\n",
      "epoch:  26  total loss L:  19.15946936607361  Took  0.1  secs.\n",
      "epoch:  27  total loss L:  18.799598693847656  Took  0.1  secs.\n",
      "epoch:  28  total loss L:  18.566656470298767  Took  0.1  secs.\n",
      "epoch:  29  total loss L:  18.63977873325348  Took  0.1  secs.\n",
      "epoch:  30  total loss L:  19.609857082366943  Took  0.1  secs.\n",
      "epoch:  31  total loss L:  23.160444855690002  Took  0.1  secs.\n",
      "epoch:  32  total loss L:  30.740111231803894  Took  0.1  secs.\n",
      "epoch:  33  total loss L:  30.21303153038025  Took  0.1  secs.\n",
      "epoch:  34  total loss L:  48.99041390419006  Took  0.1  secs.\n",
      "epoch:  35  total loss L:  50.5374493598938  Took  0.2  secs.\n",
      "epoch:  36  total loss L:  34.4404776096344  Took  0.1  secs.\n",
      "epoch:  37  total loss L:  23.56176519393921  Took  0.1  secs.\n",
      "epoch:  38  total loss L:  22.040363788604736  Took  0.1  secs.\n",
      "epoch:  39  total loss L:  22.569238662719727  Took  0.2  secs.\n",
      "epoch:  40  total loss L:  22.937493920326233  Took  0.1  secs.\n",
      "epoch:  41  total loss L:  23.147199392318726  Took  0.1  secs.\n",
      "epoch:  42  total loss L:  23.040860533714294  Took  0.1  secs.\n",
      "epoch:  43  total loss L:  22.698733806610107  Took  0.1  secs.\n",
      "epoch:  44  total loss L:  23.03765344619751  Took  0.1  secs.\n",
      "epoch:  45  total loss L:  22.118484616279602  Took  0.1  secs.\n",
      "epoch:  46  total loss L:  20.20349144935608  Took  0.1  secs.\n",
      "epoch:  47  total loss L:  19.26310384273529  Took  0.1  secs.\n",
      "epoch:  48  total loss L:  18.926671266555786  Took  0.1  secs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  49  total loss L:  19.25343954563141  Took  0.1  secs.\n",
      "epoch:  50  total loss L:  20.602195620536804  Took  0.1  secs.\n",
      "epoch:  51  total loss L:  19.577282428741455  Took  0.1  secs.\n",
      "epoch:  52  total loss L:  18.87981426715851  Took  0.1  secs.\n",
      "epoch:  53  total loss L:  18.19261121749878  Took  0.1  secs.\n",
      "epoch:  54  total loss L:  17.765746235847473  Took  0.1  secs.\n",
      "epoch:  55  total loss L:  17.36611580848694  Took  0.1  secs.\n",
      "epoch:  56  total loss L:  17.11756920814514  Took  0.1  secs.\n",
      "epoch:  57  total loss L:  16.870088696479797  Took  0.1  secs.\n",
      "epoch:  58  total loss L:  16.645677089691162  Took  0.1  secs.\n",
      "epoch:  59  total loss L:  16.42983627319336  Took  0.1  secs.\n",
      "epoch:  60  total loss L:  16.22782552242279  Took  0.1  secs.\n",
      "epoch:  61  total loss L:  16.026572227478027  Took  0.1  secs.\n",
      "epoch:  62  total loss L:  15.83881950378418  Took  0.2  secs.\n",
      "epoch:  63  total loss L:  15.655691146850586  Took  0.1  secs.\n",
      "epoch:  64  total loss L:  15.479799389839172  Took  0.1  secs.\n",
      "epoch:  65  total loss L:  15.312146306037903  Took  0.1  secs.\n",
      "epoch:  66  total loss L:  15.152056217193604  Took  0.1  secs.\n",
      "epoch:  67  total loss L:  14.99930214881897  Took  0.1  secs.\n",
      "epoch:  68  total loss L:  14.853926658630371  Took  0.1  secs.\n",
      "epoch:  69  total loss L:  14.715999007225037  Took  0.1  secs.\n",
      "epoch:  70  total loss L:  14.585397481918335  Took  0.1  secs.\n",
      "epoch:  71  total loss L:  14.46177887916565  Took  0.1  secs.\n",
      "epoch:  72  total loss L:  14.344629287719727  Took  0.1  secs.\n",
      "epoch:  73  total loss L:  14.23336112499237  Took  0.1  secs.\n",
      "epoch:  74  total loss L:  14.127412557601929  Took  0.1  secs.\n",
      "epoch:  75  total loss L:  14.026367425918579  Took  0.1  secs.\n",
      "epoch:  76  total loss L:  13.930129170417786  Took  0.1  secs.\n",
      "epoch:  77  total loss L:  13.83915627002716  Took  0.1  secs.\n",
      "epoch:  78  total loss L:  13.754769325256348  Took  0.1  secs.\n",
      "epoch:  79  total loss L:  13.679503917694092  Took  0.1  secs.\n",
      "epoch:  80  total loss L:  13.61762809753418  Took  0.1  secs.\n",
      "epoch:  81  total loss L:  13.576332807540894  Took  0.1  secs.\n",
      "epoch:  82  total loss L:  13.56922459602356  Took  0.2  secs.\n",
      "epoch:  83  total loss L:  13.626575112342834  Took  0.1  secs.\n",
      "epoch:  84  total loss L:  13.824141263961792  Took  0.1  secs.\n",
      "epoch:  85  total loss L:  14.365809082984924  Took  0.1  secs.\n",
      "epoch:  86  total loss L:  15.843722105026245  Took  0.1  secs.\n",
      "epoch:  87  total loss L:  20.149396538734436  Took  0.1  secs.\n",
      "epoch:  88  total loss L:  37.97110414505005  Took  0.1  secs.\n",
      "epoch:  89  total loss L:  28.604731917381287  Took  0.1  secs.\n",
      "epoch:  90  total loss L:  33.1694712638855  Took  0.1  secs.\n",
      "epoch:  91  total loss L:  22.689486503601074  Took  0.1  secs.\n",
      "epoch:  92  total loss L:  26.544783115386963  Took  0.1  secs.\n",
      "epoch:  93  total loss L:  19.877201914787292  Took  0.1  secs.\n",
      "epoch:  94  total loss L:  18.655234694480896  Took  0.1  secs.\n",
      "epoch:  95  total loss L:  17.076440572738647  Took  0.2  secs.\n",
      "epoch:  96  total loss L:  16.490952134132385  Took  0.1  secs.\n",
      "epoch:  97  total loss L:  15.659361600875854  Took  0.1  secs.\n",
      "epoch:  98  total loss L:  15.4800306558609  Took  0.1  secs.\n",
      "epoch:  99  total loss L:  14.954049229621887  Took  0.1  secs.\n",
      "epoch:  100  total loss L:  14.919228553771973  Took  0.1  secs.\n",
      "#\n",
      "dcmf.fit - end\n",
      "dataset_name:  ds2\n",
      "#\n",
      "X0.shape:  (200, 500)\n",
      "X1.shape:  (200, 300)\n",
      "X2.shape:  (200, 400)\n",
      "X3.shape:  (700, 500)\n",
      "X4.shape:  (600, 300)\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "WARNING: The following parameters are unused since no validation data provided.\n",
      "val_metric:  auc\n",
      "at_k:  10\n",
      "is_val_transpose:  True\n",
      "#\n",
      "dCMF:\n",
      "---\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  0.0001\n",
      "weight_decay:  0.01\n",
      "convg_thres:  -0.1\n",
      "max_epochs:  100\n",
      "isPretrain:  False\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  10\n",
      "k:  100\n",
      "kf:  0.0005\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  True\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  0\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "## fold_num:  1  ##\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "WARNING: The following parameters are unused since no validation data provided.\n",
      "val_metric:  auc\n",
      "at_k:  10\n",
      "is_val_transpose:  True\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  0.0001\n",
      "weight_decay:  0.01\n",
      "convg_thres:  -0.1\n",
      "max_epochs:  100\n",
      "isPretrain:  False\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  10\n",
      "k:  100\n",
      "kf:  0.0005\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  True\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  0\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "dcmf - model construction - start\n",
      "__input_transformation - start\n",
      "#\n",
      "concatenated-matrix construction...\n",
      "e_id:  e0\n",
      "X_id_list:  ['X0', 'X1', 'X2']\n",
      "X_id:  X0\n",
      "X[X_id].shape:  (200, 500)\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (200, 300)\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (200, 400)\n",
      "C_dict[e].shape:  torch.Size([200, 1200])\n",
      "---\n",
      "e_id:  e1\n",
      "X_id_list:  ['X0', 'X3']\n",
      "X_id:  X0\n",
      "X[X_id].shape:  (200, 500)\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (700, 500)\n",
      "C_dict[e].shape:  torch.Size([500, 900])\n",
      "---\n",
      "e_id:  e2\n",
      "X_id_list:  ['X1', 'X4']\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (200, 300)\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (600, 300)\n",
      "C_dict[e].shape:  torch.Size([300, 800])\n",
      "---\n",
      "e_id:  e3\n",
      "X_id_list:  ['X2']\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (200, 400)\n",
      "C_dict[e].shape:  torch.Size([400, 200])\n",
      "---\n",
      "e_id:  e4\n",
      "X_id_list:  ['X3']\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (700, 500)\n",
      "C_dict[e].shape:  torch.Size([700, 500])\n",
      "---\n",
      "e_id:  e5\n",
      "X_id_list:  ['X4']\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (600, 300)\n",
      "C_dict[e].shape:  torch.Size([600, 300])\n",
      "---\n",
      "#\n",
      "concatenated-matrix chunking...\n",
      "#\n",
      "e_id:  e0 , min_num_datapoints:  200 , num_chunks:  10\n",
      "e_id:  e3 , min_features:  200 , k:  100\n",
      "#\n",
      "e_id:  e0  C_dict[e_id].shape:  torch.Size([200, 1200])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([20, 1200])\n",
      "---\n",
      "e_id:  e1  C_dict[e_id].shape:  torch.Size([500, 900])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([50, 900])\n",
      "---\n",
      "e_id:  e2  C_dict[e_id].shape:  torch.Size([300, 800])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([30, 800])\n",
      "---\n",
      "e_id:  e3  C_dict[e_id].shape:  torch.Size([400, 200])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([40, 200])\n",
      "---\n",
      "e_id:  e4  C_dict[e_id].shape:  torch.Size([700, 500])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([70, 500])\n",
      "---\n",
      "e_id:  e5  C_dict[e_id].shape:  torch.Size([600, 300])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([60, 300])\n",
      "---\n",
      "#\n",
      "creating pytorch variables of input matrices...\n",
      "#\n",
      "__input_transformation - end\n",
      "__network_construction - start\n",
      "__network_construction - end\n",
      "dcmf - model construction - end\n",
      "#\n",
      "#\n",
      "dcmf.fit - start\n",
      "epoch:  1  total loss L:  52.48528051376343  Took  0.1  secs.\n",
      "epoch:  2  total loss L:  40.29866337776184  Took  0.1  secs.\n",
      "epoch:  3  total loss L:  35.86401844024658  Took  0.1  secs.\n",
      "epoch:  4  total loss L:  33.33846879005432  Took  0.1  secs.\n",
      "epoch:  5  total loss L:  32.02336764335632  Took  0.1  secs.\n",
      "epoch:  6  total loss L:  31.27127456665039  Took  0.1  secs.\n",
      "epoch:  7  total loss L:  29.99609065055847  Took  0.1  secs.\n",
      "epoch:  8  total loss L:  29.34841775894165  Took  0.1  secs.\n",
      "epoch:  9  total loss L:  27.66726779937744  Took  0.1  secs.\n",
      "epoch:  10  total loss L:  26.8087055683136  Took  0.1  secs.\n",
      "epoch:  11  total loss L:  25.919933795928955  Took  0.1  secs.\n",
      "epoch:  12  total loss L:  25.177857637405396  Took  0.1  secs.\n",
      "epoch:  13  total loss L:  24.480401754379272  Took  0.1  secs.\n",
      "epoch:  14  total loss L:  23.838502407073975  Took  0.1  secs.\n",
      "epoch:  15  total loss L:  23.215596437454224  Took  0.1  secs.\n",
      "epoch:  16  total loss L:  22.609736919403076  Took  0.1  secs.\n",
      "epoch:  17  total loss L:  22.026808977127075  Took  0.1  secs.\n",
      "epoch:  18  total loss L:  21.46845245361328  Took  0.1  secs.\n",
      "epoch:  19  total loss L:  20.943037509918213  Took  0.1  secs.\n",
      "epoch:  20  total loss L:  20.484938502311707  Took  0.1  secs.\n",
      "epoch:  21  total loss L:  20.205294489860535  Took  0.1  secs.\n",
      "epoch:  22  total loss L:  20.44852387905121  Took  0.1  secs.\n",
      "epoch:  23  total loss L:  21.63527500629425  Took  0.1  secs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  24  total loss L:  22.938276290893555  Took  0.1  secs.\n",
      "epoch:  25  total loss L:  23.48695731163025  Took  0.1  secs.\n",
      "epoch:  26  total loss L:  23.155540347099304  Took  0.1  secs.\n",
      "epoch:  27  total loss L:  20.572368025779724  Took  0.1  secs.\n",
      "epoch:  28  total loss L:  20.231263637542725  Took  0.1  secs.\n",
      "epoch:  29  total loss L:  21.27287530899048  Took  0.1  secs.\n",
      "epoch:  30  total loss L:  22.04089665412903  Took  0.1  secs.\n",
      "epoch:  31  total loss L:  18.308398842811584  Took  0.1  secs.\n",
      "epoch:  32  total loss L:  17.789539098739624  Took  0.1  secs.\n",
      "epoch:  33  total loss L:  17.180480360984802  Took  0.1  secs.\n",
      "epoch:  34  total loss L:  17.053571105003357  Took  0.1  secs.\n",
      "epoch:  35  total loss L:  16.753109216690063  Took  0.1  secs.\n",
      "epoch:  36  total loss L:  16.847508549690247  Took  0.1  secs.\n",
      "epoch:  37  total loss L:  17.51456904411316  Took  0.1  secs.\n",
      "epoch:  38  total loss L:  18.23774552345276  Took  0.2  secs.\n",
      "epoch:  39  total loss L:  18.487541913986206  Took  0.1  secs.\n",
      "epoch:  40  total loss L:  19.577653527259827  Took  0.1  secs.\n",
      "epoch:  41  total loss L:  20.441261529922485  Took  0.1  secs.\n",
      "epoch:  42  total loss L:  19.564385175704956  Took  0.1  secs.\n",
      "epoch:  43  total loss L:  17.783028602600098  Took  0.1  secs.\n",
      "epoch:  44  total loss L:  17.261388063430786  Took  0.1  secs.\n",
      "epoch:  45  total loss L:  16.559756636619568  Took  0.1  secs.\n",
      "epoch:  46  total loss L:  15.927579998970032  Took  0.1  secs.\n",
      "epoch:  47  total loss L:  15.588656783103943  Took  0.1  secs.\n",
      "epoch:  48  total loss L:  15.584819793701172  Took  0.1  secs.\n",
      "epoch:  49  total loss L:  16.07275700569153  Took  0.1  secs.\n",
      "epoch:  50  total loss L:  17.20999848842621  Took  0.1  secs.\n",
      "epoch:  51  total loss L:  19.06372582912445  Took  0.1  secs.\n",
      "epoch:  52  total loss L:  19.304934859275818  Took  0.1  secs.\n",
      "epoch:  53  total loss L:  15.055346012115479  Took  0.1  secs.\n",
      "epoch:  54  total loss L:  15.430251955986023  Took  0.1  secs.\n",
      "epoch:  55  total loss L:  14.894245028495789  Took  0.1  secs.\n",
      "epoch:  56  total loss L:  15.40800940990448  Took  0.1  secs.\n",
      "epoch:  57  total loss L:  15.784045457839966  Took  0.1  secs.\n",
      "epoch:  58  total loss L:  17.79071342945099  Took  0.1  secs.\n",
      "epoch:  59  total loss L:  17.193948984146118  Took  0.1  secs.\n",
      "epoch:  60  total loss L:  16.849883437156677  Took  0.1  secs.\n",
      "epoch:  61  total loss L:  15.467061758041382  Took  0.1  secs.\n",
      "epoch:  62  total loss L:  15.14084792137146  Took  0.1  secs.\n",
      "epoch:  63  total loss L:  14.415554761886597  Took  0.1  secs.\n",
      "epoch:  64  total loss L:  14.064998269081116  Took  0.1  secs.\n",
      "epoch:  65  total loss L:  14.196811199188232  Took  0.1  secs.\n",
      "epoch:  66  total loss L:  14.17663562297821  Took  0.1  secs.\n",
      "epoch:  67  total loss L:  14.68766963481903  Took  0.1  secs.\n",
      "epoch:  68  total loss L:  14.828114628791809  Took  0.1  secs.\n",
      "epoch:  69  total loss L:  14.443492531776428  Took  0.1  secs.\n",
      "epoch:  70  total loss L:  14.404825687408447  Took  0.1  secs.\n",
      "epoch:  71  total loss L:  14.067853927612305  Took  0.1  secs.\n",
      "epoch:  72  total loss L:  14.359152436256409  Took  0.1  secs.\n",
      "epoch:  73  total loss L:  14.677878022193909  Took  0.1  secs.\n",
      "epoch:  74  total loss L:  15.23533582687378  Took  0.1  secs.\n",
      "epoch:  75  total loss L:  15.767621636390686  Took  0.1  secs.\n",
      "epoch:  76  total loss L:  16.114871978759766  Took  0.1  secs.\n",
      "epoch:  77  total loss L:  15.451135516166687  Took  0.1  secs.\n",
      "epoch:  78  total loss L:  14.663542032241821  Took  0.1  secs.\n",
      "epoch:  79  total loss L:  14.641033053398132  Took  0.1  secs.\n",
      "epoch:  80  total loss L:  13.95751702785492  Took  0.2  secs.\n",
      "epoch:  81  total loss L:  13.997339010238647  Took  0.1  secs.\n",
      "epoch:  82  total loss L:  13.663187623023987  Took  0.1  secs.\n",
      "epoch:  83  total loss L:  13.686463117599487  Took  0.1  secs.\n",
      "epoch:  84  total loss L:  13.593348979949951  Took  0.1  secs.\n",
      "epoch:  85  total loss L:  13.476581692695618  Took  0.1  secs.\n",
      "epoch:  86  total loss L:  13.495473265647888  Took  0.1  secs.\n",
      "epoch:  87  total loss L:  13.354205965995789  Took  0.1  secs.\n",
      "epoch:  88  total loss L:  13.49284040927887  Took  0.1  secs.\n",
      "epoch:  89  total loss L:  13.391520500183105  Took  0.1  secs.\n",
      "epoch:  90  total loss L:  13.616241455078125  Took  0.1  secs.\n",
      "epoch:  91  total loss L:  13.638922214508057  Took  0.1  secs.\n",
      "epoch:  92  total loss L:  13.723083257675171  Took  0.1  secs.\n",
      "epoch:  93  total loss L:  13.93745493888855  Took  0.1  secs.\n",
      "epoch:  94  total loss L:  13.98303747177124  Took  0.1  secs.\n",
      "epoch:  95  total loss L:  14.536610007286072  Took  0.1  secs.\n",
      "epoch:  96  total loss L:  14.63527500629425  Took  0.1  secs.\n",
      "epoch:  97  total loss L:  15.676497340202332  Took  0.1  secs.\n",
      "epoch:  98  total loss L:  17.222681760787964  Took  0.1  secs.\n",
      "epoch:  99  total loss L:  21.76046347618103  Took  0.1  secs.\n",
      "epoch:  100  total loss L:  25.842803359031677  Took  0.1  secs.\n",
      "#\n",
      "dcmf.fit - end\n",
      "dataset_name:  ds3\n",
      "#\n",
      "X0.shape:  (200, 500)\n",
      "X1.shape:  (200, 300)\n",
      "X2.shape:  (200, 400)\n",
      "X3.shape:  (700, 500)\n",
      "X4.shape:  (600, 300)\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "WARNING: The following parameters are unused since no validation data provided.\n",
      "val_metric:  auc\n",
      "at_k:  10\n",
      "is_val_transpose:  True\n",
      "#\n",
      "dCMF:\n",
      "---\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  0.0001\n",
      "weight_decay:  0.01\n",
      "convg_thres:  -0.1\n",
      "max_epochs:  100\n",
      "isPretrain:  False\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  10\n",
      "k:  100\n",
      "kf:  0.0005\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  True\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  0\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "## fold_num:  1  ##\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "WARNING: The following parameters are unused since no validation data provided.\n",
      "val_metric:  auc\n",
      "at_k:  10\n",
      "is_val_transpose:  True\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  0.0001\n",
      "weight_decay:  0.01\n",
      "convg_thres:  -0.1\n",
      "max_epochs:  100\n",
      "isPretrain:  False\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  10\n",
      "k:  100\n",
      "kf:  0.0005\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  True\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  0\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "dcmf - model construction - start\n",
      "__input_transformation - start\n",
      "#\n",
      "concatenated-matrix construction...\n",
      "e_id:  e0\n",
      "X_id_list:  ['X0', 'X1', 'X2']\n",
      "X_id:  X0\n",
      "X[X_id].shape:  (200, 500)\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (200, 300)\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (200, 400)\n",
      "C_dict[e].shape:  torch.Size([200, 1200])\n",
      "---\n",
      "e_id:  e1\n",
      "X_id_list:  ['X0', 'X3']\n",
      "X_id:  X0\n",
      "X[X_id].shape:  (200, 500)\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (700, 500)\n",
      "C_dict[e].shape:  torch.Size([500, 900])\n",
      "---\n",
      "e_id:  e2\n",
      "X_id_list:  ['X1', 'X4']\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (200, 300)\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (600, 300)\n",
      "C_dict[e].shape:  torch.Size([300, 800])\n",
      "---\n",
      "e_id:  e3\n",
      "X_id_list:  ['X2']\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (200, 400)\n",
      "C_dict[e].shape:  torch.Size([400, 200])\n",
      "---\n",
      "e_id:  e4\n",
      "X_id_list:  ['X3']\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (700, 500)\n",
      "C_dict[e].shape:  torch.Size([700, 500])\n",
      "---\n",
      "e_id:  e5\n",
      "X_id_list:  ['X4']\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (600, 300)\n",
      "C_dict[e].shape:  torch.Size([600, 300])\n",
      "---\n",
      "#\n",
      "concatenated-matrix chunking...\n",
      "#\n",
      "e_id:  e0 , min_num_datapoints:  200 , num_chunks:  10\n",
      "e_id:  e3 , min_features:  200 , k:  100\n",
      "#\n",
      "e_id:  e0  C_dict[e_id].shape:  torch.Size([200, 1200])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([20, 1200])\n",
      "---\n",
      "e_id:  e1  C_dict[e_id].shape:  torch.Size([500, 900])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([50, 900])\n",
      "---\n",
      "e_id:  e2  C_dict[e_id].shape:  torch.Size([300, 800])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([30, 800])\n",
      "---\n",
      "e_id:  e3  C_dict[e_id].shape:  torch.Size([400, 200])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([40, 200])\n",
      "---\n",
      "e_id:  e4  C_dict[e_id].shape:  torch.Size([700, 500])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([70, 500])\n",
      "---\n",
      "e_id:  e5  C_dict[e_id].shape:  torch.Size([600, 300])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([60, 300])\n",
      "---\n",
      "#\n",
      "creating pytorch variables of input matrices...\n",
      "#\n",
      "__input_transformation - end\n",
      "__network_construction - start\n",
      "__network_construction - end\n",
      "dcmf - model construction - end\n",
      "#\n",
      "#\n",
      "dcmf.fit - start\n",
      "epoch:  1  total loss L:  53.45456600189209  Took  0.1  secs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  2  total loss L:  38.244521379470825  Took  0.1  secs.\n",
      "epoch:  3  total loss L:  33.68312406539917  Took  0.1  secs.\n",
      "epoch:  4  total loss L:  30.689249277114868  Took  0.1  secs.\n",
      "epoch:  5  total loss L:  29.135556936264038  Took  0.1  secs.\n",
      "epoch:  6  total loss L:  28.031147003173828  Took  0.1  secs.\n",
      "epoch:  7  total loss L:  27.41258192062378  Took  0.1  secs.\n",
      "epoch:  8  total loss L:  27.276609182357788  Took  0.1  secs.\n",
      "epoch:  9  total loss L:  27.85636830329895  Took  0.1  secs.\n",
      "epoch:  10  total loss L:  27.205251693725586  Took  0.1  secs.\n",
      "epoch:  11  total loss L:  25.632866621017456  Took  0.1  secs.\n",
      "epoch:  12  total loss L:  23.92732858657837  Took  0.1  secs.\n",
      "epoch:  13  total loss L:  22.978994131088257  Took  0.1  secs.\n",
      "epoch:  14  total loss L:  22.20632576942444  Took  0.2  secs.\n",
      "epoch:  15  total loss L:  21.59468150138855  Took  0.1  secs.\n",
      "epoch:  16  total loss L:  21.059901237487793  Took  0.1  secs.\n",
      "epoch:  17  total loss L:  20.54308831691742  Took  0.1  secs.\n",
      "epoch:  18  total loss L:  20.036229968070984  Took  0.1  secs.\n",
      "epoch:  19  total loss L:  19.545817971229553  Took  0.1  secs.\n",
      "epoch:  20  total loss L:  19.066718816757202  Took  0.1  secs.\n",
      "epoch:  21  total loss L:  18.602744817733765  Took  0.1  secs.\n",
      "epoch:  22  total loss L:  18.15180492401123  Took  0.1  secs.\n",
      "epoch:  23  total loss L:  17.724788904190063  Took  0.1  secs.\n",
      "epoch:  24  total loss L:  17.32931137084961  Took  0.1  secs.\n",
      "epoch:  25  total loss L:  16.986944437026978  Took  0.1  secs.\n",
      "epoch:  26  total loss L:  16.78057038784027  Took  0.1  secs.\n",
      "epoch:  27  total loss L:  16.922584414482117  Took  0.1  secs.\n",
      "epoch:  28  total loss L:  17.94802713394165  Took  0.1  secs.\n",
      "epoch:  29  total loss L:  21.45718026161194  Took  0.1  secs.\n",
      "epoch:  30  total loss L:  22.496716737747192  Took  0.1  secs.\n",
      "epoch:  31  total loss L:  21.053319931030273  Took  0.1  secs.\n",
      "epoch:  32  total loss L:  18.690267324447632  Took  0.1  secs.\n",
      "epoch:  33  total loss L:  18.745663166046143  Took  0.1  secs.\n",
      "epoch:  34  total loss L:  20.15970265865326  Took  0.2  secs.\n",
      "epoch:  35  total loss L:  20.242576837539673  Took  0.1  secs.\n",
      "epoch:  36  total loss L:  17.348739862442017  Took  0.1  secs.\n",
      "epoch:  37  total loss L:  15.93664813041687  Took  0.1  secs.\n",
      "epoch:  38  total loss L:  15.618367552757263  Took  0.1  secs.\n",
      "epoch:  39  total loss L:  15.542861938476562  Took  0.1  secs.\n",
      "epoch:  40  total loss L:  15.539899587631226  Took  0.1  secs.\n",
      "epoch:  41  total loss L:  15.954760789871216  Took  0.1  secs.\n",
      "epoch:  42  total loss L:  16.70238995552063  Took  0.1  secs.\n",
      "epoch:  43  total loss L:  16.902021408081055  Took  0.1  secs.\n",
      "epoch:  44  total loss L:  14.866647958755493  Took  0.1  secs.\n",
      "epoch:  45  total loss L:  14.452276945114136  Took  0.1  secs.\n",
      "epoch:  46  total loss L:  14.232335686683655  Took  0.1  secs.\n",
      "epoch:  47  total loss L:  14.0210622549057  Took  0.1  secs.\n",
      "epoch:  48  total loss L:  14.241547346115112  Took  0.1  secs.\n",
      "epoch:  49  total loss L:  15.407726645469666  Took  0.1  secs.\n",
      "epoch:  50  total loss L:  18.965678095817566  Took  0.1  secs.\n",
      "epoch:  51  total loss L:  19.758215188980103  Took  0.1  secs.\n",
      "epoch:  52  total loss L:  21.125789642333984  Took  0.1  secs.\n",
      "epoch:  53  total loss L:  18.368731260299683  Took  0.1  secs.\n",
      "epoch:  54  total loss L:  17.73877716064453  Took  0.1  secs.\n",
      "epoch:  55  total loss L:  17.50782585144043  Took  0.1  secs.\n",
      "epoch:  56  total loss L:  15.676059126853943  Took  0.1  secs.\n",
      "epoch:  57  total loss L:  15.056514024734497  Took  0.1  secs.\n",
      "epoch:  58  total loss L:  14.190398693084717  Took  0.1  secs.\n",
      "epoch:  59  total loss L:  14.58988344669342  Took  0.1  secs.\n",
      "epoch:  60  total loss L:  15.427902340888977  Took  0.1  secs.\n",
      "epoch:  61  total loss L:  17.07149064540863  Took  0.1  secs.\n",
      "epoch:  62  total loss L:  18.38584840297699  Took  0.1  secs.\n",
      "epoch:  63  total loss L:  14.708752036094666  Took  0.1  secs.\n",
      "epoch:  64  total loss L:  13.078218340873718  Took  0.1  secs.\n",
      "epoch:  65  total loss L:  13.06736671924591  Took  0.1  secs.\n",
      "epoch:  66  total loss L:  12.693424105644226  Took  0.1  secs.\n",
      "epoch:  67  total loss L:  12.751933574676514  Took  0.1  secs.\n",
      "epoch:  68  total loss L:  12.831057667732239  Took  0.1  secs.\n",
      "epoch:  69  total loss L:  13.20498788356781  Took  0.1  secs.\n",
      "epoch:  70  total loss L:  13.850918412208557  Took  0.1  secs.\n",
      "epoch:  71  total loss L:  14.659749507904053  Took  0.1  secs.\n",
      "epoch:  72  total loss L:  16.40543031692505  Took  0.1  secs.\n",
      "epoch:  73  total loss L:  18.018438577651978  Took  0.1  secs.\n",
      "epoch:  74  total loss L:  18.010226845741272  Took  0.1  secs.\n",
      "epoch:  75  total loss L:  19.593436360359192  Took  0.1  secs.\n",
      "epoch:  76  total loss L:  17.184470534324646  Took  0.1  secs.\n",
      "epoch:  77  total loss L:  16.30968725681305  Took  0.1  secs.\n",
      "epoch:  78  total loss L:  15.914214372634888  Took  0.2  secs.\n",
      "epoch:  79  total loss L:  14.10345208644867  Took  0.1  secs.\n",
      "epoch:  80  total loss L:  12.954973340034485  Took  0.1  secs.\n",
      "epoch:  81  total loss L:  12.567146301269531  Took  0.1  secs.\n",
      "epoch:  82  total loss L:  12.38054633140564  Took  0.1  secs.\n",
      "epoch:  83  total loss L:  12.15178096294403  Took  0.1  secs.\n",
      "epoch:  84  total loss L:  12.005128026008606  Took  0.1  secs.\n",
      "epoch:  85  total loss L:  11.936329007148743  Took  0.1  secs.\n",
      "epoch:  86  total loss L:  11.880991101264954  Took  0.1  secs.\n",
      "epoch:  87  total loss L:  11.850364327430725  Took  0.1  secs.\n",
      "epoch:  88  total loss L:  11.845881223678589  Took  0.1  secs.\n",
      "epoch:  89  total loss L:  11.863725423812866  Took  0.1  secs.\n",
      "epoch:  90  total loss L:  11.915775299072266  Took  0.1  secs.\n",
      "epoch:  91  total loss L:  12.04247522354126  Took  0.1  secs.\n",
      "epoch:  92  total loss L:  12.280983090400696  Took  0.1  secs.\n",
      "epoch:  93  total loss L:  12.684576749801636  Took  0.1  secs.\n",
      "epoch:  94  total loss L:  13.428964972496033  Took  0.1  secs.\n",
      "epoch:  95  total loss L:  14.166484713554382  Took  0.1  secs.\n",
      "epoch:  96  total loss L:  14.467703223228455  Took  0.1  secs.\n",
      "epoch:  97  total loss L:  14.866412878036499  Took  0.1  secs.\n",
      "epoch:  98  total loss L:  15.562601089477539  Took  0.1  secs.\n",
      "epoch:  99  total loss L:  17.800576210021973  Took  0.1  secs.\n",
      "epoch:  100  total loss L:  23.32239079475403  Took  0.1  secs.\n",
      "#\n",
      "dcmf.fit - end\n",
      "dataset_name:  dn1\n",
      "#\n",
      "X0.shape:  (200, 500)\n",
      "X1.shape:  (200, 300)\n",
      "X2.shape:  (200, 400)\n",
      "X3.shape:  (700, 500)\n",
      "X4.shape:  (600, 300)\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "WARNING: The following parameters are unused since no validation data provided.\n",
      "val_metric:  auc\n",
      "at_k:  10\n",
      "is_val_transpose:  True\n",
      "#\n",
      "dCMF:\n",
      "---\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  0.0001\n",
      "weight_decay:  0.01\n",
      "convg_thres:  -0.1\n",
      "max_epochs:  100\n",
      "isPretrain:  False\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  10\n",
      "k:  100\n",
      "kf:  0.0005\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  True\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  0\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "## fold_num:  1  ##\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "WARNING: The following parameters are unused since no validation data provided.\n",
      "val_metric:  auc\n",
      "at_k:  10\n",
      "is_val_transpose:  True\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  0.0001\n",
      "weight_decay:  0.01\n",
      "convg_thres:  -0.1\n",
      "max_epochs:  100\n",
      "isPretrain:  False\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  10\n",
      "k:  100\n",
      "kf:  0.0005\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  True\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  0\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "dcmf - model construction - start\n",
      "__input_transformation - start\n",
      "#\n",
      "concatenated-matrix construction...\n",
      "e_id:  e0\n",
      "X_id_list:  ['X0', 'X1', 'X2']\n",
      "X_id:  X0\n",
      "X[X_id].shape:  (200, 500)\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (200, 300)\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (200, 400)\n",
      "C_dict[e].shape:  torch.Size([200, 1200])\n",
      "---\n",
      "e_id:  e1\n",
      "X_id_list:  ['X0', 'X3']\n",
      "X_id:  X0\n",
      "X[X_id].shape:  (200, 500)\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (700, 500)\n",
      "C_dict[e].shape:  torch.Size([500, 900])\n",
      "---\n",
      "e_id:  e2\n",
      "X_id_list:  ['X1', 'X4']\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (200, 300)\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (600, 300)\n",
      "C_dict[e].shape:  torch.Size([300, 800])\n",
      "---\n",
      "e_id:  e3\n",
      "X_id_list:  ['X2']\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (200, 400)\n",
      "C_dict[e].shape:  torch.Size([400, 200])\n",
      "---\n",
      "e_id:  e4\n",
      "X_id_list:  ['X3']\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (700, 500)\n",
      "C_dict[e].shape:  torch.Size([700, 500])\n",
      "---\n",
      "e_id:  e5\n",
      "X_id_list:  ['X4']\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (600, 300)\n",
      "C_dict[e].shape:  torch.Size([600, 300])\n",
      "---\n",
      "#\n",
      "concatenated-matrix chunking...\n",
      "#\n",
      "e_id:  e0 , min_num_datapoints:  200 , num_chunks:  10\n",
      "e_id:  e3 , min_features:  200 , k:  100\n",
      "#\n",
      "e_id:  e0  C_dict[e_id].shape:  torch.Size([200, 1200])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([20, 1200])\n",
      "---\n",
      "e_id:  e1  C_dict[e_id].shape:  torch.Size([500, 900])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([50, 900])\n",
      "---\n",
      "e_id:  e2  C_dict[e_id].shape:  torch.Size([300, 800])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([30, 800])\n",
      "---\n",
      "e_id:  e3  C_dict[e_id].shape:  torch.Size([400, 200])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([40, 200])\n",
      "---\n",
      "e_id:  e4  C_dict[e_id].shape:  torch.Size([700, 500])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([70, 500])\n",
      "---\n",
      "e_id:  e5  C_dict[e_id].shape:  torch.Size([600, 300])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([60, 300])\n",
      "---\n",
      "#\n",
      "creating pytorch variables of input matrices...\n",
      "#\n",
      "__input_transformation - end\n",
      "__network_construction - start\n",
      "__network_construction - end\n",
      "dcmf - model construction - end\n",
      "#\n",
      "#\n",
      "dcmf.fit - start\n",
      "epoch:  1  total loss L:  77.57312536239624  Took  0.1  secs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  2  total loss L:  56.24999284744263  Took  0.1  secs.\n",
      "epoch:  3  total loss L:  48.18857479095459  Took  0.1  secs.\n",
      "epoch:  4  total loss L:  43.77168321609497  Took  0.1  secs.\n",
      "epoch:  5  total loss L:  41.092777729034424  Took  0.1  secs.\n",
      "epoch:  6  total loss L:  39.4577317237854  Took  0.1  secs.\n",
      "epoch:  7  total loss L:  38.123000144958496  Took  0.1  secs.\n",
      "epoch:  8  total loss L:  37.157015323638916  Took  0.1  secs.\n",
      "epoch:  9  total loss L:  36.324572801589966  Took  0.1  secs.\n",
      "epoch:  10  total loss L:  35.560038566589355  Took  0.1  secs.\n",
      "epoch:  11  total loss L:  34.848777532577515  Took  0.1  secs.\n",
      "epoch:  12  total loss L:  34.18915581703186  Took  0.1  secs.\n",
      "epoch:  13  total loss L:  33.68381667137146  Took  0.2  secs.\n",
      "epoch:  14  total loss L:  33.3653564453125  Took  0.1  secs.\n",
      "epoch:  15  total loss L:  33.11101150512695  Took  0.1  secs.\n",
      "epoch:  16  total loss L:  32.753657817840576  Took  0.1  secs.\n",
      "epoch:  17  total loss L:  31.740914821624756  Took  0.1  secs.\n",
      "epoch:  18  total loss L:  30.356104373931885  Took  0.1  secs.\n",
      "epoch:  19  total loss L:  29.58938217163086  Took  0.1  secs.\n",
      "epoch:  20  total loss L:  28.786609411239624  Took  0.1  secs.\n",
      "epoch:  21  total loss L:  28.00332760810852  Took  0.1  secs.\n",
      "epoch:  22  total loss L:  27.40440797805786  Took  0.1  secs.\n",
      "epoch:  23  total loss L:  26.923138856887817  Took  0.1  secs.\n",
      "epoch:  24  total loss L:  26.442073583602905  Took  0.1  secs.\n",
      "epoch:  25  total loss L:  25.92620849609375  Took  0.2  secs.\n",
      "epoch:  26  total loss L:  25.43915295600891  Took  0.1  secs.\n",
      "epoch:  27  total loss L:  25.044720888137817  Took  0.1  secs.\n",
      "epoch:  28  total loss L:  24.792831897735596  Took  0.1  secs.\n",
      "epoch:  29  total loss L:  24.89387559890747  Took  0.1  secs.\n",
      "epoch:  30  total loss L:  26.241708278656006  Took  0.1  secs.\n",
      "epoch:  31  total loss L:  33.066535234451294  Took  0.1  secs.\n",
      "epoch:  32  total loss L:  44.15293264389038  Took  0.1  secs.\n",
      "epoch:  33  total loss L:  48.90529489517212  Took  0.1  secs.\n",
      "epoch:  34  total loss L:  32.306331396102905  Took  0.1  secs.\n",
      "epoch:  35  total loss L:  30.615521907806396  Took  0.1  secs.\n",
      "epoch:  36  total loss L:  27.54509711265564  Took  0.2  secs.\n",
      "epoch:  37  total loss L:  25.677618980407715  Took  0.1  secs.\n",
      "epoch:  38  total loss L:  24.326475620269775  Took  0.1  secs.\n",
      "epoch:  39  total loss L:  23.777587175369263  Took  0.1  secs.\n",
      "epoch:  40  total loss L:  23.34962296485901  Took  0.1  secs.\n",
      "epoch:  41  total loss L:  22.882717609405518  Took  0.1  secs.\n",
      "epoch:  42  total loss L:  22.36690592765808  Took  0.1  secs.\n",
      "epoch:  43  total loss L:  21.717103719711304  Took  0.1  secs.\n",
      "epoch:  44  total loss L:  21.490689992904663  Took  0.1  secs.\n",
      "epoch:  45  total loss L:  21.097453832626343  Took  0.1  secs.\n",
      "epoch:  46  total loss L:  20.797741532325745  Took  0.1  secs.\n",
      "epoch:  47  total loss L:  20.533344864845276  Took  0.1  secs.\n",
      "epoch:  48  total loss L:  20.271010875701904  Took  0.2  secs.\n",
      "epoch:  49  total loss L:  20.137843132019043  Took  0.1  secs.\n",
      "epoch:  50  total loss L:  20.112897872924805  Took  0.1  secs.\n",
      "epoch:  51  total loss L:  20.389451026916504  Took  0.1  secs.\n",
      "epoch:  52  total loss L:  21.362932205200195  Took  0.1  secs.\n",
      "epoch:  53  total loss L:  24.76295566558838  Took  0.1  secs.\n",
      "epoch:  54  total loss L:  24.146352529525757  Took  0.1  secs.\n",
      "epoch:  55  total loss L:  24.657521963119507  Took  0.1  secs.\n",
      "epoch:  56  total loss L:  28.323932647705078  Took  0.1  secs.\n",
      "epoch:  57  total loss L:  26.98173999786377  Took  0.1  secs.\n",
      "epoch:  58  total loss L:  25.86036968231201  Took  0.1  secs.\n",
      "epoch:  59  total loss L:  23.43415856361389  Took  0.1  secs.\n",
      "epoch:  60  total loss L:  21.483319759368896  Took  0.1  secs.\n",
      "epoch:  61  total loss L:  20.853931665420532  Took  0.1  secs.\n",
      "epoch:  62  total loss L:  20.819199323654175  Took  0.1  secs.\n",
      "epoch:  63  total loss L:  20.877034068107605  Took  0.1  secs.\n",
      "epoch:  64  total loss L:  21.507449865341187  Took  0.1  secs.\n",
      "epoch:  65  total loss L:  22.82584524154663  Took  0.1  secs.\n",
      "epoch:  66  total loss L:  25.83113980293274  Took  0.1  secs.\n",
      "epoch:  67  total loss L:  33.1947455406189  Took  0.1  secs.\n",
      "epoch:  68  total loss L:  36.446513175964355  Took  0.1  secs.\n",
      "epoch:  69  total loss L:  26.596792936325073  Took  0.1  secs.\n",
      "epoch:  70  total loss L:  22.100757122039795  Took  0.1  secs.\n",
      "epoch:  71  total loss L:  21.791508078575134  Took  0.1  secs.\n",
      "epoch:  72  total loss L:  22.089909315109253  Took  0.1  secs.\n",
      "epoch:  73  total loss L:  21.156876921653748  Took  0.1  secs.\n",
      "epoch:  74  total loss L:  21.92629301548004  Took  0.1  secs.\n",
      "epoch:  75  total loss L:  20.785353422164917  Took  0.1  secs.\n",
      "epoch:  76  total loss L:  19.558425545692444  Took  0.1  secs.\n",
      "epoch:  77  total loss L:  18.63872003555298  Took  0.1  secs.\n",
      "epoch:  78  total loss L:  18.230202198028564  Took  0.1  secs.\n",
      "epoch:  79  total loss L:  17.812739729881287  Took  0.1  secs.\n",
      "epoch:  80  total loss L:  17.621803045272827  Took  0.1  secs.\n",
      "epoch:  81  total loss L:  17.565551280975342  Took  0.1  secs.\n",
      "epoch:  82  total loss L:  17.637075543403625  Took  0.1  secs.\n",
      "epoch:  83  total loss L:  17.872718811035156  Took  0.1  secs.\n",
      "epoch:  84  total loss L:  18.86738657951355  Took  0.1  secs.\n",
      "epoch:  85  total loss L:  20.43960177898407  Took  0.1  secs.\n",
      "epoch:  86  total loss L:  20.775527119636536  Took  0.2  secs.\n",
      "epoch:  87  total loss L:  22.254686951637268  Took  0.1  secs.\n",
      "epoch:  88  total loss L:  19.77917766571045  Took  0.2  secs.\n",
      "epoch:  89  total loss L:  21.390833854675293  Took  0.1  secs.\n",
      "epoch:  90  total loss L:  19.487516164779663  Took  0.1  secs.\n",
      "epoch:  91  total loss L:  18.81461775302887  Took  0.1  secs.\n",
      "epoch:  92  total loss L:  17.421391487121582  Took  0.1  secs.\n",
      "epoch:  93  total loss L:  17.163496017456055  Took  0.2  secs.\n",
      "epoch:  94  total loss L:  17.217654705047607  Took  0.1  secs.\n",
      "epoch:  95  total loss L:  17.529035091400146  Took  0.1  secs.\n",
      "epoch:  96  total loss L:  18.05281364917755  Took  0.1  secs.\n",
      "epoch:  97  total loss L:  18.56229829788208  Took  0.1  secs.\n",
      "epoch:  98  total loss L:  19.527881622314453  Took  0.1  secs.\n",
      "epoch:  99  total loss L:  19.116721153259277  Took  0.1  secs.\n",
      "epoch:  100  total loss L:  18.19064688682556  Took  0.1  secs.\n",
      "#\n",
      "dcmf.fit - end\n",
      "dataset_name:  dn2\n",
      "#\n",
      "X0.shape:  (200, 500)\n",
      "X1.shape:  (200, 300)\n",
      "X2.shape:  (200, 400)\n",
      "X3.shape:  (700, 500)\n",
      "X4.shape:  (600, 300)\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "WARNING: The following parameters are unused since no validation data provided.\n",
      "val_metric:  auc\n",
      "at_k:  10\n",
      "is_val_transpose:  True\n",
      "#\n",
      "dCMF:\n",
      "---\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  0.0001\n",
      "weight_decay:  0.01\n",
      "convg_thres:  -0.1\n",
      "max_epochs:  100\n",
      "isPretrain:  False\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  10\n",
      "k:  100\n",
      "kf:  0.0005\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  True\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  0\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "## fold_num:  1  ##\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "WARNING: The following parameters are unused since no validation data provided.\n",
      "val_metric:  auc\n",
      "at_k:  10\n",
      "is_val_transpose:  True\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  0.0001\n",
      "weight_decay:  0.01\n",
      "convg_thres:  -0.1\n",
      "max_epochs:  100\n",
      "isPretrain:  False\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  10\n",
      "k:  100\n",
      "kf:  0.0005\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  True\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  0\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "dcmf - model construction - start\n",
      "__input_transformation - start\n",
      "#\n",
      "concatenated-matrix construction...\n",
      "e_id:  e0\n",
      "X_id_list:  ['X0', 'X1', 'X2']\n",
      "X_id:  X0\n",
      "X[X_id].shape:  (200, 500)\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (200, 300)\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (200, 400)\n",
      "C_dict[e].shape:  torch.Size([200, 1200])\n",
      "---\n",
      "e_id:  e1\n",
      "X_id_list:  ['X0', 'X3']\n",
      "X_id:  X0\n",
      "X[X_id].shape:  (200, 500)\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (700, 500)\n",
      "C_dict[e].shape:  torch.Size([500, 900])\n",
      "---\n",
      "e_id:  e2\n",
      "X_id_list:  ['X1', 'X4']\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (200, 300)\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (600, 300)\n",
      "C_dict[e].shape:  torch.Size([300, 800])\n",
      "---\n",
      "e_id:  e3\n",
      "X_id_list:  ['X2']\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (200, 400)\n",
      "C_dict[e].shape:  torch.Size([400, 200])\n",
      "---\n",
      "e_id:  e4\n",
      "X_id_list:  ['X3']\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (700, 500)\n",
      "C_dict[e].shape:  torch.Size([700, 500])\n",
      "---\n",
      "e_id:  e5\n",
      "X_id_list:  ['X4']\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (600, 300)\n",
      "C_dict[e].shape:  torch.Size([600, 300])\n",
      "---\n",
      "#\n",
      "concatenated-matrix chunking...\n",
      "#\n",
      "e_id:  e0 , min_num_datapoints:  200 , num_chunks:  10\n",
      "e_id:  e3 , min_features:  200 , k:  100\n",
      "#\n",
      "e_id:  e0  C_dict[e_id].shape:  torch.Size([200, 1200])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([20, 1200])\n",
      "---\n",
      "e_id:  e1  C_dict[e_id].shape:  torch.Size([500, 900])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([50, 900])\n",
      "---\n",
      "e_id:  e2  C_dict[e_id].shape:  torch.Size([300, 800])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([30, 800])\n",
      "---\n",
      "e_id:  e3  C_dict[e_id].shape:  torch.Size([400, 200])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([40, 200])\n",
      "---\n",
      "e_id:  e4  C_dict[e_id].shape:  torch.Size([700, 500])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([70, 500])\n",
      "---\n",
      "e_id:  e5  C_dict[e_id].shape:  torch.Size([600, 300])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([60, 300])\n",
      "---\n",
      "#\n",
      "creating pytorch variables of input matrices...\n",
      "#\n",
      "__input_transformation - end\n",
      "__network_construction - start\n",
      "__network_construction - end\n",
      "dcmf - model construction - end\n",
      "#\n",
      "#\n",
      "dcmf.fit - start\n",
      "epoch:  1  total loss L:  75.25205135345459  Took  0.1  secs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  2  total loss L:  59.09098958969116  Took  0.1  secs.\n",
      "epoch:  3  total loss L:  51.915940284729004  Took  0.1  secs.\n",
      "epoch:  4  total loss L:  47.907501220703125  Took  0.1  secs.\n",
      "epoch:  5  total loss L:  45.387736320495605  Took  0.1  secs.\n",
      "epoch:  6  total loss L:  43.69818449020386  Took  0.1  secs.\n",
      "epoch:  7  total loss L:  42.41418123245239  Took  0.1  secs.\n",
      "epoch:  8  total loss L:  41.28723382949829  Took  0.1  secs.\n",
      "epoch:  9  total loss L:  40.28891563415527  Took  0.1  secs.\n",
      "epoch:  10  total loss L:  39.353976011276245  Took  0.1  secs.\n",
      "epoch:  11  total loss L:  38.51643204689026  Took  0.1  secs.\n",
      "epoch:  12  total loss L:  37.79183268547058  Took  0.2  secs.\n",
      "epoch:  13  total loss L:  37.27522039413452  Took  0.1  secs.\n",
      "epoch:  14  total loss L:  37.157134771347046  Took  0.1  secs.\n",
      "epoch:  15  total loss L:  37.8447470664978  Took  0.1  secs.\n",
      "epoch:  16  total loss L:  40.95951294898987  Took  0.1  secs.\n",
      "epoch:  17  total loss L:  43.51673340797424  Took  0.2  secs.\n",
      "epoch:  18  total loss L:  37.835012912750244  Took  0.1  secs.\n",
      "epoch:  19  total loss L:  35.83844614028931  Took  0.1  secs.\n",
      "epoch:  20  total loss L:  34.700279235839844  Took  0.1  secs.\n",
      "epoch:  21  total loss L:  33.98924136161804  Took  0.1  secs.\n",
      "epoch:  22  total loss L:  35.843820571899414  Took  0.1  secs.\n",
      "epoch:  23  total loss L:  43.97919750213623  Took  0.1  secs.\n",
      "epoch:  24  total loss L:  49.090458154678345  Took  0.1  secs.\n",
      "epoch:  25  total loss L:  43.457282304763794  Took  0.1  secs.\n",
      "epoch:  26  total loss L:  35.50381541252136  Took  0.1  secs.\n",
      "epoch:  27  total loss L:  33.86130428314209  Took  0.1  secs.\n",
      "epoch:  28  total loss L:  32.71954679489136  Took  0.1  secs.\n",
      "epoch:  29  total loss L:  31.76711678504944  Took  0.1  secs.\n",
      "epoch:  30  total loss L:  31.127933025360107  Took  0.1  secs.\n",
      "epoch:  31  total loss L:  30.36734962463379  Took  0.1  secs.\n",
      "epoch:  32  total loss L:  29.749452590942383  Took  0.1  secs.\n",
      "epoch:  33  total loss L:  29.166852951049805  Took  0.1  secs.\n",
      "epoch:  34  total loss L:  28.609444618225098  Took  0.1  secs.\n",
      "epoch:  35  total loss L:  28.094136476516724  Took  0.1  secs.\n",
      "epoch:  36  total loss L:  27.597087621688843  Took  0.1  secs.\n",
      "epoch:  37  total loss L:  27.1414897441864  Took  0.1  secs.\n",
      "epoch:  38  total loss L:  26.718698978424072  Took  0.1  secs.\n",
      "epoch:  39  total loss L:  26.33880352973938  Took  0.1  secs.\n",
      "epoch:  40  total loss L:  26.014806032180786  Took  0.1  secs.\n",
      "epoch:  41  total loss L:  25.777986526489258  Took  0.1  secs.\n",
      "epoch:  42  total loss L:  25.731014013290405  Took  0.1  secs.\n",
      "epoch:  43  total loss L:  26.172083616256714  Took  0.2  secs.\n",
      "epoch:  44  total loss L:  27.177738666534424  Took  0.1  secs.\n",
      "epoch:  45  total loss L:  29.029394388198853  Took  0.1  secs.\n",
      "epoch:  46  total loss L:  26.19978356361389  Took  0.1  secs.\n",
      "epoch:  47  total loss L:  25.337889671325684  Took  0.1  secs.\n",
      "epoch:  48  total loss L:  25.134138345718384  Took  0.1  secs.\n",
      "epoch:  49  total loss L:  25.142194271087646  Took  0.1  secs.\n",
      "epoch:  50  total loss L:  25.715712308883667  Took  0.1  secs.\n",
      "epoch:  51  total loss L:  27.305448532104492  Took  0.1  secs.\n",
      "epoch:  52  total loss L:  31.24695873260498  Took  0.1  secs.\n",
      "epoch:  53  total loss L:  36.431782960891724  Took  0.1  secs.\n",
      "epoch:  54  total loss L:  41.926668643951416  Took  0.1  secs.\n",
      "epoch:  55  total loss L:  31.125964164733887  Took  0.1  secs.\n",
      "epoch:  56  total loss L:  27.26351499557495  Took  0.1  secs.\n",
      "epoch:  57  total loss L:  25.770963668823242  Took  0.1  secs.\n",
      "epoch:  58  total loss L:  26.421459436416626  Took  0.1  secs.\n",
      "epoch:  59  total loss L:  27.651267528533936  Took  0.1  secs.\n",
      "epoch:  60  total loss L:  30.811173677444458  Took  0.1  secs.\n",
      "epoch:  61  total loss L:  32.83597755432129  Took  0.2  secs.\n",
      "epoch:  62  total loss L:  31.95912456512451  Took  0.1  secs.\n",
      "epoch:  63  total loss L:  33.0341112613678  Took  0.1  secs.\n",
      "epoch:  64  total loss L:  32.47822093963623  Took  0.1  secs.\n",
      "epoch:  65  total loss L:  35.623026847839355  Took  0.1  secs.\n",
      "epoch:  66  total loss L:  27.468164443969727  Took  0.1  secs.\n",
      "epoch:  67  total loss L:  27.09969925880432  Took  0.1  secs.\n",
      "epoch:  68  total loss L:  24.308810472488403  Took  0.1  secs.\n",
      "epoch:  69  total loss L:  22.592965841293335  Took  0.1  secs.\n",
      "epoch:  70  total loss L:  21.932438611984253  Took  0.1  secs.\n",
      "epoch:  71  total loss L:  21.6510751247406  Took  0.1  secs.\n",
      "epoch:  72  total loss L:  21.724060773849487  Took  0.1  secs.\n",
      "epoch:  73  total loss L:  22.075308561325073  Took  0.1  secs.\n",
      "epoch:  74  total loss L:  22.770793437957764  Took  0.1  secs.\n",
      "epoch:  75  total loss L:  23.4766948223114  Took  0.1  secs.\n",
      "epoch:  76  total loss L:  23.471725702285767  Took  0.1  secs.\n",
      "epoch:  77  total loss L:  24.042417526245117  Took  0.1  secs.\n",
      "epoch:  78  total loss L:  23.60207724571228  Took  0.1  secs.\n",
      "epoch:  79  total loss L:  23.316268920898438  Took  0.1  secs.\n",
      "epoch:  80  total loss L:  23.743611097335815  Took  0.1  secs.\n",
      "epoch:  81  total loss L:  24.341347217559814  Took  0.1  secs.\n",
      "epoch:  82  total loss L:  22.712520360946655  Took  0.1  secs.\n",
      "epoch:  83  total loss L:  21.68218755722046  Took  0.2  secs.\n",
      "epoch:  84  total loss L:  20.54660964012146  Took  0.1  secs.\n",
      "epoch:  85  total loss L:  20.057471752166748  Took  0.1  secs.\n",
      "epoch:  86  total loss L:  19.667124152183533  Took  0.1  secs.\n",
      "epoch:  87  total loss L:  19.477166175842285  Took  0.1  secs.\n",
      "epoch:  88  total loss L:  19.269217133522034  Took  0.1  secs.\n",
      "epoch:  89  total loss L:  19.17811954021454  Took  0.1  secs.\n",
      "epoch:  90  total loss L:  19.1372709274292  Took  0.2  secs.\n",
      "epoch:  91  total loss L:  19.12216305732727  Took  0.1  secs.\n",
      "epoch:  92  total loss L:  19.088322281837463  Took  0.1  secs.\n",
      "epoch:  93  total loss L:  19.154287815093994  Took  0.1  secs.\n",
      "epoch:  94  total loss L:  19.429088473320007  Took  0.1  secs.\n",
      "epoch:  95  total loss L:  20.14905595779419  Took  0.1  secs.\n",
      "epoch:  96  total loss L:  21.461601495742798  Took  0.1  secs.\n",
      "epoch:  97  total loss L:  22.794103860855103  Took  0.1  secs.\n",
      "epoch:  98  total loss L:  28.689131259918213  Took  0.1  secs.\n",
      "epoch:  99  total loss L:  26.24737787246704  Took  0.1  secs.\n",
      "epoch:  100  total loss L:  26.72228503227234  Took  0.2  secs.\n",
      "#\n",
      "dcmf.fit - end\n",
      "dataset_name:  dn3\n",
      "#\n",
      "X0.shape:  (200, 500)\n",
      "X1.shape:  (200, 300)\n",
      "X2.shape:  (200, 400)\n",
      "X3.shape:  (700, 500)\n",
      "X4.shape:  (600, 300)\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "WARNING: The following parameters are unused since no validation data provided.\n",
      "val_metric:  auc\n",
      "at_k:  10\n",
      "is_val_transpose:  True\n",
      "#\n",
      "dCMF:\n",
      "---\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  0.0001\n",
      "weight_decay:  0.01\n",
      "convg_thres:  -0.1\n",
      "max_epochs:  100\n",
      "isPretrain:  False\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  10\n",
      "k:  100\n",
      "kf:  0.0005\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  True\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  0\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "## fold_num:  1  ##\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "WARNING: The following parameters are unused since no validation data provided.\n",
      "val_metric:  auc\n",
      "at_k:  10\n",
      "is_val_transpose:  True\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  0.0001\n",
      "weight_decay:  0.01\n",
      "convg_thres:  -0.1\n",
      "max_epochs:  100\n",
      "isPretrain:  False\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  10\n",
      "k:  100\n",
      "kf:  0.0005\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  True\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  0\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "dcmf - model construction - start\n",
      "__input_transformation - start\n",
      "#\n",
      "concatenated-matrix construction...\n",
      "e_id:  e0\n",
      "X_id_list:  ['X0', 'X1', 'X2']\n",
      "X_id:  X0\n",
      "X[X_id].shape:  (200, 500)\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (200, 300)\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (200, 400)\n",
      "C_dict[e].shape:  torch.Size([200, 1200])\n",
      "---\n",
      "e_id:  e1\n",
      "X_id_list:  ['X0', 'X3']\n",
      "X_id:  X0\n",
      "X[X_id].shape:  (200, 500)\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (700, 500)\n",
      "C_dict[e].shape:  torch.Size([500, 900])\n",
      "---\n",
      "e_id:  e2\n",
      "X_id_list:  ['X1', 'X4']\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (200, 300)\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (600, 300)\n",
      "C_dict[e].shape:  torch.Size([300, 800])\n",
      "---\n",
      "e_id:  e3\n",
      "X_id_list:  ['X2']\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (200, 400)\n",
      "C_dict[e].shape:  torch.Size([400, 200])\n",
      "---\n",
      "e_id:  e4\n",
      "X_id_list:  ['X3']\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (700, 500)\n",
      "C_dict[e].shape:  torch.Size([700, 500])\n",
      "---\n",
      "e_id:  e5\n",
      "X_id_list:  ['X4']\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (600, 300)\n",
      "C_dict[e].shape:  torch.Size([600, 300])\n",
      "---\n",
      "#\n",
      "concatenated-matrix chunking...\n",
      "#\n",
      "e_id:  e0 , min_num_datapoints:  200 , num_chunks:  10\n",
      "e_id:  e3 , min_features:  200 , k:  100\n",
      "#\n",
      "e_id:  e0  C_dict[e_id].shape:  torch.Size([200, 1200])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([20, 1200])\n",
      "---\n",
      "e_id:  e1  C_dict[e_id].shape:  torch.Size([500, 900])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([50, 900])\n",
      "---\n",
      "e_id:  e2  C_dict[e_id].shape:  torch.Size([300, 800])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([30, 800])\n",
      "---\n",
      "e_id:  e3  C_dict[e_id].shape:  torch.Size([400, 200])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([40, 200])\n",
      "---\n",
      "e_id:  e4  C_dict[e_id].shape:  torch.Size([700, 500])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([70, 500])\n",
      "---\n",
      "e_id:  e5  C_dict[e_id].shape:  torch.Size([600, 300])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([60, 300])\n",
      "---\n",
      "#\n",
      "creating pytorch variables of input matrices...\n",
      "#\n",
      "__input_transformation - end\n",
      "__network_construction - start\n",
      "__network_construction - end\n",
      "dcmf - model construction - end\n",
      "#\n",
      "#\n",
      "dcmf.fit - start\n",
      "epoch:  1  total loss L:  76.71977472305298  Took  0.1  secs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  2  total loss L:  60.2227725982666  Took  0.1  secs.\n",
      "epoch:  3  total loss L:  53.60260772705078  Took  0.1  secs.\n",
      "epoch:  4  total loss L:  49.30392265319824  Took  0.1  secs.\n",
      "epoch:  5  total loss L:  46.49147653579712  Took  0.1  secs.\n",
      "epoch:  6  total loss L:  44.58919143676758  Took  0.1  secs.\n",
      "epoch:  7  total loss L:  43.17924451828003  Took  0.1  secs.\n",
      "epoch:  8  total loss L:  42.05451488494873  Took  0.1  secs.\n",
      "epoch:  9  total loss L:  41.20753192901611  Took  0.1  secs.\n",
      "epoch:  10  total loss L:  40.62948298454285  Took  0.1  secs.\n",
      "epoch:  11  total loss L:  40.41703748703003  Took  0.1  secs.\n",
      "epoch:  12  total loss L:  40.26822280883789  Took  0.1  secs.\n",
      "epoch:  13  total loss L:  40.94781517982483  Took  0.1  secs.\n",
      "epoch:  14  total loss L:  39.35275602340698  Took  0.1  secs.\n",
      "epoch:  15  total loss L:  37.26787447929382  Took  0.1  secs.\n",
      "epoch:  16  total loss L:  37.46761512756348  Took  0.1  secs.\n",
      "epoch:  17  total loss L:  43.93247127532959  Took  0.1  secs.\n",
      "epoch:  18  total loss L:  46.2542142868042  Took  0.1  secs.\n",
      "epoch:  19  total loss L:  39.58015727996826  Took  0.1  secs.\n",
      "epoch:  20  total loss L:  35.04213786125183  Took  0.1  secs.\n",
      "epoch:  21  total loss L:  33.48920392990112  Took  0.1  secs.\n",
      "epoch:  22  total loss L:  32.56576633453369  Took  0.1  secs.\n",
      "epoch:  23  total loss L:  31.64354705810547  Took  0.1  secs.\n",
      "epoch:  24  total loss L:  30.931394577026367  Took  0.1  secs.\n",
      "epoch:  25  total loss L:  30.162078619003296  Took  0.1  secs.\n",
      "epoch:  26  total loss L:  29.48601746559143  Took  0.1  secs.\n",
      "epoch:  27  total loss L:  28.81006932258606  Took  0.1  secs.\n",
      "epoch:  28  total loss L:  28.1751127243042  Took  0.1  secs.\n",
      "epoch:  29  total loss L:  27.56306266784668  Took  0.1  secs.\n",
      "epoch:  30  total loss L:  26.97956371307373  Took  0.1  secs.\n",
      "epoch:  31  total loss L:  26.42770481109619  Took  0.1  secs.\n",
      "epoch:  32  total loss L:  25.901902675628662  Took  0.1  secs.\n",
      "epoch:  33  total loss L:  25.410287141799927  Took  0.1  secs.\n",
      "epoch:  34  total loss L:  24.989846229553223  Took  0.1  secs.\n",
      "epoch:  35  total loss L:  24.71833109855652  Took  0.1  secs.\n",
      "epoch:  36  total loss L:  24.686469078063965  Took  0.1  secs.\n",
      "epoch:  37  total loss L:  24.974194049835205  Took  0.2  secs.\n",
      "epoch:  38  total loss L:  27.608844757080078  Took  0.1  secs.\n",
      "epoch:  39  total loss L:  30.41572904586792  Took  0.1  secs.\n",
      "epoch:  40  total loss L:  30.151923894882202  Took  0.2  secs.\n",
      "epoch:  41  total loss L:  29.940014123916626  Took  0.1  secs.\n",
      "epoch:  42  total loss L:  31.767865896224976  Took  0.1  secs.\n",
      "epoch:  43  total loss L:  25.950512886047363  Took  0.1  secs.\n",
      "epoch:  44  total loss L:  26.407845735549927  Took  0.1  secs.\n",
      "epoch:  45  total loss L:  26.372541904449463  Took  0.1  secs.\n",
      "epoch:  46  total loss L:  28.467429637908936  Took  0.1  secs.\n",
      "epoch:  47  total loss L:  30.711439609527588  Took  0.1  secs.\n",
      "epoch:  48  total loss L:  35.157721281051636  Took  0.1  secs.\n",
      "epoch:  49  total loss L:  53.399253129959106  Took  0.2  secs.\n",
      "epoch:  50  total loss L:  47.92370104789734  Took  0.1  secs.\n",
      "epoch:  51  total loss L:  37.291584968566895  Took  0.1  secs.\n",
      "epoch:  52  total loss L:  30.5526442527771  Took  0.1  secs.\n",
      "epoch:  53  total loss L:  27.782689809799194  Took  0.1  secs.\n",
      "epoch:  54  total loss L:  26.109798669815063  Took  0.1  secs.\n",
      "epoch:  55  total loss L:  24.866459369659424  Took  0.1  secs.\n",
      "epoch:  56  total loss L:  24.422647953033447  Took  0.1  secs.\n",
      "epoch:  57  total loss L:  24.011770009994507  Took  0.1  secs.\n",
      "epoch:  58  total loss L:  23.586138010025024  Took  0.1  secs.\n",
      "epoch:  59  total loss L:  23.122272491455078  Took  0.1  secs.\n",
      "epoch:  60  total loss L:  22.646667003631592  Took  0.2  secs.\n",
      "epoch:  61  total loss L:  22.202133417129517  Took  0.1  secs.\n",
      "epoch:  62  total loss L:  21.795357704162598  Took  0.1  secs.\n",
      "epoch:  63  total loss L:  21.437495946884155  Took  0.1  secs.\n",
      "epoch:  64  total loss L:  21.126755952835083  Took  0.1  secs.\n",
      "epoch:  65  total loss L:  20.856510400772095  Took  0.1  secs.\n",
      "epoch:  66  total loss L:  20.629016160964966  Took  0.1  secs.\n",
      "epoch:  67  total loss L:  20.472779273986816  Took  0.1  secs.\n",
      "epoch:  68  total loss L:  20.370726943016052  Took  0.1  secs.\n",
      "epoch:  69  total loss L:  20.332202911376953  Took  0.1  secs.\n",
      "epoch:  70  total loss L:  20.756595373153687  Took  0.1  secs.\n",
      "epoch:  71  total loss L:  21.62789487838745  Took  0.1  secs.\n",
      "epoch:  72  total loss L:  22.72657823562622  Took  0.1  secs.\n",
      "epoch:  73  total loss L:  25.47418475151062  Took  0.1  secs.\n",
      "epoch:  74  total loss L:  24.5906982421875  Took  0.1  secs.\n",
      "epoch:  75  total loss L:  24.147955894470215  Took  0.1  secs.\n",
      "epoch:  76  total loss L:  23.649372339248657  Took  0.1  secs.\n",
      "epoch:  77  total loss L:  23.260786533355713  Took  0.1  secs.\n",
      "epoch:  78  total loss L:  23.45214533805847  Took  0.1  secs.\n",
      "epoch:  79  total loss L:  23.150002002716064  Took  0.1  secs.\n",
      "epoch:  80  total loss L:  23.285058736801147  Took  0.1  secs.\n",
      "epoch:  81  total loss L:  25.143455266952515  Took  0.1  secs.\n",
      "epoch:  82  total loss L:  27.430240392684937  Took  0.1  secs.\n",
      "epoch:  83  total loss L:  33.32923126220703  Took  0.1  secs.\n",
      "epoch:  84  total loss L:  36.83321928977966  Took  0.1  secs.\n",
      "epoch:  85  total loss L:  30.183090686798096  Took  0.1  secs.\n",
      "epoch:  86  total loss L:  25.28113627433777  Took  0.1  secs.\n",
      "epoch:  87  total loss L:  23.382551431655884  Took  0.1  secs.\n",
      "epoch:  88  total loss L:  21.48205327987671  Took  0.1  secs.\n",
      "epoch:  89  total loss L:  20.581966400146484  Took  0.2  secs.\n",
      "epoch:  90  total loss L:  20.24939727783203  Took  0.1  secs.\n",
      "epoch:  91  total loss L:  19.92485213279724  Took  0.1  secs.\n",
      "epoch:  92  total loss L:  19.56799876689911  Took  0.1  secs.\n",
      "epoch:  93  total loss L:  19.27921950817108  Took  0.1  secs.\n",
      "epoch:  94  total loss L:  19.04037034511566  Took  0.1  secs.\n",
      "epoch:  95  total loss L:  18.849084496498108  Took  0.1  secs.\n",
      "epoch:  96  total loss L:  18.71522867679596  Took  0.1  secs.\n",
      "epoch:  97  total loss L:  18.6132835149765  Took  0.1  secs.\n",
      "epoch:  98  total loss L:  18.526990056037903  Took  0.1  secs.\n",
      "epoch:  99  total loss L:  18.439685463905334  Took  0.1  secs.\n",
      "epoch:  100  total loss L:  18.355497121810913  Took  0.1  secs.\n",
      "#\n",
      "dcmf.fit - end\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in ['dt1', 'ds1', 'ds2', 'ds3', 'dn1', 'dn2', 'dn3']:\n",
    "    print(\"dataset_name: \",dataset_name)\n",
    "    print(\"#\")\n",
    "    #\n",
    "    fname = in_dir + dataset_name + \"/0.csv\"\n",
    "    X0 = pd.read_csv(fname,header=None).values\n",
    "    #\n",
    "    fname = in_dir + dataset_name + \"/1.csv\"\n",
    "    X1 = pd.read_csv(fname,header=None).values\n",
    "    #\n",
    "    fname = in_dir + dataset_name + \"/2.csv\"\n",
    "    X2 = pd.read_csv(fname,header=None).values\n",
    "    #\n",
    "    fname = in_dir + dataset_name + \"/3.csv\"\n",
    "    X3 = pd.read_csv(fname,header=None).values\n",
    "    #\n",
    "    fname = in_dir + dataset_name + \"/4.csv\"\n",
    "    X4 = pd.read_csv(fname,header=None).values\n",
    "    #\n",
    "    out_dir = out_dir_base + dataset_name + \"/\"\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    #\n",
    "    print(\"X0.shape: \",X0.shape)\n",
    "    print(\"X1.shape: \",X1.shape)\n",
    "    print(\"X2.shape: \",X2.shape)\n",
    "    print(\"X3.shape: \",X3.shape)\n",
    "    print(\"X4.shape: \",X4.shape)\n",
    "    #\n",
    "    G = {\"e0\":[\"X0\",\"X1\",\"X2\"],\\\n",
    "         \"e1\":[\"X0\",\"X3\"],\\\n",
    "         \"e2\":[\"X1\",\"X4\"],\\\n",
    "         \"e3\":[\"X2\"],\\\n",
    "         \"e4\":[\"X3\"],\\\n",
    "         \"e5\":[\"X4\"]}\n",
    "    #\n",
    "    X_data = {\n",
    "        \"X0\":X0,\n",
    "        \"X1\":X1,\n",
    "        \"X2\":X2,\n",
    "        \"X3\":X3,\n",
    "        \"X4\":X4}\n",
    "    #\n",
    "    X_meta = {\"X0\":[\"e0\",\"e1\"],\\\n",
    "         \"X1\":[\"e0\",\"e2\"],\\\n",
    "         \"X2\":[\"e0\",\"e3\"],\\\n",
    "         \"X3\":[\"e4\",\"e1\"],\\\n",
    "         \"X4\":[\"e5\",\"e2\"]}\n",
    "    #\n",
    "    X_val = {}\n",
    "    #\n",
    "    kf = 0.0005\n",
    "    k = 100\n",
    "    e_actf = \"tanh\"\n",
    "    d_actf = \"tanh\"\n",
    "    is_linear_last_enc_layer = False\n",
    "    is_linear_last_dec_layer = False\n",
    "    num_chunks = 10\n",
    "    #\n",
    "    learning_rate = 0.0001\n",
    "    weight_decay = 0.01\n",
    "    max_epochs = 100\n",
    "    convg_thres = -0.1\n",
    "    #\n",
    "    is_pretrain=False\n",
    "    pretrain_thres= 0.1\n",
    "    max_pretrain_epochs = 2\n",
    "    #\n",
    "    val_metric = \"auc\"\n",
    "    is_val_transpose = True\n",
    "    at_k = 10\n",
    "    #\n",
    "    is_gpu = True\n",
    "    gpu_ids = \"1\"\n",
    "    #\n",
    "    num_folds = 1\n",
    "    #\n",
    "    dcmf_model = dcmf(G, X_data, X_meta,\\\n",
    "                num_chunks=num_chunks,k=k, kf=kf, e_actf=e_actf, d_actf=d_actf,\\\n",
    "                learning_rate=learning_rate, weight_decay=weight_decay, convg_thres=convg_thres, max_epochs=max_epochs,\\\n",
    "                is_gpu=is_gpu,gpu_ids=gpu_ids,is_pretrain=is_pretrain, pretrain_thres=pretrain_thres,\\\n",
    "                max_pretrain_epochs=max_pretrain_epochs,X_val=X_val,val_metric=val_metric,\\\n",
    "                is_val_transpose=is_val_transpose, at_k=at_k,\\\n",
    "                is_linear_last_enc_layer=is_linear_last_enc_layer,is_linear_last_dec_layer=is_linear_last_dec_layer,num_val_sets=num_folds)\n",
    "    #\n",
    "    dcmf_model.fit()\n",
    "    #\n",
    "    dict_out = dcmf_model.out_dict_X_prime[\"1\"]\n",
    "    dict_out_np = {}\n",
    "    for cur_mat_id in dict_out:\n",
    "        cur_mat_tensor = dict_out[cur_mat_id]\n",
    "        cur_mat_np = cur_mat_tensor.cpu().detach().numpy()\n",
    "        dict_out_np[cur_mat_id] = cur_mat_np\n",
    "    #\n",
    "    fname_out = out_dir + \"dict_out_dcmf.pkl\"\n",
    "    pkl.dump(dict_out_np,open(fname_out,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
