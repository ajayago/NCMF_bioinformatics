{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dCMF\n",
    "Example of running the \"dcmf\" module with the use provided parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import time\n",
    "import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.dcmf import dcmf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the sample dataset\n",
    "\n",
    "This directory contains a sample synthetic dataset generated for the augmented setting of Fig 1(c) in the [paper](https://arxiv.org/abs/1811.11427).\n",
    "You can download the sample data from [here](https://drive.google.com/open?id=1EFF_kuOIg2aYyOGZY_peX3NziqCSxxP1) and unzip it to the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"../data/sample_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from data_dir:  ../data/sample_data/\n"
     ]
    }
   ],
   "source": [
    "#Loads the dataset into a dict\n",
    "#Note: This dataset contains 5-folds for the matrix X_12 (matrix R below)\n",
    "num_folds = 1\n",
    "#\n",
    "pp = pprint.PrettyPrinter()\n",
    "print(\"Loading data from data_dir: \",data_dir)\n",
    "U1 = pkl.load(open(data_dir+\"X_13.pkl\",'rb'))\n",
    "U2 = pkl.load(open(data_dir+\"X_14.pkl\",'rb'))\n",
    "V1 = pkl.load(open(data_dir+\"X_26.pkl\",'rb'))\n",
    "W1 = pkl.load(open(data_dir+\"X_53.pkl\",'rb'))\n",
    "R_temp_dict = {}\n",
    "for fold_num in np.arange(1,num_folds+1):\n",
    "    Rtrain = pkl.load(open(data_dir+'/X_12_train_fold_'+str(fold_num)+'.pkl','rb'))\n",
    "    Rtrain = Rtrain\n",
    "    Rtrain_idx = pkl.load(open(data_dir+'/X_12_train_idx_'+str(fold_num)+'.pkl','rb')) \n",
    "    Rtest = pkl.load(open(data_dir+'/X_12_test_fold_'+str(fold_num)+'.pkl','rb'))\n",
    "    Rtest_idx = pkl.load(open(data_dir+'/X_12_test_idx_'+str(fold_num)+'.pkl','rb'))\n",
    "    Rdoublets = pkl.load(open(data_dir+'/R_doublets_'+str(fold_num)+'.pkl','rb'))\n",
    "    R_temp_dict[fold_num] = {\"Rtrain\":Rtrain,\"Rtrain_idx\":Rtrain_idx,\"Rtest\":Rtest,\"Rtest_idx\":Rtest_idx,\"Rdoublets\":Rdoublets}\n",
    "#\n",
    "data_dict = {\"U1\":U1,\"U2\":U2,\"V1\":V1,\"W1\":W1,\"R\":R_temp_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U1.shape:  (1000, 20)\n",
      "U2.shape:  (1000, 150)\n",
      "V1.shape:  (2000, 250)\n",
      "W1.shape:  (300, 20)\n",
      "R.shape:  (1000, 2000)\n"
     ]
    }
   ],
   "source": [
    "print(\"U1.shape: \",U1.shape)\n",
    "print(\"U2.shape: \",U2.shape)\n",
    "print(\"V1.shape: \",V1.shape)\n",
    "print(\"W1.shape: \",W1.shape)\n",
    "print(\"R.shape: \",data_dict['R'][1]['Rtrain'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the required data structures\n",
    "\n",
    "Here we construct the data structures required as input to the dcmf API\n",
    "\n",
    "#### *entity matrix relationship graph *\n",
    "\n",
    "- **G**: dict, keys are entity IDs and values are lists of associated matrix IDs\n",
    "\n",
    "#### * training data*\n",
    "- **X_data**: dict, keys are matrix IDs and values are (1) np.array, or (2) dict, (if this matrix is in validation set **X_val**) with validation set IDs as keys & values as np.array\n",
    "- **X_meta**: dict, keys are matrix IDs and values are lists of the 2 associated entity IDs\n",
    "\n",
    "#### *validation data*\n",
    "- **X_val**: dict, keys are IDs of the matrices that are part of validation set and values are dict with validation set IDs as keys and values are (1) scipy.sparse matrix, or (2) list of triplets corresponding to the validation entries (if you would like to perform classification and measure AUC)  \n",
    "**Note**: To perform K folds cross validation, use K validation sets for the corresponsing matrix/matrices. In the example below, we used a single validation set with ID \"1\" for each of the matrices with IDs \"X1\" and \"X2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G = {\n",
    "    \"e1\":[\"X1\",\"X2\",\"X3\"],\\\n",
    "    \"e2\":[\"X1\",\"X4\"],\\\n",
    "    \"e3\":[\"X2\",\"X5\"],\\\n",
    "    \"e4\":[\"X3\"],\\\n",
    "    \"e5\":[\"X5\"],\\\n",
    "    \"e6\":[\"X4\"]}\n",
    "    #\"e6\":[\"X4\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_data = {\n",
    "    \"X1\":{\"1\":data_dict['R'][1][\"Rtrain\"]},\\\n",
    "    \"X2\":{\"1\":U1},\\\n",
    "    \"X3\":U2,\\\n",
    "    \"X4\":V1,\\\n",
    "    \"X5\":W1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_meta = {\n",
    "    \"X1\":[\"e1\",\"e2\"],\\\n",
    "    \"X2\":[\"e1\",\"e3\"],\\\n",
    "    \"X3\":[\"e1\",\"e4\"],\\\n",
    "    \"X4\":[\"e2\",\"e6\"],\\\n",
    "    \"X5\":[\"e5\",\"e3\"]}\n",
    "    #\"X5\":[\"e5\",\"e3\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Rtest_triplets1 = [[1,1,1],[2,2,0]]\n",
    "Rtest_triplets2 = [[1,1,1],[3,3,0],[1,2,0],[0,1,0],[0,2,0],[0,3,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_val = {\n",
    "    \"X1\":{\"1\":Rtest_triplets1},\n",
    "    \"X2\":{\"1\":Rtest_triplets2}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *dCMF network construction - hyperparameters*\n",
    "\n",
    "- **kf**: float, in the range (0,1) \n",
    "- **k**: int, entity representation or encoding size. Refer Appendix A in the [paper](https://arxiv.org/abs/1811.11427) for info about how k and kf are used in the dCMF network construction. \n",
    "- **e_actf**: str, autoencoder's encoding activation function.\n",
    "- **d_actf**: str, autoencoder's decoding activation function. Supported functions are \"tanh\",\"sigma\",\"relu\",\"lrelu\"\n",
    "- **is_linear_last_enc_layer**: bool, True to set linear activation for the bottleneck/encoding generation layer \n",
    "- **is_linear_last_dec_layer**: bool, True to set linear activation for the output/decoding generation layer \n",
    "- **num_chunks**: int, number of training batches to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kf = 0.5\n",
    "k = 100\n",
    "e_actf = \"tanh\"\n",
    "d_actf = \"tanh\"\n",
    "is_linear_last_enc_layer = False\n",
    "is_linear_last_dec_layer = False\n",
    "num_chunks = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Optimization/training - hyperparamteres*\n",
    "\n",
    "- **learning_rate**: float, Adam optimizer's learning rate\n",
    "- **weight_decay**: float, Adam optimizers's weight decay (L2 penalty)\n",
    "- **max_epochs**: int, maximum number of training epochs at which the training stops \n",
    "- **convg_thres**: float, convergence threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.05\n",
    "max_epochs = 5\n",
    "convg_thres = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Hyperparamteres related to pre-training*\n",
    "\n",
    "- **is_pretrain**: bool, True for pretraining \n",
    "- **pretrain_thres**: bool, pre-training convergence thresholsd\n",
    "- **max_pretrain_epochs**: int, maximum number of pre-training epochs at which the training stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_pretrain=True\n",
    "pretrain_thres= 0.1\n",
    "max_pretrain_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Parameters related to validation*\n",
    "\n",
    "- **val_metric**: str, Validation performance metric. Supported metrics: [\"rmse\",\"r@k\",\"p@k\",\"auc\"]. Where,  \n",
    "     *rmse* - Root [mean square error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)  \n",
    "     *r@k* - Recall@k. Refer section 5.2's sub-section \"Evaluation metric\" in the [paper](https://arxiv.org/abs/1811.11427)      \n",
    "     *p@k* - Probability@k. Refer section 5.3's sub-section \"Evaluation metric\" in the [paper](https://arxiv.org/abs/1811.11427)      \n",
    "     *auc* - [Area under the curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)\n",
    "    \n",
    "- **is_val_transpose**: bool, True if the reconstructed matrix has to be transposed before computing the validation performance\n",
    "- **at_k**: int, the value of k if the **val_metric** is either \"r@k\" or \"p@k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_metric = \"auc\"\n",
    "is_val_transpose = True\n",
    "at_k = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *GPU - parameters *\n",
    "\n",
    "- **is_gpu**: bool, True if pytorch tensors storage and operations has to be done in GPU\n",
    "- **gpu_ids**: str, Comma separated string of CUDA GPU ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_gpu = False\n",
    "gpu_ids = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Instantiating the dCMF model...*\n",
    "- Initializes dCMF after validating the input data and the (hyper)parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "#\n",
      "dCMF:\n",
      "---\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  0.001\n",
      "weight_decay:  0.05\n",
      "convg_thres:  0.1\n",
      "max_epochs:  5\n",
      "isPretrain:  True\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  2\n",
      "k:  100\n",
      "kf:  0.5\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  False\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  2\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n"
     ]
    }
   ],
   "source": [
    "dcmf_model = dcmf(G, X_data, X_meta,\\\n",
    "            num_chunks=num_chunks,k=k, kf=kf, e_actf=e_actf, d_actf=d_actf,\\\n",
    "            learning_rate=learning_rate, weight_decay=weight_decay, convg_thres=convg_thres, max_epochs=max_epochs,\\\n",
    "            is_gpu=is_gpu,gpu_ids=gpu_ids,is_pretrain=is_pretrain, pretrain_thres=pretrain_thres,\\\n",
    "            max_pretrain_epochs=max_pretrain_epochs,X_val=X_val,val_metric=val_metric,\\\n",
    "            is_val_transpose=is_val_transpose, at_k=at_k,\\\n",
    "            is_linear_last_enc_layer=is_linear_last_enc_layer,is_linear_last_dec_layer=is_linear_last_dec_layer,num_val_sets=num_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Fitting... *\n",
    "- Performs the input transformation and network construction\n",
    "- (Pre-trains and) trains the model to obtain the entity representations\n",
    "- Reconstruct the input matrices using the entity representations obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## fold_num:  1  ##\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  0.001\n",
      "weight_decay:  0.05\n",
      "convg_thres:  0.1\n",
      "max_epochs:  5\n",
      "isPretrain:  True\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  2\n",
      "k:  100\n",
      "kf:  0.5\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  False\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  2\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "dcmf - model construction - start\n",
      "__input_transformation - start\n",
      "#\n",
      "concatenated-matrix construction...\n",
      "e_id:  e1\n",
      "X_id_list:  ['X1', 'X2', 'X3']\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (1000, 2000)\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (1000, 20)\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (1000, 150)\n",
      "C_dict[e].shape:  torch.Size([1000, 2170])\n",
      "---\n",
      "e_id:  e2\n",
      "X_id_list:  ['X1', 'X4']\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (1000, 2000)\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (2000, 250)\n",
      "C_dict[e].shape:  torch.Size([2000, 1250])\n",
      "---\n",
      "e_id:  e3\n",
      "X_id_list:  ['X2', 'X5']\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (1000, 20)\n",
      "X_id:  X5\n",
      "X[X_id].shape:  (300, 20)\n",
      "C_dict[e].shape:  torch.Size([20, 1300])\n",
      "---\n",
      "e_id:  e4\n",
      "X_id_list:  ['X3']\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (1000, 150)\n",
      "C_dict[e].shape:  torch.Size([150, 1000])\n",
      "---\n",
      "e_id:  e5\n",
      "X_id_list:  ['X5']\n",
      "X_id:  X5\n",
      "X[X_id].shape:  (300, 20)\n",
      "C_dict[e].shape:  torch.Size([300, 20])\n",
      "---\n",
      "e_id:  e6\n",
      "X_id_list:  ['X4']\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (2000, 250)\n",
      "C_dict[e].shape:  torch.Size([250, 2000])\n",
      "---\n",
      "#\n",
      "concatenated-matrix chunking...\n",
      "WARNING: Entity with ID: e5 has minimum feature size 20 in the setting. The encoding length k 100 is larger than minimim entity feature size.\n",
      "#\n",
      "e_id:  e3 , min_num_datapoints:  20 , num_chunks:  2\n",
      "e_id:  e5 , min_features:  20 , k:  100\n",
      "#\n",
      "e_id:  e1  C_dict[e_id].shape:  torch.Size([1000, 2170])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([500, 2170])\n",
      "---\n",
      "e_id:  e2  C_dict[e_id].shape:  torch.Size([2000, 1250])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([1000, 1250])\n",
      "---\n",
      "e_id:  e3  C_dict[e_id].shape:  torch.Size([20, 1300])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([10, 1300])\n",
      "---\n",
      "e_id:  e4  C_dict[e_id].shape:  torch.Size([150, 1000])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([75, 1000])\n",
      "---\n",
      "e_id:  e5  C_dict[e_id].shape:  torch.Size([300, 20])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([150, 20])\n",
      "---\n",
      "e_id:  e6  C_dict[e_id].shape:  torch.Size([250, 2000])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([125, 2000])\n",
      "---\n",
      "#\n",
      "creating pytorch variables of input matrices...\n",
      "#\n",
      "__input_transformation - end\n",
      "__network_construction - start\n",
      "__network_construction - end\n",
      "dcmf - model construction - end\n",
      "#\n",
      "__pretrain - start\n",
      "pretrain epoch:  1  total loss L:  6.176684379577637  Took  3.3  secs.\n",
      "pretrain epoch:  2  total loss L:  5.805932998657227  Took  1.3  secs.\n",
      "__pretrain - end\n",
      "#\n",
      "dcmf.fit - start\n",
      "epoch:  1  total loss L:  55.185845375061035  Took  1.5  secs.\n",
      "epoch:  2  total loss L:  23.878856658935547  Took  1.2  secs.\n",
      "epoch:  3  total loss L:  15.90020227432251  Took  1.2  secs.\n",
      "epoch:  4  total loss L:  9.70403003692627  Took  1.2  secs.\n",
      "epoch:  5  total loss L:  8.811324119567871  Took  1.2  secs.\n",
      "Computing AUC.\n",
      "Rpred.shape:  (2000, 1000)\n",
      "Rtest_triplets.shape:  (2, 3)\n",
      "Computing AUC.\n",
      "Rpred.shape:  (20, 1000)\n",
      "Rtest_triplets.shape:  (6, 3)\n",
      "#\n",
      "dcmf.fit - end\n"
     ]
    }
   ],
   "source": [
    "dcmf_model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Result attributes:*\n",
    "- **out_dict_U**:  dict, keys are validation set IDs and values are dict with entity IDs as keys and np.array of entity representations/encodings as values\n",
    "- **out_dict_X_prime**: dict, keys are matrix IDs and values are matrix reconstructions\n",
    "- **out_dict_info**: dict, keys are loss/validation performance attributes and values are corresponding results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['e1', 'e2', 'e3', 'e4', 'e5', 'e6'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcmf_model.out_dict_U['1'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['X1', 'X2', 'X3', 'X4', 'X5'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcmf_model.out_dict_X_prime['1'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'E': 6,\n",
       " 'M': 5,\n",
       " 'loss_all_folds': {'1': [0.6764270067214966,\n",
       "   0.7010240256786346,\n",
       "   1.0403445959091187,\n",
       "   0.9245219826698303,\n",
       "   0.5641582608222961,\n",
       "   0.9195870459079742,\n",
       "   0.8119459450244904,\n",
       "   0.13825598172843456,\n",
       "   1.1052338425070047,\n",
       "   0.37322503700852394,\n",
       "   1.556600421667099]},\n",
       " 'loss_all_folds_avg_sum': 8.811324145644903,\n",
       " 'loss_all_folds_avg_tuple': [0.6764270067214966,\n",
       "  0.7010240256786346,\n",
       "  1.0403445959091187,\n",
       "  0.9245219826698303,\n",
       "  0.5641582608222961,\n",
       "  0.9195870459079742,\n",
       "  0.8119459450244904,\n",
       "  0.13825598172843456,\n",
       "  1.1052338425070047,\n",
       "  0.37322503700852394,\n",
       "  1.556600421667099],\n",
       " 'num_val_sets': 1,\n",
       " 'params': {'convg_thres': 0.1,\n",
       "  'd_actf': 'tanh',\n",
       "  'e_actf': 'tanh',\n",
       "  'is_linear_last_dec_layer': False,\n",
       "  'is_linear_last_enc_layer': False,\n",
       "  'is_pretrain': True,\n",
       "  'k': 100,\n",
       "  'kf': 0.5,\n",
       "  'learning_rate': 0.001,\n",
       "  'max_epochs': 5,\n",
       "  'max_pretrain_epochs': 2,\n",
       "  'num_chunks': 2,\n",
       "  'pretrain_thres': 0.1,\n",
       "  'weight_decay': 0.05},\n",
       " 'val_metric': 'auc',\n",
       " 'val_perf_all_folds': {'1': {'X1': 1.0, 'X2': 0.19999999999999996}},\n",
       " 'val_perf_all_folds_avg': {'X1': 1.0, 'X2': 0.19999999999999996},\n",
       " 'val_perf_all_folds_total': {'1': 1.2},\n",
       " 'val_perf_all_folds_total_avg': 1.2}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcmf_model.out_dict_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
