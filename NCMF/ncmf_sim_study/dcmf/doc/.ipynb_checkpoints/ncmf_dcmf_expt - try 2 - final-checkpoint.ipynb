{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import os\n",
    "#\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import time\n",
    "import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.dcmf import dcmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_dname = \"./../../sample_data_NCMF/ncmf_sim_data/\"\n",
    "in_dir = base_dname + \"cmf/\"\n",
    "out_dir_base = base_dname + \"dcmf/out/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_name:  dt1\n",
      "#\n",
      "X0.shape:  (200, 500)\n",
      "X1.shape:  (200, 300)\n",
      "X2.shape:  (200, 400)\n",
      "X3.shape:  (700, 500)\n",
      "X4.shape:  (600, 300)\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "WARNING: The following parameters are unused since no validation data provided.\n",
      "val_metric:  auc\n",
      "at_k:  10\n",
      "is_val_transpose:  True\n",
      "#\n",
      "dCMF:\n",
      "---\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  0.001\n",
      "weight_decay:  0.05\n",
      "convg_thres:  0.1\n",
      "max_epochs:  5\n",
      "isPretrain:  False\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  2\n",
      "k:  100\n",
      "kf:  0.5\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  False\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  0\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "## fold_num:  1  ##\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "WARNING: The following parameters are unused since no validation data provided.\n",
      "val_metric:  auc\n",
      "at_k:  10\n",
      "is_val_transpose:  True\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  0.001\n",
      "weight_decay:  0.05\n",
      "convg_thres:  0.1\n",
      "max_epochs:  5\n",
      "isPretrain:  False\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  2\n",
      "k:  100\n",
      "kf:  0.5\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  False\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  0\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "dcmf - model construction - start\n",
      "__input_transformation - start\n",
      "#\n",
      "concatenated-matrix construction...\n",
      "e_id:  e0\n",
      "X_id_list:  ['X0', 'X1', 'X2']\n",
      "X_id:  X0\n",
      "X[X_id].shape:  (200, 500)\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (200, 300)\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (200, 400)\n",
      "C_dict[e].shape:  torch.Size([200, 1200])\n",
      "---\n",
      "e_id:  e1\n",
      "X_id_list:  ['X0', 'X3']\n",
      "X_id:  X0\n",
      "X[X_id].shape:  (200, 500)\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (700, 500)\n",
      "C_dict[e].shape:  torch.Size([500, 900])\n",
      "---\n",
      "e_id:  e2\n",
      "X_id_list:  ['X1', 'X4']\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (200, 300)\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (600, 300)\n",
      "C_dict[e].shape:  torch.Size([300, 800])\n",
      "---\n",
      "e_id:  e3\n",
      "X_id_list:  ['X2']\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (200, 400)\n",
      "C_dict[e].shape:  torch.Size([400, 200])\n",
      "---\n",
      "e_id:  e4\n",
      "X_id_list:  ['X3']\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (700, 500)\n",
      "C_dict[e].shape:  torch.Size([700, 500])\n",
      "---\n",
      "e_id:  e5\n",
      "X_id_list:  ['X4']\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (600, 300)\n",
      "C_dict[e].shape:  torch.Size([600, 300])\n",
      "---\n",
      "#\n",
      "concatenated-matrix chunking...\n",
      "#\n",
      "e_id:  e0 , min_num_datapoints:  200 , num_chunks:  2\n",
      "e_id:  e3 , min_features:  200 , k:  100\n",
      "#\n",
      "e_id:  e0  C_dict[e_id].shape:  torch.Size([200, 1200])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([100, 1200])\n",
      "---\n",
      "e_id:  e1  C_dict[e_id].shape:  torch.Size([500, 900])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([250, 900])\n",
      "---\n",
      "e_id:  e2  C_dict[e_id].shape:  torch.Size([300, 800])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([150, 800])\n",
      "---\n",
      "e_id:  e3  C_dict[e_id].shape:  torch.Size([400, 200])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([200, 200])\n",
      "---\n",
      "e_id:  e4  C_dict[e_id].shape:  torch.Size([700, 500])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([350, 500])\n",
      "---\n",
      "e_id:  e5  C_dict[e_id].shape:  torch.Size([600, 300])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([300, 300])\n",
      "---\n",
      "#\n",
      "creating pytorch variables of input matrices...\n",
      "#\n",
      "__input_transformation - end\n",
      "__network_construction - start\n",
      "__network_construction - end\n",
      "dcmf - model construction - end\n",
      "#\n",
      "#\n",
      "dcmf.fit - start\n",
      "epoch:  1  total loss L:  42.84028100967407  Took  0.3  secs.\n",
      "epoch:  2  total loss L:  16.36841344833374  Took  0.3  secs.\n",
      "epoch:  3  total loss L:  12.267645835876465  Took  0.3  secs.\n",
      "epoch:  4  total loss L:  9.762195587158203  Took  0.3  secs.\n",
      "epoch:  5  total loss L:  9.37302279472351  Took  0.3  secs.\n",
      "#\n",
      "dcmf.fit - end\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in ['dt1']:\n",
    "    print(\"dataset_name: \",dataset_name)\n",
    "    print(\"#\")\n",
    "    #\n",
    "    fname = in_dir + dataset_name + \"/0.csv\"\n",
    "    X0 = pd.read_csv(fname,header=None).to_numpy()\n",
    "    #\n",
    "    fname = in_dir + dataset_name + \"/1.csv\"\n",
    "    X1 = pd.read_csv(fname,header=None).to_numpy()\n",
    "    #\n",
    "    fname = in_dir + dataset_name + \"/2.csv\"\n",
    "    X2 = pd.read_csv(fname,header=None).to_numpy()\n",
    "    #\n",
    "    fname = in_dir + dataset_name + \"/3.csv\"\n",
    "    X3 = pd.read_csv(fname,header=None).to_numpy()\n",
    "    #\n",
    "    fname = in_dir + dataset_name + \"/4.csv\"\n",
    "    X4 = pd.read_csv(fname,header=None).to_numpy()\n",
    "    #\n",
    "    out_dir = out_dir_base + dataset_name + \"/\"\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    #\n",
    "    print(\"X0.shape: \",X0.shape)\n",
    "    print(\"X1.shape: \",X1.shape)\n",
    "    print(\"X2.shape: \",X2.shape)\n",
    "    print(\"X3.shape: \",X3.shape)\n",
    "    print(\"X4.shape: \",X4.shape)\n",
    "    #\n",
    "    G = {\"e0\":[\"X0\",\"X1\",\"X2\"],\\\n",
    "         \"e1\":[\"X0\",\"X3\"],\\\n",
    "         \"e2\":[\"X1\",\"X4\"],\\\n",
    "         \"e3\":[\"X2\"],\\\n",
    "         \"e4\":[\"X3\"],\\\n",
    "         \"e5\":[\"X4\"]}\n",
    "    #\n",
    "    X_data = {\n",
    "        \"X0\":X0,\n",
    "        \"X1\":X1,\n",
    "        \"X2\":X2,\n",
    "        \"X3\":X3,\n",
    "        \"X4\":X4}\n",
    "    #\n",
    "    X_meta = {\"X0\":[\"e0\",\"e1\"],\\\n",
    "         \"X1\":[\"e0\",\"e2\"],\\\n",
    "         \"X2\":[\"e0\",\"e3\"],\\\n",
    "         \"X3\":[\"e4\",\"e1\"],\\\n",
    "         \"X4\":[\"e5\",\"e2\"]}\n",
    "    #\n",
    "    X_val = {}\n",
    "    #\n",
    "    kf = 0.5\n",
    "    k = 100\n",
    "    e_actf = \"tanh\"\n",
    "    d_actf = \"tanh\"\n",
    "    is_linear_last_enc_layer = False\n",
    "    is_linear_last_dec_layer = False\n",
    "    num_chunks = 2\n",
    "    #\n",
    "    learning_rate = 0.001\n",
    "    weight_decay = 0.05\n",
    "    max_epochs = 5\n",
    "    convg_thres = 0.1\n",
    "    #\n",
    "    is_pretrain=False\n",
    "    pretrain_thres= 0.1\n",
    "    max_pretrain_epochs = 2\n",
    "    #\n",
    "    val_metric = \"auc\"\n",
    "    is_val_transpose = True\n",
    "    at_k = 10\n",
    "    #\n",
    "    is_gpu = False\n",
    "    gpu_ids = \"1\"\n",
    "    #\n",
    "    num_folds = 1\n",
    "    #\n",
    "    dcmf_model = dcmf(G, X_data, X_meta,\\\n",
    "                num_chunks=num_chunks,k=k, kf=kf, e_actf=e_actf, d_actf=d_actf,\\\n",
    "                learning_rate=learning_rate, weight_decay=weight_decay, convg_thres=convg_thres, max_epochs=max_epochs,\\\n",
    "                is_gpu=is_gpu,gpu_ids=gpu_ids,is_pretrain=is_pretrain, pretrain_thres=pretrain_thres,\\\n",
    "                max_pretrain_epochs=max_pretrain_epochs,X_val=X_val,val_metric=val_metric,\\\n",
    "                is_val_transpose=is_val_transpose, at_k=at_k,\\\n",
    "                is_linear_last_enc_layer=is_linear_last_enc_layer,is_linear_last_dec_layer=is_linear_last_dec_layer,num_val_sets=num_folds)\n",
    "    #\n",
    "    dcmf_model.fit()\n",
    "    #\n",
    "    dict_out = dcmf_model.out_dict_X_prime[\"1\"]\n",
    "    dict_out_np = {}\n",
    "    for cur_mat_id in dict_out:\n",
    "        cur_mat_tensor = dict_out[cur_mat_id]\n",
    "        cur_mat_np = cur_mat_tensor.cpu().detach().numpy()\n",
    "        dict_out_np[cur_mat_id] = cur_mat_np\n",
    "    #\n",
    "    fname_out = out_dir + \"dict_out_dcmf.pkl\"\n",
    "    pkl.dump(dict_out_np,open(fname_out,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
