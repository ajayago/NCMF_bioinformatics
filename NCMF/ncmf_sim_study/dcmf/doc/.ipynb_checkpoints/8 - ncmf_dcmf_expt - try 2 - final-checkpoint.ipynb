{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import os\n",
    "#\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import time\n",
    "import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dcmf import dcmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dname = \"./../../ncmf_sim_data/\"\n",
    "in_dir = base_dname + \"cmf/\"\n",
    "out_dir_base = base_dname + \"dcmf/out/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_name:  dt1\n",
      "#\n",
      "X0.shape:  (200, 500)\n",
      "X1.shape:  (200, 300)\n",
      "X2.shape:  (200, 400)\n",
      "X3.shape:  (700, 500)\n",
      "X4.shape:  (600, 300)\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "WARNING: The following parameters are unused since no validation data provided.\n",
      "val_metric:  auc\n",
      "at_k:  10\n",
      "is_val_transpose:  True\n",
      "#\n",
      "dCMF:\n",
      "---\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  0.0001\n",
      "weight_decay:  0.01\n",
      "convg_thres:  -0.1\n",
      "max_epochs:  100\n",
      "isPretrain:  False\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  1\n",
      "k:  100\n",
      "kf:  0.0005\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  True\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  0\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "## fold_num:  1  ##\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "WARNING: The following parameters are unused since no validation data provided.\n",
      "val_metric:  auc\n",
      "at_k:  10\n",
      "is_val_transpose:  True\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  0.0001\n",
      "weight_decay:  0.01\n",
      "convg_thres:  -0.1\n",
      "max_epochs:  100\n",
      "isPretrain:  False\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  1\n",
      "k:  100\n",
      "kf:  0.0005\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  True\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  0\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "dcmf - model construction - start\n",
      "__input_transformation - start\n",
      "#\n",
      "concatenated-matrix construction...\n",
      "e_id:  e0\n",
      "X_id_list:  ['X0', 'X1', 'X2']\n",
      "X_id:  X0\n",
      "X[X_id].shape:  (200, 500)\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (200, 300)\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (200, 400)\n",
      "C_dict[e].shape:  torch.Size([200, 1200])\n",
      "---\n",
      "e_id:  e1\n",
      "X_id_list:  ['X0', 'X3']\n",
      "X_id:  X0\n",
      "X[X_id].shape:  (200, 500)\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (700, 500)\n",
      "C_dict[e].shape:  torch.Size([500, 900])\n",
      "---\n",
      "e_id:  e2\n",
      "X_id_list:  ['X1', 'X4']\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (200, 300)\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (600, 300)\n",
      "C_dict[e].shape:  torch.Size([300, 800])\n",
      "---\n",
      "e_id:  e3\n",
      "X_id_list:  ['X2']\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (200, 400)\n",
      "C_dict[e].shape:  torch.Size([400, 200])\n",
      "---\n",
      "e_id:  e4\n",
      "X_id_list:  ['X3']\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (700, 500)\n",
      "C_dict[e].shape:  torch.Size([700, 500])\n",
      "---\n",
      "e_id:  e5\n",
      "X_id_list:  ['X4']\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (600, 300)\n",
      "C_dict[e].shape:  torch.Size([600, 300])\n",
      "---\n",
      "#\n",
      "concatenated-matrix chunking...\n",
      "#\n",
      "e_id:  e0 , min_num_datapoints:  200 , num_chunks:  1\n",
      "e_id:  e3 , min_features:  200 , k:  100\n",
      "#\n",
      "e_id:  e0  C_dict[e_id].shape:  torch.Size([200, 1200])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([200, 1200])\n",
      "---\n",
      "e_id:  e1  C_dict[e_id].shape:  torch.Size([500, 900])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([500, 900])\n",
      "---\n",
      "e_id:  e2  C_dict[e_id].shape:  torch.Size([300, 800])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([300, 800])\n",
      "---\n",
      "e_id:  e3  C_dict[e_id].shape:  torch.Size([400, 200])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([400, 200])\n",
      "---\n",
      "e_id:  e4  C_dict[e_id].shape:  torch.Size([700, 500])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([700, 500])\n",
      "---\n",
      "e_id:  e5  C_dict[e_id].shape:  torch.Size([600, 300])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([600, 300])\n",
      "---\n",
      "#\n",
      "creating pytorch variables of input matrices...\n",
      "#\n",
      "__input_transformation - end\n",
      "__network_construction - start\n",
      "__network_construction - end\n",
      "dcmf - model construction - end\n",
      "#\n",
      "#\n",
      "dcmf.fit - start\n",
      "epoch:  1  total loss L:  8.216495513916016  Took  0.4  secs.\n",
      "epoch:  2  total loss L:  8.15602970123291  Took  0.0  secs.\n",
      "epoch:  3  total loss L:  7.317155838012695  Took  0.0  secs.\n",
      "epoch:  4  total loss L:  6.694122314453125  Took  0.0  secs.\n",
      "epoch:  5  total loss L:  6.86048698425293  Took  0.0  secs.\n",
      "epoch:  6  total loss L:  6.798832893371582  Took  0.0  secs.\n",
      "epoch:  7  total loss L:  6.382303237915039  Took  0.0  secs.\n",
      "epoch:  8  total loss L:  6.079071044921875  Took  0.0  secs.\n",
      "epoch:  9  total loss L:  6.034424781799316  Took  0.0  secs.\n",
      "epoch:  10  total loss L:  6.02372932434082  Took  0.0  secs.\n",
      "epoch:  11  total loss L:  5.883672714233398  Took  0.0  secs.\n",
      "epoch:  12  total loss L:  5.684556484222412  Took  0.0  secs.\n",
      "epoch:  13  total loss L:  5.553677558898926  Took  0.0  secs.\n",
      "epoch:  14  total loss L:  5.511236667633057  Took  0.0  secs.\n",
      "epoch:  15  total loss L:  5.485325813293457  Took  0.0  secs.\n",
      "epoch:  16  total loss L:  5.420174598693848  Took  0.0  secs.\n",
      "epoch:  17  total loss L:  5.325528144836426  Took  0.0  secs.\n",
      "epoch:  18  total loss L:  5.240781784057617  Took  0.0  secs.\n",
      "epoch:  19  total loss L:  5.183130741119385  Took  0.0  secs.\n",
      "epoch:  20  total loss L:  5.139678478240967  Took  0.0  secs.\n",
      "epoch:  21  total loss L:  5.094738960266113  Took  0.0  secs.\n",
      "epoch:  22  total loss L:  5.04583740234375  Took  0.0  secs.\n",
      "epoch:  23  total loss L:  4.997559547424316  Took  0.0  secs.\n",
      "epoch:  24  total loss L:  4.951601028442383  Took  0.0  secs.\n",
      "epoch:  25  total loss L:  4.907066345214844  Took  0.0  secs.\n",
      "epoch:  26  total loss L:  4.8664140701293945  Took  0.0  secs.\n",
      "epoch:  27  total loss L:  4.833364486694336  Took  0.0  secs.\n",
      "epoch:  28  total loss L:  4.80515193939209  Took  0.0  secs.\n",
      "epoch:  29  total loss L:  4.7730512619018555  Took  0.0  secs.\n",
      "epoch:  30  total loss L:  4.732909202575684  Took  0.0  secs.\n",
      "epoch:  31  total loss L:  4.691877365112305  Took  0.0  secs.\n",
      "epoch:  32  total loss L:  4.660859107971191  Took  0.0  secs.\n",
      "epoch:  33  total loss L:  4.641007423400879  Took  0.0  secs.\n",
      "epoch:  34  total loss L:  4.621861457824707  Took  0.0  secs.\n",
      "epoch:  35  total loss L:  4.59424352645874  Took  0.0  secs.\n",
      "epoch:  36  total loss L:  4.560976028442383  Took  0.0  secs.\n",
      "epoch:  37  total loss L:  4.531946182250977  Took  0.0  secs.\n",
      "epoch:  38  total loss L:  4.511253356933594  Took  0.0  secs.\n",
      "epoch:  39  total loss L:  4.493156433105469  Took  0.0  secs.\n",
      "epoch:  40  total loss L:  4.470673561096191  Took  0.0  secs.\n",
      "epoch:  41  total loss L:  4.444214820861816  Took  0.0  secs.\n",
      "epoch:  42  total loss L:  4.419444561004639  Took  0.0  secs.\n",
      "epoch:  43  total loss L:  4.3993072509765625  Took  0.0  secs.\n",
      "epoch:  44  total loss L:  4.381414413452148  Took  0.0  secs.\n",
      "epoch:  45  total loss L:  4.3625383377075195  Took  0.0  secs.\n",
      "epoch:  46  total loss L:  4.342425346374512  Took  0.0  secs.\n",
      "epoch:  47  total loss L:  4.322433948516846  Took  0.0  secs.\n",
      "epoch:  48  total loss L:  4.302968978881836  Took  0.0  secs.\n",
      "epoch:  49  total loss L:  4.283824920654297  Took  0.0  secs.\n",
      "epoch:  50  total loss L:  4.265480041503906  Took  0.0  secs.\n",
      "epoch:  51  total loss L:  4.248263835906982  Took  0.0  secs.\n",
      "epoch:  52  total loss L:  4.23114013671875  Took  0.0  secs.\n",
      "epoch:  53  total loss L:  4.213043212890625  Took  0.0  secs.\n",
      "epoch:  54  total loss L:  4.194826602935791  Took  0.0  secs.\n",
      "epoch:  55  total loss L:  4.178096771240234  Took  0.0  secs.\n",
      "epoch:  56  total loss L:  4.162591457366943  Took  0.0  secs.\n",
      "epoch:  57  total loss L:  4.146535873413086  Took  0.0  secs.\n",
      "epoch:  58  total loss L:  4.129391193389893  Took  0.0  secs.\n",
      "epoch:  59  total loss L:  4.112479209899902  Took  0.0  secs.\n",
      "epoch:  60  total loss L:  4.096741199493408  Took  0.0  secs.\n",
      "epoch:  61  total loss L:  4.081490516662598  Took  0.0  secs.\n",
      "epoch:  62  total loss L:  4.065871715545654  Took  0.0  secs.\n",
      "epoch:  63  total loss L:  4.050090789794922  Took  0.0  secs.\n",
      "epoch:  64  total loss L:  4.0345916748046875  Took  0.0  secs.\n",
      "epoch:  65  total loss L:  4.019289016723633  Took  0.0  secs.\n",
      "epoch:  66  total loss L:  4.004094123840332  Took  0.0  secs.\n",
      "epoch:  67  total loss L:  3.989170551300049  Took  0.0  secs.\n",
      "epoch:  68  total loss L:  3.97438907623291  Took  0.0  secs.\n",
      "epoch:  69  total loss L:  3.959432601928711  Took  0.0  secs.\n",
      "epoch:  70  total loss L:  3.9444992542266846  Took  0.0  secs.\n",
      "epoch:  71  total loss L:  3.9300127029418945  Took  0.0  secs.\n",
      "epoch:  72  total loss L:  3.915768623352051  Took  0.0  secs.\n",
      "epoch:  73  total loss L:  3.9012980461120605  Took  0.0  secs.\n",
      "epoch:  74  total loss L:  3.886733293533325  Took  0.0  secs.\n",
      "epoch:  75  total loss L:  3.872467041015625  Took  0.0  secs.\n",
      "epoch:  76  total loss L:  3.858428716659546  Took  0.0  secs.\n",
      "epoch:  77  total loss L:  3.844364643096924  Took  0.0  secs.\n",
      "epoch:  78  total loss L:  3.830317497253418  Took  0.0  secs.\n",
      "epoch:  79  total loss L:  3.816359043121338  Took  0.0  secs.\n",
      "epoch:  80  total loss L:  3.8024466037750244  Took  0.0  secs.\n",
      "epoch:  81  total loss L:  3.7886452674865723  Took  0.0  secs.\n",
      "epoch:  82  total loss L:  3.7749886512756348  Took  0.0  secs.\n",
      "epoch:  83  total loss L:  3.761322021484375  Took  0.0  secs.\n",
      "epoch:  84  total loss L:  3.747640371322632  Took  0.0  secs.\n",
      "epoch:  85  total loss L:  3.7341198921203613  Took  0.0  secs.\n",
      "epoch:  86  total loss L:  3.7207133769989014  Took  0.0  secs.\n",
      "epoch:  87  total loss L:  3.7072761058807373  Took  0.0  secs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  88  total loss L:  3.693878650665283  Took  0.0  secs.\n",
      "epoch:  89  total loss L:  3.680596113204956  Took  0.0  secs.\n",
      "epoch:  90  total loss L:  3.6673734188079834  Took  0.0  secs.\n",
      "epoch:  91  total loss L:  3.6542141437530518  Took  0.0  secs.\n",
      "epoch:  92  total loss L:  3.6411094665527344  Took  0.0  secs.\n",
      "epoch:  93  total loss L:  3.628021478652954  Took  0.0  secs.\n",
      "epoch:  94  total loss L:  3.6150152683258057  Took  0.0  secs.\n",
      "epoch:  95  total loss L:  3.602107524871826  Took  0.0  secs.\n",
      "epoch:  96  total loss L:  3.589216709136963  Took  0.0  secs.\n",
      "epoch:  97  total loss L:  3.576371669769287  Took  0.0  secs.\n",
      "epoch:  98  total loss L:  3.5636143684387207  Took  0.0  secs.\n",
      "epoch:  99  total loss L:  3.550906181335449  Took  0.0  secs.\n",
      "epoch:  100  total loss L:  3.5382542610168457  Took  0.0  secs.\n",
      "#\n",
      "dcmf.fit - end\n",
      "dataset_name:  ds1\n",
      "#\n",
      "X0.shape:  (200, 500)\n",
      "X1.shape:  (200, 300)\n",
      "X2.shape:  (200, 400)\n",
      "X3.shape:  (700, 500)\n",
      "X4.shape:  (600, 300)\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "WARNING: The following parameters are unused since no validation data provided.\n",
      "val_metric:  auc\n",
      "at_k:  10\n",
      "is_val_transpose:  True\n",
      "#\n",
      "dCMF:\n",
      "---\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  0.0001\n",
      "weight_decay:  0.01\n",
      "convg_thres:  -0.1\n",
      "max_epochs:  100\n",
      "isPretrain:  False\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  1\n",
      "k:  100\n",
      "kf:  0.0005\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  True\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  0\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "## fold_num:  1  ##\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "WARNING: The following parameters are unused since no validation data provided.\n",
      "val_metric:  auc\n",
      "at_k:  10\n",
      "is_val_transpose:  True\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  0.0001\n",
      "weight_decay:  0.01\n",
      "convg_thres:  -0.1\n",
      "max_epochs:  100\n",
      "isPretrain:  False\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  1\n",
      "k:  100\n",
      "kf:  0.0005\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  True\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  0\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "dcmf - model construction - start\n",
      "__input_transformation - start\n",
      "#\n",
      "concatenated-matrix construction...\n",
      "e_id:  e0\n",
      "X_id_list:  ['X0', 'X1', 'X2']\n",
      "X_id:  X0\n",
      "X[X_id].shape:  (200, 500)\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (200, 300)\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (200, 400)\n",
      "C_dict[e].shape:  torch.Size([200, 1200])\n",
      "---\n",
      "e_id:  e1\n",
      "X_id_list:  ['X0', 'X3']\n",
      "X_id:  X0\n",
      "X[X_id].shape:  (200, 500)\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (700, 500)\n",
      "C_dict[e].shape:  torch.Size([500, 900])\n",
      "---\n",
      "e_id:  e2\n",
      "X_id_list:  ['X1', 'X4']\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (200, 300)\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (600, 300)\n",
      "C_dict[e].shape:  torch.Size([300, 800])\n",
      "---\n",
      "e_id:  e3\n",
      "X_id_list:  ['X2']\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (200, 400)\n",
      "C_dict[e].shape:  torch.Size([400, 200])\n",
      "---\n",
      "e_id:  e4\n",
      "X_id_list:  ['X3']\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (700, 500)\n",
      "C_dict[e].shape:  torch.Size([700, 500])\n",
      "---\n",
      "e_id:  e5\n",
      "X_id_list:  ['X4']\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (600, 300)\n",
      "C_dict[e].shape:  torch.Size([600, 300])\n",
      "---\n",
      "#\n",
      "concatenated-matrix chunking...\n",
      "#\n",
      "e_id:  e0 , min_num_datapoints:  200 , num_chunks:  1\n",
      "e_id:  e3 , min_features:  200 , k:  100\n",
      "#\n",
      "e_id:  e0  C_dict[e_id].shape:  torch.Size([200, 1200])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([200, 1200])\n",
      "---\n",
      "e_id:  e1  C_dict[e_id].shape:  torch.Size([500, 900])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([500, 900])\n",
      "---\n",
      "e_id:  e2  C_dict[e_id].shape:  torch.Size([300, 800])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([300, 800])\n",
      "---\n",
      "e_id:  e3  C_dict[e_id].shape:  torch.Size([400, 200])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([400, 200])\n",
      "---\n",
      "e_id:  e4  C_dict[e_id].shape:  torch.Size([700, 500])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([700, 500])\n",
      "---\n",
      "e_id:  e5  C_dict[e_id].shape:  torch.Size([600, 300])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([600, 300])\n",
      "---\n",
      "#\n",
      "creating pytorch variables of input matrices...\n",
      "#\n",
      "__input_transformation - end\n",
      "__network_construction - start\n",
      "__network_construction - end\n",
      "dcmf - model construction - end\n",
      "#\n",
      "#\n",
      "dcmf.fit - start\n",
      "epoch:  1  total loss L:  6.429940223693848  Took  0.0  secs.\n",
      "epoch:  2  total loss L:  6.458856582641602  Took  0.0  secs.\n",
      "epoch:  3  total loss L:  5.4824323654174805  Took  0.0  secs.\n",
      "epoch:  4  total loss L:  5.292395114898682  Took  0.0  secs.\n",
      "epoch:  5  total loss L:  5.598752021789551  Took  0.0  secs.\n",
      "epoch:  6  total loss L:  5.246822834014893  Took  0.0  secs.\n",
      "epoch:  7  total loss L:  4.712081432342529  Took  0.0  secs.\n",
      "epoch:  8  total loss L:  4.599858283996582  Took  0.0  secs.\n",
      "epoch:  9  total loss L:  4.773257255554199  Took  0.0  secs.\n",
      "epoch:  10  total loss L:  4.778266429901123  Took  0.0  secs.\n",
      "epoch:  11  total loss L:  4.5591936111450195  Took  0.0  secs.\n",
      "epoch:  12  total loss L:  4.357593536376953  Took  0.0  secs.\n",
      "epoch:  13  total loss L:  4.312682151794434  Took  0.0  secs.\n",
      "epoch:  14  total loss L:  4.32956600189209  Took  0.0  secs.\n",
      "epoch:  15  total loss L:  4.2720232009887695  Took  0.0  secs.\n",
      "epoch:  16  total loss L:  4.15427303314209  Took  0.0  secs.\n",
      "epoch:  17  total loss L:  4.078131198883057  Took  0.0  secs.\n",
      "epoch:  18  total loss L:  4.079574108123779  Took  0.0  secs.\n",
      "epoch:  19  total loss L:  4.090630531311035  Took  0.0  secs.\n",
      "epoch:  20  total loss L:  4.03865385055542  Took  0.0  secs.\n",
      "epoch:  21  total loss L:  3.937549114227295  Took  0.0  secs.\n",
      "epoch:  22  total loss L:  3.859314441680908  Took  0.0  secs.\n",
      "epoch:  23  total loss L:  3.843785285949707  Took  0.0  secs.\n",
      "epoch:  24  total loss L:  3.857616901397705  Took  0.0  secs.\n",
      "epoch:  25  total loss L:  3.843276023864746  Took  0.0  secs.\n",
      "epoch:  26  total loss L:  3.789323091506958  Took  0.0  secs.\n",
      "epoch:  27  total loss L:  3.7325263023376465  Took  0.0  secs.\n",
      "epoch:  28  total loss L:  3.703908920288086  Took  0.0  secs.\n",
      "epoch:  29  total loss L:  3.6944594383239746  Took  0.0  secs.\n",
      "epoch:  30  total loss L:  3.676389217376709  Took  0.0  secs.\n",
      "epoch:  31  total loss L:  3.6424179077148438  Took  0.0  secs.\n",
      "epoch:  32  total loss L:  3.6102709770202637  Took  0.0  secs.\n",
      "epoch:  33  total loss L:  3.5939760208129883  Took  0.0  secs.\n",
      "epoch:  34  total loss L:  3.584413528442383  Took  0.0  secs.\n",
      "epoch:  35  total loss L:  3.563875198364258  Took  0.0  secs.\n",
      "epoch:  36  total loss L:  3.5316905975341797  Took  0.0  secs.\n",
      "epoch:  37  total loss L:  3.5040488243103027  Took  0.0  secs.\n",
      "epoch:  38  total loss L:  3.490832805633545  Took  0.0  secs.\n",
      "epoch:  39  total loss L:  3.4833924770355225  Took  0.0  secs.\n",
      "epoch:  40  total loss L:  3.4681758880615234  Took  0.0  secs.\n",
      "epoch:  41  total loss L:  3.4445784091949463  Took  0.0  secs.\n",
      "epoch:  42  total loss L:  3.422891139984131  Took  0.0  secs.\n",
      "epoch:  43  total loss L:  3.408609390258789  Took  0.0  secs.\n",
      "epoch:  44  total loss L:  3.396477222442627  Took  0.0  secs.\n",
      "epoch:  45  total loss L:  3.3806300163269043  Took  0.0  secs.\n",
      "epoch:  46  total loss L:  3.363205909729004  Took  0.0  secs.\n",
      "epoch:  47  total loss L:  3.349026679992676  Took  0.0  secs.\n",
      "epoch:  48  total loss L:  3.3371715545654297  Took  0.0  secs.\n",
      "epoch:  49  total loss L:  3.3231687545776367  Took  0.0  secs.\n",
      "epoch:  50  total loss L:  3.3067870140075684  Took  0.0  secs.\n",
      "epoch:  51  total loss L:  3.2921462059020996  Took  0.0  secs.\n",
      "epoch:  52  total loss L:  3.2807884216308594  Took  0.0  secs.\n",
      "epoch:  53  total loss L:  3.2695741653442383  Took  0.0  secs.\n",
      "epoch:  54  total loss L:  3.2562196254730225  Took  0.0  secs.\n",
      "epoch:  55  total loss L:  3.24246883392334  Took  0.0  secs.\n",
      "epoch:  56  total loss L:  3.2302422523498535  Took  0.0  secs.\n",
      "epoch:  57  total loss L:  3.218564510345459  Took  0.0  secs.\n",
      "epoch:  58  total loss L:  3.206240653991699  Took  0.0  secs.\n",
      "epoch:  59  total loss L:  3.194243907928467  Took  0.0  secs.\n",
      "epoch:  60  total loss L:  3.1833643913269043  Took  0.0  secs.\n",
      "epoch:  61  total loss L:  3.1723155975341797  Took  0.0  secs.\n",
      "epoch:  62  total loss L:  3.160219192504883  Took  0.0  secs.\n",
      "epoch:  63  total loss L:  3.1483962535858154  Took  0.0  secs.\n",
      "epoch:  64  total loss L:  3.1378304958343506  Took  0.0  secs.\n",
      "epoch:  65  total loss L:  3.1274046897888184  Took  0.0  secs.\n",
      "epoch:  66  total loss L:  3.1162497997283936  Took  0.0  secs.\n",
      "epoch:  67  total loss L:  3.1052730083465576  Took  0.0  secs.\n",
      "epoch:  68  total loss L:  3.0950136184692383  Took  0.0  secs.\n",
      "epoch:  69  total loss L:  3.084559917449951  Took  0.0  secs.\n",
      "epoch:  70  total loss L:  3.073665142059326  Took  0.0  secs.\n",
      "epoch:  71  total loss L:  3.06331729888916  Took  0.0  secs.\n",
      "epoch:  72  total loss L:  3.0535964965820312  Took  0.0  secs.\n",
      "epoch:  73  total loss L:  3.0435233116149902  Took  0.0  secs.\n",
      "epoch:  74  total loss L:  3.033090114593506  Took  0.0  secs.\n",
      "epoch:  75  total loss L:  3.023078441619873  Took  0.0  secs.\n",
      "epoch:  76  total loss L:  3.0133824348449707  Took  0.0  secs.\n",
      "epoch:  77  total loss L:  3.0035083293914795  Took  0.0  secs.\n",
      "epoch:  78  total loss L:  2.993690013885498  Took  0.0  secs.\n",
      "epoch:  79  total loss L:  2.9841132164001465  Took  0.0  secs.\n",
      "epoch:  80  total loss L:  2.9744503498077393  Took  0.0  secs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  81  total loss L:  2.964749813079834  Took  0.0  secs.\n",
      "epoch:  82  total loss L:  2.955305337905884  Took  0.0  secs.\n",
      "epoch:  83  total loss L:  2.9459147453308105  Took  0.0  secs.\n",
      "epoch:  84  total loss L:  2.936432361602783  Took  0.0  secs.\n",
      "epoch:  85  total loss L:  2.92710280418396  Took  0.0  secs.\n",
      "epoch:  86  total loss L:  2.9178543090820312  Took  0.0  secs.\n",
      "epoch:  87  total loss L:  2.9084949493408203  Took  0.0  secs.\n",
      "epoch:  88  total loss L:  2.8992514610290527  Took  0.0  secs.\n",
      "epoch:  89  total loss L:  2.8901824951171875  Took  0.0  secs.\n",
      "epoch:  90  total loss L:  2.881037473678589  Took  0.0  secs.\n",
      "epoch:  91  total loss L:  2.8718881607055664  Took  0.0  secs.\n",
      "epoch:  92  total loss L:  2.8628616333007812  Took  0.0  secs.\n",
      "epoch:  93  total loss L:  2.8538339138031006  Took  0.0  secs.\n",
      "epoch:  94  total loss L:  2.8448410034179688  Took  0.0  secs.\n",
      "epoch:  95  total loss L:  2.835935592651367  Took  0.0  secs.\n",
      "epoch:  96  total loss L:  2.8270015716552734  Took  0.0  secs.\n",
      "epoch:  97  total loss L:  2.8180971145629883  Took  0.0  secs.\n",
      "epoch:  98  total loss L:  2.8092763423919678  Took  0.0  secs.\n",
      "epoch:  99  total loss L:  2.800440549850464  Took  0.0  secs.\n",
      "epoch:  100  total loss L:  2.7916433811187744  Took  0.0  secs.\n",
      "#\n",
      "dcmf.fit - end\n",
      "dataset_name:  ds2\n",
      "#\n",
      "X0.shape:  (200, 500)\n",
      "X1.shape:  (200, 300)\n",
      "X2.shape:  (200, 400)\n",
      "X3.shape:  (700, 500)\n",
      "X4.shape:  (600, 300)\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "WARNING: The following parameters are unused since no validation data provided.\n",
      "val_metric:  auc\n",
      "at_k:  10\n",
      "is_val_transpose:  True\n",
      "#\n",
      "dCMF:\n",
      "---\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  0.0001\n",
      "weight_decay:  0.01\n",
      "convg_thres:  -0.1\n",
      "max_epochs:  100\n",
      "isPretrain:  False\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  1\n",
      "k:  100\n",
      "kf:  0.0005\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  True\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  0\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "## fold_num:  1  ##\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "WARNING: The following parameters are unused since no validation data provided.\n",
      "val_metric:  auc\n",
      "at_k:  10\n",
      "is_val_transpose:  True\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  0.0001\n",
      "weight_decay:  0.01\n",
      "convg_thres:  -0.1\n",
      "max_epochs:  100\n",
      "isPretrain:  False\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  1\n",
      "k:  100\n",
      "kf:  0.0005\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  True\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  0\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "dcmf - model construction - start\n",
      "__input_transformation - start\n",
      "#\n",
      "concatenated-matrix construction...\n",
      "e_id:  e0\n",
      "X_id_list:  ['X0', 'X1', 'X2']\n",
      "X_id:  X0\n",
      "X[X_id].shape:  (200, 500)\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (200, 300)\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (200, 400)\n",
      "C_dict[e].shape:  torch.Size([200, 1200])\n",
      "---\n",
      "e_id:  e1\n",
      "X_id_list:  ['X0', 'X3']\n",
      "X_id:  X0\n",
      "X[X_id].shape:  (200, 500)\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (700, 500)\n",
      "C_dict[e].shape:  torch.Size([500, 900])\n",
      "---\n",
      "e_id:  e2\n",
      "X_id_list:  ['X1', 'X4']\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (200, 300)\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (600, 300)\n",
      "C_dict[e].shape:  torch.Size([300, 800])\n",
      "---\n",
      "e_id:  e3\n",
      "X_id_list:  ['X2']\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (200, 400)\n",
      "C_dict[e].shape:  torch.Size([400, 200])\n",
      "---\n",
      "e_id:  e4\n",
      "X_id_list:  ['X3']\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (700, 500)\n",
      "C_dict[e].shape:  torch.Size([700, 500])\n",
      "---\n",
      "e_id:  e5\n",
      "X_id_list:  ['X4']\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (600, 300)\n",
      "C_dict[e].shape:  torch.Size([600, 300])\n",
      "---\n",
      "#\n",
      "concatenated-matrix chunking...\n",
      "#\n",
      "e_id:  e0 , min_num_datapoints:  200 , num_chunks:  1\n",
      "e_id:  e3 , min_features:  200 , k:  100\n",
      "#\n",
      "e_id:  e0  C_dict[e_id].shape:  torch.Size([200, 1200])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([200, 1200])\n",
      "---\n",
      "e_id:  e1  C_dict[e_id].shape:  torch.Size([500, 900])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([500, 900])\n",
      "---\n",
      "e_id:  e2  C_dict[e_id].shape:  torch.Size([300, 800])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([300, 800])\n",
      "---\n",
      "e_id:  e3  C_dict[e_id].shape:  torch.Size([400, 200])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([400, 200])\n",
      "---\n",
      "e_id:  e4  C_dict[e_id].shape:  torch.Size([700, 500])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([700, 500])\n",
      "---\n",
      "e_id:  e5  C_dict[e_id].shape:  torch.Size([600, 300])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([600, 300])\n",
      "---\n",
      "#\n",
      "creating pytorch variables of input matrices...\n",
      "#\n",
      "__input_transformation - end\n",
      "__network_construction - start\n",
      "__network_construction - end\n",
      "dcmf - model construction - end\n",
      "#\n",
      "#\n",
      "dcmf.fit - start\n",
      "epoch:  1  total loss L:  6.098870754241943  Took  0.0  secs.\n",
      "epoch:  2  total loss L:  5.535494327545166  Took  0.0  secs.\n",
      "epoch:  3  total loss L:  5.551999092102051  Took  0.0  secs.\n",
      "epoch:  4  total loss L:  4.581932067871094  Took  0.0  secs.\n",
      "epoch:  5  total loss L:  4.546292304992676  Took  0.0  secs.\n",
      "epoch:  6  total loss L:  4.805540561676025  Took  0.0  secs.\n",
      "epoch:  7  total loss L:  4.570841312408447  Took  0.0  secs.\n",
      "epoch:  8  total loss L:  4.194031715393066  Took  0.0  secs.\n",
      "epoch:  9  total loss L:  4.098175048828125  Took  0.0  secs.\n",
      "epoch:  10  total loss L:  4.18422794342041  Took  0.0  secs.\n",
      "epoch:  11  total loss L:  4.170705318450928  Took  0.0  secs.\n",
      "epoch:  12  total loss L:  4.020012855529785  Took  0.0  secs.\n",
      "epoch:  13  total loss L:  3.8757119178771973  Took  0.0  secs.\n",
      "epoch:  14  total loss L:  3.817574977874756  Took  0.0  secs.\n",
      "epoch:  15  total loss L:  3.794236183166504  Took  0.0  secs.\n",
      "epoch:  16  total loss L:  3.7510128021240234  Took  0.0  secs.\n",
      "epoch:  17  total loss L:  3.6984901428222656  Took  0.0  secs.\n",
      "epoch:  18  total loss L:  3.660461902618408  Took  0.0  secs.\n",
      "epoch:  19  total loss L:  3.625589370727539  Took  0.0  secs.\n",
      "epoch:  20  total loss L:  3.5695395469665527  Took  0.0  secs.\n",
      "epoch:  21  total loss L:  3.5047411918640137  Took  0.0  secs.\n",
      "epoch:  22  total loss L:  3.4680325984954834  Took  0.0  secs.\n",
      "epoch:  23  total loss L:  3.4638895988464355  Took  0.0  secs.\n",
      "epoch:  24  total loss L:  3.45371675491333  Took  0.0  secs.\n",
      "epoch:  25  total loss L:  3.4092659950256348  Took  0.0  secs.\n",
      "epoch:  26  total loss L:  3.351655960083008  Took  0.0  secs.\n",
      "epoch:  27  total loss L:  3.3200578689575195  Took  0.0  secs.\n",
      "epoch:  28  total loss L:  3.3175830841064453  Took  0.0  secs.\n",
      "epoch:  29  total loss L:  3.3099992275238037  Took  0.0  secs.\n",
      "epoch:  30  total loss L:  3.275196075439453  Took  0.0  secs.\n",
      "epoch:  31  total loss L:  3.23069167137146  Took  0.0  secs.\n",
      "epoch:  32  total loss L:  3.205251693725586  Took  0.0  secs.\n",
      "epoch:  33  total loss L:  3.20013165473938  Took  0.0  secs.\n",
      "epoch:  34  total loss L:  3.1920278072357178  Took  0.0  secs.\n",
      "epoch:  35  total loss L:  3.167484760284424  Took  0.0  secs.\n",
      "epoch:  36  total loss L:  3.137162923812866  Took  0.0  secs.\n",
      "epoch:  37  total loss L:  3.1156301498413086  Took  0.0  secs.\n",
      "epoch:  38  total loss L:  3.102388858795166  Took  0.0  secs.\n",
      "epoch:  39  total loss L:  3.088651657104492  Took  0.0  secs.\n",
      "epoch:  40  total loss L:  3.07204532623291  Took  0.0  secs.\n",
      "epoch:  41  total loss L:  3.0558924674987793  Took  0.0  secs.\n",
      "epoch:  42  total loss L:  3.0402097702026367  Took  0.0  secs.\n",
      "epoch:  43  total loss L:  3.0225577354431152  Took  0.0  secs.\n",
      "epoch:  44  total loss L:  3.0049924850463867  Took  0.0  secs.\n",
      "epoch:  45  total loss L:  2.991750717163086  Took  0.0  secs.\n",
      "epoch:  46  total loss L:  2.981196880340576  Took  0.0  secs.\n",
      "epoch:  47  total loss L:  2.967519521713257  Took  0.0  secs.\n",
      "epoch:  48  total loss L:  2.950265884399414  Took  0.0  secs.\n",
      "epoch:  49  total loss L:  2.9351248741149902  Took  0.0  secs.\n",
      "epoch:  50  total loss L:  2.9244399070739746  Took  0.0  secs.\n",
      "epoch:  51  total loss L:  2.9137518405914307  Took  0.0  secs.\n",
      "epoch:  52  total loss L:  2.899702787399292  Took  0.0  secs.\n",
      "epoch:  53  total loss L:  2.8850338459014893  Took  0.0  secs.\n",
      "epoch:  54  total loss L:  2.873007297515869  Took  0.0  secs.\n",
      "epoch:  55  total loss L:  2.8623604774475098  Took  0.0  secs.\n",
      "epoch:  56  total loss L:  2.8507373332977295  Took  0.0  secs.\n",
      "epoch:  57  total loss L:  2.8385987281799316  Took  0.0  secs.\n",
      "epoch:  58  total loss L:  2.827031135559082  Took  0.0  secs.\n",
      "epoch:  59  total loss L:  2.815462827682495  Took  0.0  secs.\n",
      "epoch:  60  total loss L:  2.8037056922912598  Took  0.0  secs.\n",
      "epoch:  61  total loss L:  2.7929091453552246  Took  0.0  secs.\n",
      "epoch:  62  total loss L:  2.7829933166503906  Took  0.0  secs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  63  total loss L:  2.7723238468170166  Took  0.0  secs.\n",
      "epoch:  64  total loss L:  2.760782241821289  Took  0.0  secs.\n",
      "epoch:  65  total loss L:  2.750011920928955  Took  0.0  secs.\n",
      "epoch:  66  total loss L:  2.740318775177002  Took  0.0  secs.\n",
      "epoch:  67  total loss L:  2.7303850650787354  Took  0.0  secs.\n",
      "epoch:  68  total loss L:  2.71992564201355  Took  0.0  secs.\n",
      "epoch:  69  total loss L:  2.7098100185394287  Took  0.0  secs.\n",
      "epoch:  70  total loss L:  2.700101852416992  Took  0.0  secs.\n",
      "epoch:  71  total loss L:  2.6903038024902344  Took  0.0  secs.\n",
      "epoch:  72  total loss L:  2.6805920600891113  Took  0.0  secs.\n",
      "epoch:  73  total loss L:  2.6711485385894775  Took  0.0  secs.\n",
      "epoch:  74  total loss L:  2.6615967750549316  Took  0.0  secs.\n",
      "epoch:  75  total loss L:  2.6519875526428223  Took  0.0  secs.\n",
      "epoch:  76  total loss L:  2.6427907943725586  Took  0.0  secs.\n",
      "epoch:  77  total loss L:  2.6337900161743164  Took  0.0  secs.\n",
      "epoch:  78  total loss L:  2.6245133876800537  Took  0.0  secs.\n",
      "epoch:  79  total loss L:  2.6152327060699463  Took  0.0  secs.\n",
      "epoch:  80  total loss L:  2.606294870376587  Took  0.0  secs.\n",
      "epoch:  81  total loss L:  2.5974502563476562  Took  0.0  secs.\n",
      "epoch:  82  total loss L:  2.58854603767395  Took  0.0  secs.\n",
      "epoch:  83  total loss L:  2.5797369480133057  Took  0.0  secs.\n",
      "epoch:  84  total loss L:  2.57096529006958  Took  0.0  secs.\n",
      "epoch:  85  total loss L:  2.562187671661377  Took  0.0  secs.\n",
      "epoch:  86  total loss L:  2.553586959838867  Took  0.0  secs.\n",
      "epoch:  87  total loss L:  2.5450918674468994  Took  0.0  secs.\n",
      "epoch:  88  total loss L:  2.5365047454833984  Took  0.0  secs.\n",
      "epoch:  89  total loss L:  2.5279622077941895  Took  0.0  secs.\n",
      "epoch:  90  total loss L:  2.5195744037628174  Took  0.0  secs.\n",
      "epoch:  91  total loss L:  2.511202335357666  Took  0.0  secs.\n",
      "epoch:  92  total loss L:  2.502840995788574  Took  0.0  secs.\n",
      "epoch:  93  total loss L:  2.494558334350586  Took  0.0  secs.\n",
      "epoch:  94  total loss L:  2.4862935543060303  Took  0.0  secs.\n",
      "epoch:  95  total loss L:  2.4780731201171875  Took  0.0  secs.\n",
      "epoch:  96  total loss L:  2.4699392318725586  Took  0.0  secs.\n",
      "epoch:  97  total loss L:  2.461801767349243  Took  0.0  secs.\n",
      "epoch:  98  total loss L:  2.453679323196411  Took  0.0  secs.\n",
      "epoch:  99  total loss L:  2.4456467628479004  Took  0.0  secs.\n",
      "epoch:  100  total loss L:  2.4376440048217773  Took  0.0  secs.\n",
      "#\n",
      "dcmf.fit - end\n",
      "dataset_name:  ds3\n",
      "#\n",
      "X0.shape:  (200, 500)\n",
      "X1.shape:  (200, 300)\n",
      "X2.shape:  (200, 400)\n",
      "X3.shape:  (700, 500)\n",
      "X4.shape:  (600, 300)\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "WARNING: The following parameters are unused since no validation data provided.\n",
      "val_metric:  auc\n",
      "at_k:  10\n",
      "is_val_transpose:  True\n",
      "#\n",
      "dCMF:\n",
      "---\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  0.0001\n",
      "weight_decay:  0.01\n",
      "convg_thres:  -0.1\n",
      "max_epochs:  100\n",
      "isPretrain:  False\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  1\n",
      "k:  100\n",
      "kf:  0.0005\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  True\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  0\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "## fold_num:  1  ##\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "WARNING: The following parameters are unused since no validation data provided.\n",
      "val_metric:  auc\n",
      "at_k:  10\n",
      "is_val_transpose:  True\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  0.0001\n",
      "weight_decay:  0.01\n",
      "convg_thres:  -0.1\n",
      "max_epochs:  100\n",
      "isPretrain:  False\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  1\n",
      "k:  100\n",
      "kf:  0.0005\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  True\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  0\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "dcmf - model construction - start\n",
      "__input_transformation - start\n",
      "#\n",
      "concatenated-matrix construction...\n",
      "e_id:  e0\n",
      "X_id_list:  ['X0', 'X1', 'X2']\n",
      "X_id:  X0\n",
      "X[X_id].shape:  (200, 500)\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (200, 300)\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (200, 400)\n",
      "C_dict[e].shape:  torch.Size([200, 1200])\n",
      "---\n",
      "e_id:  e1\n",
      "X_id_list:  ['X0', 'X3']\n",
      "X_id:  X0\n",
      "X[X_id].shape:  (200, 500)\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (700, 500)\n",
      "C_dict[e].shape:  torch.Size([500, 900])\n",
      "---\n",
      "e_id:  e2\n",
      "X_id_list:  ['X1', 'X4']\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (200, 300)\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (600, 300)\n",
      "C_dict[e].shape:  torch.Size([300, 800])\n",
      "---\n",
      "e_id:  e3\n",
      "X_id_list:  ['X2']\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (200, 400)\n",
      "C_dict[e].shape:  torch.Size([400, 200])\n",
      "---\n",
      "e_id:  e4\n",
      "X_id_list:  ['X3']\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (700, 500)\n",
      "C_dict[e].shape:  torch.Size([700, 500])\n",
      "---\n",
      "e_id:  e5\n",
      "X_id_list:  ['X4']\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (600, 300)\n",
      "C_dict[e].shape:  torch.Size([600, 300])\n",
      "---\n",
      "#\n",
      "concatenated-matrix chunking...\n",
      "#\n",
      "e_id:  e0 , min_num_datapoints:  200 , num_chunks:  1\n",
      "e_id:  e3 , min_features:  200 , k:  100\n",
      "#\n",
      "e_id:  e0  C_dict[e_id].shape:  torch.Size([200, 1200])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([200, 1200])\n",
      "---\n",
      "e_id:  e1  C_dict[e_id].shape:  torch.Size([500, 900])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([500, 900])\n",
      "---\n",
      "e_id:  e2  C_dict[e_id].shape:  torch.Size([300, 800])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([300, 800])\n",
      "---\n",
      "e_id:  e3  C_dict[e_id].shape:  torch.Size([400, 200])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([400, 200])\n",
      "---\n",
      "e_id:  e4  C_dict[e_id].shape:  torch.Size([700, 500])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([700, 500])\n",
      "---\n",
      "e_id:  e5  C_dict[e_id].shape:  torch.Size([600, 300])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([600, 300])\n",
      "---\n",
      "#\n",
      "creating pytorch variables of input matrices...\n",
      "#\n",
      "__input_transformation - end\n",
      "__network_construction - start\n",
      "__network_construction - end\n",
      "dcmf - model construction - end\n",
      "#\n",
      "#\n",
      "dcmf.fit - start\n",
      "epoch:  1  total loss L:  7.28859806060791  Took  0.0  secs.\n",
      "epoch:  2  total loss L:  5.049684524536133  Took  0.0  secs.\n",
      "epoch:  3  total loss L:  6.125699043273926  Took  0.0  secs.\n",
      "epoch:  4  total loss L:  5.204946517944336  Took  0.0  secs.\n",
      "epoch:  5  total loss L:  4.303143501281738  Took  0.0  secs.\n",
      "epoch:  6  total loss L:  4.421825408935547  Took  0.0  secs.\n",
      "epoch:  7  total loss L:  4.846334457397461  Took  0.0  secs.\n",
      "epoch:  8  total loss L:  4.745071887969971  Took  0.0  secs.\n",
      "epoch:  9  total loss L:  4.253446578979492  Took  0.0  secs.\n",
      "epoch:  10  total loss L:  3.881504535675049  Took  0.0  secs.\n",
      "epoch:  11  total loss L:  3.8621363639831543  Took  0.0  secs.\n",
      "epoch:  12  total loss L:  4.008732795715332  Took  0.0  secs.\n",
      "epoch:  13  total loss L:  4.024514198303223  Took  0.0  secs.\n",
      "epoch:  14  total loss L:  3.8465263843536377  Took  0.0  secs.\n",
      "epoch:  15  total loss L:  3.6345009803771973  Took  0.0  secs.\n",
      "epoch:  16  total loss L:  3.550884246826172  Took  0.0  secs.\n",
      "epoch:  17  total loss L:  3.601742744445801  Took  0.0  secs.\n",
      "epoch:  18  total loss L:  3.6580934524536133  Took  0.0  secs.\n",
      "epoch:  19  total loss L:  3.611865520477295  Took  0.0  secs.\n",
      "epoch:  20  total loss L:  3.4807140827178955  Took  0.0  secs.\n",
      "epoch:  21  total loss L:  3.363255023956299  Took  0.0  secs.\n",
      "epoch:  22  total loss L:  3.3285975456237793  Took  0.0  secs.\n",
      "epoch:  23  total loss L:  3.356375217437744  Took  0.0  secs.\n",
      "epoch:  24  total loss L:  3.3727502822875977  Took  0.0  secs.\n",
      "epoch:  25  total loss L:  3.3329217433929443  Took  0.0  secs.\n",
      "epoch:  26  total loss L:  3.258837938308716  Took  0.0  secs.\n",
      "epoch:  27  total loss L:  3.203613758087158  Took  0.0  secs.\n",
      "epoch:  28  total loss L:  3.192556381225586  Took  0.0  secs.\n",
      "epoch:  29  total loss L:  3.2016303539276123  Took  0.0  secs.\n",
      "epoch:  30  total loss L:  3.1897785663604736  Took  0.0  secs.\n",
      "epoch:  31  total loss L:  3.144923686981201  Took  0.0  secs.\n",
      "epoch:  32  total loss L:  3.0926358699798584  Took  0.0  secs.\n",
      "epoch:  33  total loss L:  3.0640947818756104  Took  0.0  secs.\n",
      "epoch:  34  total loss L:  3.062899589538574  Took  0.0  secs.\n",
      "epoch:  35  total loss L:  3.0650851726531982  Took  0.0  secs.\n",
      "epoch:  36  total loss L:  3.048111915588379  Took  0.0  secs.\n",
      "epoch:  37  total loss L:  3.014069080352783  Took  0.0  secs.\n",
      "epoch:  38  total loss L:  2.983043909072876  Took  0.0  secs.\n",
      "epoch:  39  total loss L:  2.9684295654296875  Took  0.0  secs.\n",
      "epoch:  40  total loss L:  2.9634976387023926  Took  0.0  secs.\n",
      "epoch:  41  total loss L:  2.9524385929107666  Took  0.0  secs.\n",
      "epoch:  42  total loss L:  2.9301071166992188  Took  0.0  secs.\n",
      "epoch:  43  total loss L:  2.9063916206359863  Took  0.0  secs.\n",
      "epoch:  44  total loss L:  2.892265796661377  Took  0.0  secs.\n",
      "epoch:  45  total loss L:  2.886430025100708  Took  0.0  secs.\n",
      "epoch:  46  total loss L:  2.878300189971924  Took  0.0  secs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  47  total loss L:  2.8618104457855225  Took  0.0  secs.\n",
      "epoch:  48  total loss L:  2.842006206512451  Took  0.0  secs.\n",
      "epoch:  49  total loss L:  2.8273394107818604  Took  0.0  secs.\n",
      "epoch:  50  total loss L:  2.8189496994018555  Took  0.0  secs.\n",
      "epoch:  51  total loss L:  2.8105130195617676  Took  0.0  secs.\n",
      "epoch:  52  total loss L:  2.7974491119384766  Took  0.0  secs.\n",
      "epoch:  53  total loss L:  2.782468318939209  Took  0.0  secs.\n",
      "epoch:  54  total loss L:  2.7707762718200684  Took  0.0  secs.\n",
      "epoch:  55  total loss L:  2.7626733779907227  Took  0.0  secs.\n",
      "epoch:  56  total loss L:  2.753753185272217  Took  0.0  secs.\n",
      "epoch:  57  total loss L:  2.7414915561676025  Took  0.0  secs.\n",
      "epoch:  58  total loss L:  2.72845196723938  Took  0.0  secs.\n",
      "epoch:  59  total loss L:  2.718111038208008  Took  0.0  secs.\n",
      "epoch:  60  total loss L:  2.70997953414917  Took  0.0  secs.\n",
      "epoch:  61  total loss L:  2.700904369354248  Took  0.0  secs.\n",
      "epoch:  62  total loss L:  2.6899144649505615  Took  0.0  secs.\n",
      "epoch:  63  total loss L:  2.679206132888794  Took  0.0  secs.\n",
      "epoch:  64  total loss L:  2.67037296295166  Took  0.0  secs.\n",
      "epoch:  65  total loss L:  2.662081003189087  Took  0.0  secs.\n",
      "epoch:  66  total loss L:  2.652477502822876  Took  0.0  secs.\n",
      "epoch:  67  total loss L:  2.6420950889587402  Took  0.0  secs.\n",
      "epoch:  68  total loss L:  2.6327409744262695  Took  0.0  secs.\n",
      "epoch:  69  total loss L:  2.6245644092559814  Took  0.0  secs.\n",
      "epoch:  70  total loss L:  2.6160695552825928  Took  0.0  secs.\n",
      "epoch:  71  total loss L:  2.606684923171997  Took  0.0  secs.\n",
      "epoch:  72  total loss L:  2.597489833831787  Took  0.0  secs.\n",
      "epoch:  73  total loss L:  2.5891878604888916  Took  0.0  secs.\n",
      "epoch:  74  total loss L:  2.580996036529541  Took  0.0  secs.\n",
      "epoch:  75  total loss L:  2.5721592903137207  Took  0.0  secs.\n",
      "epoch:  76  total loss L:  2.563225269317627  Took  0.0  secs.\n",
      "epoch:  77  total loss L:  2.554953098297119  Took  0.0  secs.\n",
      "epoch:  78  total loss L:  2.547001838684082  Took  0.0  secs.\n",
      "epoch:  79  total loss L:  2.538668632507324  Took  0.0  secs.\n",
      "epoch:  80  total loss L:  2.5301380157470703  Took  0.0  secs.\n",
      "epoch:  81  total loss L:  2.521998405456543  Took  0.0  secs.\n",
      "epoch:  82  total loss L:  2.514120578765869  Took  0.0  secs.\n",
      "epoch:  83  total loss L:  2.505995035171509  Took  0.0  secs.\n",
      "epoch:  84  total loss L:  2.497718095779419  Took  0.0  secs.\n",
      "epoch:  85  total loss L:  2.4897360801696777  Took  0.0  secs.\n",
      "epoch:  86  total loss L:  2.4819717407226562  Took  0.0  secs.\n",
      "epoch:  87  total loss L:  2.474043130874634  Took  0.0  secs.\n",
      "epoch:  88  total loss L:  2.466017007827759  Took  0.0  secs.\n",
      "epoch:  89  total loss L:  2.4581990242004395  Took  0.0  secs.\n",
      "epoch:  90  total loss L:  2.450500011444092  Took  0.0  secs.\n",
      "epoch:  91  total loss L:  2.4426674842834473  Took  0.0  secs.\n",
      "epoch:  92  total loss L:  2.4348111152648926  Took  0.0  secs.\n",
      "epoch:  93  total loss L:  2.4271342754364014  Took  0.0  secs.\n",
      "epoch:  94  total loss L:  2.419513702392578  Took  0.0  secs.\n",
      "epoch:  95  total loss L:  2.4117956161499023  Took  0.0  secs.\n",
      "epoch:  96  total loss L:  2.4041073322296143  Took  0.0  secs.\n",
      "epoch:  97  total loss L:  2.396543264389038  Took  0.0  secs.\n",
      "epoch:  98  total loss L:  2.388967990875244  Took  0.0  secs.\n",
      "epoch:  99  total loss L:  2.3813366889953613  Took  0.0  secs.\n",
      "epoch:  100  total loss L:  2.3737778663635254  Took  0.0  secs.\n",
      "#\n",
      "dcmf.fit - end\n",
      "dataset_name:  dn1\n",
      "#\n",
      "X0.shape:  (200, 500)\n",
      "X1.shape:  (200, 300)\n",
      "X2.shape:  (200, 400)\n",
      "X3.shape:  (700, 500)\n",
      "X4.shape:  (600, 300)\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "WARNING: The following parameters are unused since no validation data provided.\n",
      "val_metric:  auc\n",
      "at_k:  10\n",
      "is_val_transpose:  True\n",
      "#\n",
      "dCMF:\n",
      "---\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  0.0001\n",
      "weight_decay:  0.01\n",
      "convg_thres:  -0.1\n",
      "max_epochs:  100\n",
      "isPretrain:  False\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  1\n",
      "k:  100\n",
      "kf:  0.0005\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  True\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  0\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "## fold_num:  1  ##\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "WARNING: The following parameters are unused since no validation data provided.\n",
      "val_metric:  auc\n",
      "at_k:  10\n",
      "is_val_transpose:  True\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  0.0001\n",
      "weight_decay:  0.01\n",
      "convg_thres:  -0.1\n",
      "max_epochs:  100\n",
      "isPretrain:  False\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  1\n",
      "k:  100\n",
      "kf:  0.0005\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  True\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  0\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "dcmf - model construction - start\n",
      "__input_transformation - start\n",
      "#\n",
      "concatenated-matrix construction...\n",
      "e_id:  e0\n",
      "X_id_list:  ['X0', 'X1', 'X2']\n",
      "X_id:  X0\n",
      "X[X_id].shape:  (200, 500)\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (200, 300)\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (200, 400)\n",
      "C_dict[e].shape:  torch.Size([200, 1200])\n",
      "---\n",
      "e_id:  e1\n",
      "X_id_list:  ['X0', 'X3']\n",
      "X_id:  X0\n",
      "X[X_id].shape:  (200, 500)\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (700, 500)\n",
      "C_dict[e].shape:  torch.Size([500, 900])\n",
      "---\n",
      "e_id:  e2\n",
      "X_id_list:  ['X1', 'X4']\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (200, 300)\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (600, 300)\n",
      "C_dict[e].shape:  torch.Size([300, 800])\n",
      "---\n",
      "e_id:  e3\n",
      "X_id_list:  ['X2']\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (200, 400)\n",
      "C_dict[e].shape:  torch.Size([400, 200])\n",
      "---\n",
      "e_id:  e4\n",
      "X_id_list:  ['X3']\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (700, 500)\n",
      "C_dict[e].shape:  torch.Size([700, 500])\n",
      "---\n",
      "e_id:  e5\n",
      "X_id_list:  ['X4']\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (600, 300)\n",
      "C_dict[e].shape:  torch.Size([600, 300])\n",
      "---\n",
      "#\n",
      "concatenated-matrix chunking...\n",
      "#\n",
      "e_id:  e0 , min_num_datapoints:  200 , num_chunks:  1\n",
      "e_id:  e3 , min_features:  200 , k:  100\n",
      "#\n",
      "e_id:  e0  C_dict[e_id].shape:  torch.Size([200, 1200])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([200, 1200])\n",
      "---\n",
      "e_id:  e1  C_dict[e_id].shape:  torch.Size([500, 900])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([500, 900])\n",
      "---\n",
      "e_id:  e2  C_dict[e_id].shape:  torch.Size([300, 800])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([300, 800])\n",
      "---\n",
      "e_id:  e3  C_dict[e_id].shape:  torch.Size([400, 200])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([400, 200])\n",
      "---\n",
      "e_id:  e4  C_dict[e_id].shape:  torch.Size([700, 500])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([700, 500])\n",
      "---\n",
      "e_id:  e5  C_dict[e_id].shape:  torch.Size([600, 300])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([600, 300])\n",
      "---\n",
      "#\n",
      "creating pytorch variables of input matrices...\n",
      "#\n",
      "__input_transformation - end\n",
      "__network_construction - start\n",
      "__network_construction - end\n",
      "dcmf - model construction - end\n",
      "#\n",
      "#\n",
      "dcmf.fit - start\n",
      "epoch:  1  total loss L:  8.123088836669922  Took  0.0  secs.\n",
      "epoch:  2  total loss L:  7.676602363586426  Took  0.0  secs.\n",
      "epoch:  3  total loss L:  6.867487907409668  Took  0.0  secs.\n",
      "epoch:  4  total loss L:  6.778012275695801  Took  0.0  secs.\n",
      "epoch:  5  total loss L:  6.500610828399658  Took  0.0  secs.\n",
      "epoch:  6  total loss L:  6.024609088897705  Took  0.0  secs.\n",
      "epoch:  7  total loss L:  5.889652729034424  Took  0.0  secs.\n",
      "epoch:  8  total loss L:  5.984316825866699  Took  0.0  secs.\n",
      "epoch:  9  total loss L:  5.891659736633301  Took  0.0  secs.\n",
      "epoch:  10  total loss L:  5.599613666534424  Took  0.0  secs.\n",
      "epoch:  11  total loss L:  5.364659786224365  Took  0.0  secs.\n",
      "epoch:  12  total loss L:  5.298320770263672  Took  0.0  secs.\n",
      "epoch:  13  total loss L:  5.2770490646362305  Took  0.0  secs.\n",
      "epoch:  14  total loss L:  5.176792144775391  Took  0.0  secs.\n",
      "epoch:  15  total loss L:  5.028764247894287  Took  0.0  secs.\n",
      "epoch:  16  total loss L:  4.927906036376953  Took  0.0  secs.\n",
      "epoch:  17  total loss L:  4.895092964172363  Took  0.0  secs.\n",
      "epoch:  18  total loss L:  4.875069618225098  Took  0.0  secs.\n",
      "epoch:  19  total loss L:  4.821290016174316  Took  0.0  secs.\n",
      "epoch:  20  total loss L:  4.7390828132629395  Took  0.0  secs.\n",
      "epoch:  21  total loss L:  4.65887451171875  Took  0.0  secs.\n",
      "epoch:  22  total loss L:  4.594425201416016  Took  0.0  secs.\n",
      "epoch:  23  total loss L:  4.542430877685547  Took  0.0  secs.\n",
      "epoch:  24  total loss L:  4.501737117767334  Took  0.0  secs.\n",
      "epoch:  25  total loss L:  4.471546649932861  Took  0.0  secs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  26  total loss L:  4.442094802856445  Took  0.0  secs.\n",
      "epoch:  27  total loss L:  4.4008307456970215  Took  0.0  secs.\n",
      "epoch:  28  total loss L:  4.349030494689941  Took  0.0  secs.\n",
      "epoch:  29  total loss L:  4.3032684326171875  Took  0.0  secs.\n",
      "epoch:  30  total loss L:  4.2751784324646  Took  0.0  secs.\n",
      "epoch:  31  total loss L:  4.257008075714111  Took  0.0  secs.\n",
      "epoch:  32  total loss L:  4.233029365539551  Took  0.0  secs.\n",
      "epoch:  33  total loss L:  4.199867248535156  Took  0.0  secs.\n",
      "epoch:  34  total loss L:  4.167097091674805  Took  0.0  secs.\n",
      "epoch:  35  total loss L:  4.140471935272217  Took  0.0  secs.\n",
      "epoch:  36  total loss L:  4.11481237411499  Took  0.0  secs.\n",
      "epoch:  37  total loss L:  4.085461616516113  Took  0.0  secs.\n",
      "epoch:  38  total loss L:  4.057570934295654  Took  0.0  secs.\n",
      "epoch:  39  total loss L:  4.037782192230225  Took  0.0  secs.\n",
      "epoch:  40  total loss L:  4.022586822509766  Took  0.0  secs.\n",
      "epoch:  41  total loss L:  4.002933979034424  Took  0.0  secs.\n",
      "epoch:  42  total loss L:  3.9778709411621094  Took  0.0  secs.\n",
      "epoch:  43  total loss L:  3.955326795578003  Took  0.0  secs.\n",
      "epoch:  44  total loss L:  3.939417839050293  Took  0.0  secs.\n",
      "epoch:  45  total loss L:  3.9246814250946045  Took  0.0  secs.\n",
      "epoch:  46  total loss L:  3.9052367210388184  Took  0.0  secs.\n",
      "epoch:  47  total loss L:  3.883236885070801  Took  0.0  secs.\n",
      "epoch:  48  total loss L:  3.8642961978912354  Took  0.0  secs.\n",
      "epoch:  49  total loss L:  3.8492588996887207  Took  0.0  secs.\n",
      "epoch:  50  total loss L:  3.8349764347076416  Took  0.0  secs.\n",
      "epoch:  51  total loss L:  3.8200464248657227  Took  0.0  secs.\n",
      "epoch:  52  total loss L:  3.8052515983581543  Took  0.0  secs.\n",
      "epoch:  53  total loss L:  3.79036283493042  Took  0.0  secs.\n",
      "epoch:  54  total loss L:  3.7746315002441406  Took  0.0  secs.\n",
      "epoch:  55  total loss L:  3.7591609954833984  Took  0.0  secs.\n",
      "epoch:  56  total loss L:  3.7453975677490234  Took  0.0  secs.\n",
      "epoch:  57  total loss L:  3.732426881790161  Took  0.0  secs.\n",
      "epoch:  58  total loss L:  3.718564748764038  Took  0.0  secs.\n",
      "epoch:  59  total loss L:  3.704319953918457  Took  0.0  secs.\n",
      "epoch:  60  total loss L:  3.691171407699585  Took  0.0  secs.\n",
      "epoch:  61  total loss L:  3.6788392066955566  Took  0.0  secs.\n",
      "epoch:  62  total loss L:  3.6662001609802246  Took  0.0  secs.\n",
      "epoch:  63  total loss L:  3.65346622467041  Took  0.0  secs.\n",
      "epoch:  64  total loss L:  3.6412642002105713  Took  0.0  secs.\n",
      "epoch:  65  total loss L:  3.6290671825408936  Took  0.0  secs.\n",
      "epoch:  66  total loss L:  3.616405487060547  Took  0.0  secs.\n",
      "epoch:  67  total loss L:  3.6040573120117188  Took  0.0  secs.\n",
      "epoch:  68  total loss L:  3.592569351196289  Took  0.0  secs.\n",
      "epoch:  69  total loss L:  3.581228494644165  Took  0.0  secs.\n",
      "epoch:  70  total loss L:  3.5695011615753174  Took  0.0  secs.\n",
      "epoch:  71  total loss L:  3.5578370094299316  Took  0.0  secs.\n",
      "epoch:  72  total loss L:  3.5465168952941895  Took  0.0  secs.\n",
      "epoch:  73  total loss L:  3.535264253616333  Took  0.0  secs.\n",
      "epoch:  74  total loss L:  3.5240750312805176  Took  0.0  secs.\n",
      "epoch:  75  total loss L:  3.513093948364258  Took  0.0  secs.\n",
      "epoch:  76  total loss L:  3.502077579498291  Took  0.0  secs.\n",
      "epoch:  77  total loss L:  3.4909403324127197  Took  0.0  secs.\n",
      "epoch:  78  total loss L:  3.4800381660461426  Took  0.0  secs.\n",
      "epoch:  79  total loss L:  3.469402313232422  Took  0.0  secs.\n",
      "epoch:  80  total loss L:  3.4587020874023438  Took  0.0  secs.\n",
      "epoch:  81  total loss L:  3.447977066040039  Took  0.0  secs.\n",
      "epoch:  82  total loss L:  3.4374184608459473  Took  0.0  secs.\n",
      "epoch:  83  total loss L:  3.426877737045288  Took  0.0  secs.\n",
      "epoch:  84  total loss L:  3.416283130645752  Took  0.0  secs.\n",
      "epoch:  85  total loss L:  3.4058175086975098  Took  0.0  secs.\n",
      "epoch:  86  total loss L:  3.3954474925994873  Took  0.0  secs.\n",
      "epoch:  87  total loss L:  3.385031223297119  Took  0.0  secs.\n",
      "epoch:  88  total loss L:  3.374645948410034  Took  0.0  secs.\n",
      "epoch:  89  total loss L:  3.3643362522125244  Took  0.0  secs.\n",
      "epoch:  90  total loss L:  3.3540358543395996  Took  0.0  secs.\n",
      "epoch:  91  total loss L:  3.343792200088501  Took  0.0  secs.\n",
      "epoch:  92  total loss L:  3.3335981369018555  Took  0.0  secs.\n",
      "epoch:  93  total loss L:  3.3233580589294434  Took  0.0  secs.\n",
      "epoch:  94  total loss L:  3.3131356239318848  Took  0.0  secs.\n",
      "epoch:  95  total loss L:  3.302994966506958  Took  0.0  secs.\n",
      "epoch:  96  total loss L:  3.292853832244873  Took  0.0  secs.\n",
      "epoch:  97  total loss L:  3.2827162742614746  Took  0.0  secs.\n",
      "epoch:  98  total loss L:  3.2726244926452637  Took  0.0  secs.\n",
      "epoch:  99  total loss L:  3.2625317573547363  Took  0.0  secs.\n",
      "epoch:  100  total loss L:  3.252450942993164  Took  0.0  secs.\n",
      "#\n",
      "dcmf.fit - end\n",
      "dataset_name:  dn2\n",
      "#\n",
      "X0.shape:  (200, 500)\n",
      "X1.shape:  (200, 300)\n",
      "X2.shape:  (200, 400)\n",
      "X3.shape:  (700, 500)\n",
      "X4.shape:  (600, 300)\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "WARNING: The following parameters are unused since no validation data provided.\n",
      "val_metric:  auc\n",
      "at_k:  10\n",
      "is_val_transpose:  True\n",
      "#\n",
      "dCMF:\n",
      "---\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  0.0001\n",
      "weight_decay:  0.01\n",
      "convg_thres:  -0.1\n",
      "max_epochs:  100\n",
      "isPretrain:  False\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  1\n",
      "k:  100\n",
      "kf:  0.0005\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  True\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  0\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "## fold_num:  1  ##\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "WARNING: The following parameters are unused since no validation data provided.\n",
      "val_metric:  auc\n",
      "at_k:  10\n",
      "is_val_transpose:  True\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  0.0001\n",
      "weight_decay:  0.01\n",
      "convg_thres:  -0.1\n",
      "max_epochs:  100\n",
      "isPretrain:  False\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  1\n",
      "k:  100\n",
      "kf:  0.0005\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  True\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  0\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "dcmf - model construction - start\n",
      "__input_transformation - start\n",
      "#\n",
      "concatenated-matrix construction...\n",
      "e_id:  e0\n",
      "X_id_list:  ['X0', 'X1', 'X2']\n",
      "X_id:  X0\n",
      "X[X_id].shape:  (200, 500)\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (200, 300)\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (200, 400)\n",
      "C_dict[e].shape:  torch.Size([200, 1200])\n",
      "---\n",
      "e_id:  e1\n",
      "X_id_list:  ['X0', 'X3']\n",
      "X_id:  X0\n",
      "X[X_id].shape:  (200, 500)\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (700, 500)\n",
      "C_dict[e].shape:  torch.Size([500, 900])\n",
      "---\n",
      "e_id:  e2\n",
      "X_id_list:  ['X1', 'X4']\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (200, 300)\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (600, 300)\n",
      "C_dict[e].shape:  torch.Size([300, 800])\n",
      "---\n",
      "e_id:  e3\n",
      "X_id_list:  ['X2']\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (200, 400)\n",
      "C_dict[e].shape:  torch.Size([400, 200])\n",
      "---\n",
      "e_id:  e4\n",
      "X_id_list:  ['X3']\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (700, 500)\n",
      "C_dict[e].shape:  torch.Size([700, 500])\n",
      "---\n",
      "e_id:  e5\n",
      "X_id_list:  ['X4']\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (600, 300)\n",
      "C_dict[e].shape:  torch.Size([600, 300])\n",
      "---\n",
      "#\n",
      "concatenated-matrix chunking...\n",
      "#\n",
      "e_id:  e0 , min_num_datapoints:  200 , num_chunks:  1\n",
      "e_id:  e3 , min_features:  200 , k:  100\n",
      "#\n",
      "e_id:  e0  C_dict[e_id].shape:  torch.Size([200, 1200])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([200, 1200])\n",
      "---\n",
      "e_id:  e1  C_dict[e_id].shape:  torch.Size([500, 900])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([500, 900])\n",
      "---\n",
      "e_id:  e2  C_dict[e_id].shape:  torch.Size([300, 800])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([300, 800])\n",
      "---\n",
      "e_id:  e3  C_dict[e_id].shape:  torch.Size([400, 200])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([400, 200])\n",
      "---\n",
      "e_id:  e4  C_dict[e_id].shape:  torch.Size([700, 500])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([700, 500])\n",
      "---\n",
      "e_id:  e5  C_dict[e_id].shape:  torch.Size([600, 300])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([600, 300])\n",
      "---\n",
      "#\n",
      "creating pytorch variables of input matrices...\n",
      "#\n",
      "__input_transformation - end\n",
      "__network_construction - start\n",
      "__network_construction - end\n",
      "dcmf - model construction - end\n",
      "#\n",
      "#\n",
      "dcmf.fit - start\n",
      "epoch:  1  total loss L:  13.049173355102539  Took  0.0  secs.\n",
      "epoch:  2  total loss L:  7.530649185180664  Took  0.0  secs.\n",
      "epoch:  3  total loss L:  7.98090124130249  Took  0.0  secs.\n",
      "epoch:  4  total loss L:  9.12794303894043  Took  0.0  secs.\n",
      "epoch:  5  total loss L:  8.63461971282959  Took  0.0  secs.\n",
      "epoch:  6  total loss L:  7.429024696350098  Took  0.0  secs.\n",
      "epoch:  7  total loss L:  6.508984565734863  Took  0.0  secs.\n",
      "epoch:  8  total loss L:  6.265703201293945  Took  0.0  secs.\n",
      "epoch:  9  total loss L:  6.516421318054199  Took  0.0  secs.\n",
      "epoch:  10  total loss L:  6.782950401306152  Took  0.0  secs.\n",
      "epoch:  11  total loss L:  6.728373050689697  Took  0.0  secs.\n",
      "epoch:  12  total loss L:  6.359105110168457  Took  0.0  secs.\n",
      "epoch:  13  total loss L:  5.900185585021973  Took  0.0  secs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  14  total loss L:  5.580731391906738  Took  0.0  secs.\n",
      "epoch:  15  total loss L:  5.495172500610352  Took  0.0  secs.\n",
      "epoch:  16  total loss L:  5.5786638259887695  Took  0.0  secs.\n",
      "epoch:  17  total loss L:  5.681441783905029  Took  0.0  secs.\n",
      "epoch:  18  total loss L:  5.682453155517578  Took  0.0  secs.\n",
      "epoch:  19  total loss L:  5.557621002197266  Took  0.0  secs.\n",
      "epoch:  20  total loss L:  5.367086410522461  Took  0.0  secs.\n",
      "epoch:  21  total loss L:  5.196890830993652  Took  0.0  secs.\n",
      "epoch:  22  total loss L:  5.104503154754639  Took  0.0  secs.\n",
      "epoch:  23  total loss L:  5.094219207763672  Took  0.0  secs.\n",
      "epoch:  24  total loss L:  5.125699043273926  Took  0.0  secs.\n",
      "epoch:  25  total loss L:  5.144561767578125  Took  0.0  secs.\n",
      "epoch:  26  total loss L:  5.115379333496094  Took  0.0  secs.\n",
      "epoch:  27  total loss L:  5.038349151611328  Took  0.0  secs.\n",
      "epoch:  28  total loss L:  4.943001747131348  Took  0.0  secs.\n",
      "epoch:  29  total loss L:  4.866214752197266  Took  0.0  secs.\n",
      "epoch:  30  total loss L:  4.829015731811523  Took  0.0  secs.\n",
      "epoch:  31  total loss L:  4.825875282287598  Took  0.0  secs.\n",
      "epoch:  32  total loss L:  4.832000732421875  Took  0.0  secs.\n",
      "epoch:  33  total loss L:  4.822077751159668  Took  0.0  secs.\n",
      "epoch:  34  total loss L:  4.786396026611328  Took  0.0  secs.\n",
      "epoch:  35  total loss L:  4.733917236328125  Took  0.0  secs.\n",
      "epoch:  36  total loss L:  4.682903289794922  Took  0.0  secs.\n",
      "epoch:  37  total loss L:  4.647839069366455  Took  0.0  secs.\n",
      "epoch:  38  total loss L:  4.631458282470703  Took  0.0  secs.\n",
      "epoch:  39  total loss L:  4.625353813171387  Took  0.0  secs.\n",
      "epoch:  40  total loss L:  4.617004871368408  Took  0.0  secs.\n",
      "epoch:  41  total loss L:  4.597945213317871  Took  0.0  secs.\n",
      "epoch:  42  total loss L:  4.568195343017578  Took  0.0  secs.\n",
      "epoch:  43  total loss L:  4.5349626541137695  Took  0.0  secs.\n",
      "epoch:  44  total loss L:  4.50708532333374  Took  0.0  secs.\n",
      "epoch:  45  total loss L:  4.488979816436768  Took  0.0  secs.\n",
      "epoch:  46  total loss L:  4.478085517883301  Took  0.0  secs.\n",
      "epoch:  47  total loss L:  4.467485427856445  Took  0.0  secs.\n",
      "epoch:  48  total loss L:  4.451495170593262  Took  0.0  secs.\n",
      "epoch:  49  total loss L:  4.4296488761901855  Took  0.0  secs.\n",
      "epoch:  50  total loss L:  4.406098365783691  Took  0.0  secs.\n",
      "epoch:  51  total loss L:  4.385633945465088  Took  0.0  secs.\n",
      "epoch:  52  total loss L:  4.370092391967773  Took  0.0  secs.\n",
      "epoch:  53  total loss L:  4.357708930969238  Took  0.0  secs.\n",
      "epoch:  54  total loss L:  4.345065116882324  Took  0.0  secs.\n",
      "epoch:  55  total loss L:  4.329662322998047  Took  0.0  secs.\n",
      "epoch:  56  total loss L:  4.31148624420166  Took  0.0  secs.\n",
      "epoch:  57  total loss L:  4.292781829833984  Took  0.0  secs.\n",
      "epoch:  58  total loss L:  4.2761735916137695  Took  0.0  secs.\n",
      "epoch:  59  total loss L:  4.262490272521973  Took  0.0  secs.\n",
      "epoch:  60  total loss L:  4.250199317932129  Took  0.0  secs.\n",
      "epoch:  61  total loss L:  4.236983299255371  Took  0.0  secs.\n",
      "epoch:  62  total loss L:  4.221844673156738  Took  0.0  secs.\n",
      "epoch:  63  total loss L:  4.205662250518799  Took  0.0  secs.\n",
      "epoch:  64  total loss L:  4.190061092376709  Took  0.0  secs.\n",
      "epoch:  65  total loss L:  4.175997734069824  Took  0.0  secs.\n",
      "epoch:  66  total loss L:  4.163207054138184  Took  0.0  secs.\n",
      "epoch:  67  total loss L:  4.15057897567749  Took  0.0  secs.\n",
      "epoch:  68  total loss L:  4.137089729309082  Took  0.0  secs.\n",
      "epoch:  69  total loss L:  4.122673034667969  Took  0.0  secs.\n",
      "epoch:  70  total loss L:  4.108224868774414  Took  0.0  secs.\n",
      "epoch:  71  total loss L:  4.094626426696777  Took  0.0  secs.\n",
      "epoch:  72  total loss L:  4.081884860992432  Took  0.0  secs.\n",
      "epoch:  73  total loss L:  4.069315433502197  Took  0.0  secs.\n",
      "epoch:  74  total loss L:  4.056326389312744  Took  0.0  secs.\n",
      "epoch:  75  total loss L:  4.04287052154541  Took  0.0  secs.\n",
      "epoch:  76  total loss L:  4.0293660163879395  Took  0.0  secs.\n",
      "epoch:  77  total loss L:  4.016304016113281  Took  0.0  secs.\n",
      "epoch:  78  total loss L:  4.003775119781494  Took  0.0  secs.\n",
      "epoch:  79  total loss L:  3.9913735389709473  Took  0.0  secs.\n",
      "epoch:  80  total loss L:  3.9786887168884277  Took  0.0  secs.\n",
      "epoch:  81  total loss L:  3.965749502182007  Took  0.0  secs.\n",
      "epoch:  82  total loss L:  3.9528822898864746  Took  0.0  secs.\n",
      "epoch:  83  total loss L:  3.9403305053710938  Took  0.0  secs.\n",
      "epoch:  84  total loss L:  3.9280667304992676  Took  0.0  secs.\n",
      "epoch:  85  total loss L:  3.9158363342285156  Took  0.0  secs.\n",
      "epoch:  86  total loss L:  3.903419256210327  Took  0.0  secs.\n",
      "epoch:  87  total loss L:  3.8908913135528564  Took  0.0  secs.\n",
      "epoch:  88  total loss L:  3.8784937858581543  Took  0.0  secs.\n",
      "epoch:  89  total loss L:  3.8663268089294434  Took  0.0  secs.\n",
      "epoch:  90  total loss L:  3.854295253753662  Took  0.0  secs.\n",
      "epoch:  91  total loss L:  3.842233657836914  Took  0.0  secs.\n",
      "epoch:  92  total loss L:  3.830070972442627  Took  0.0  secs.\n",
      "epoch:  93  total loss L:  3.8179192543029785  Took  0.0  secs.\n",
      "epoch:  94  total loss L:  3.8059158325195312  Took  0.0  secs.\n",
      "epoch:  95  total loss L:  3.7940473556518555  Took  0.0  secs.\n",
      "epoch:  96  total loss L:  3.782214403152466  Took  0.0  secs.\n",
      "epoch:  97  total loss L:  3.7703440189361572  Took  0.0  secs.\n",
      "epoch:  98  total loss L:  3.7584621906280518  Took  0.0  secs.\n",
      "epoch:  99  total loss L:  3.746669292449951  Took  0.0  secs.\n",
      "epoch:  100  total loss L:  3.7349917888641357  Took  0.0  secs.\n",
      "#\n",
      "dcmf.fit - end\n",
      "dataset_name:  dn3\n",
      "#\n",
      "X0.shape:  (200, 500)\n",
      "X1.shape:  (200, 300)\n",
      "X2.shape:  (200, 400)\n",
      "X3.shape:  (700, 500)\n",
      "X4.shape:  (600, 300)\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "WARNING: The following parameters are unused since no validation data provided.\n",
      "val_metric:  auc\n",
      "at_k:  10\n",
      "is_val_transpose:  True\n",
      "#\n",
      "dCMF:\n",
      "---\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  0.0001\n",
      "weight_decay:  0.01\n",
      "convg_thres:  -0.1\n",
      "max_epochs:  100\n",
      "isPretrain:  False\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  1\n",
      "k:  100\n",
      "kf:  0.0005\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  True\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  0\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "## fold_num:  1  ##\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "WARNING: The following parameters are unused since no validation data provided.\n",
      "val_metric:  auc\n",
      "at_k:  10\n",
      "is_val_transpose:  True\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  0.0001\n",
      "weight_decay:  0.01\n",
      "convg_thres:  -0.1\n",
      "max_epochs:  100\n",
      "isPretrain:  False\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  1\n",
      "k:  100\n",
      "kf:  0.0005\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  True\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  0\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "dcmf - model construction - start\n",
      "__input_transformation - start\n",
      "#\n",
      "concatenated-matrix construction...\n",
      "e_id:  e0\n",
      "X_id_list:  ['X0', 'X1', 'X2']\n",
      "X_id:  X0\n",
      "X[X_id].shape:  (200, 500)\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (200, 300)\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (200, 400)\n",
      "C_dict[e].shape:  torch.Size([200, 1200])\n",
      "---\n",
      "e_id:  e1\n",
      "X_id_list:  ['X0', 'X3']\n",
      "X_id:  X0\n",
      "X[X_id].shape:  (200, 500)\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (700, 500)\n",
      "C_dict[e].shape:  torch.Size([500, 900])\n",
      "---\n",
      "e_id:  e2\n",
      "X_id_list:  ['X1', 'X4']\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (200, 300)\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (600, 300)\n",
      "C_dict[e].shape:  torch.Size([300, 800])\n",
      "---\n",
      "e_id:  e3\n",
      "X_id_list:  ['X2']\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (200, 400)\n",
      "C_dict[e].shape:  torch.Size([400, 200])\n",
      "---\n",
      "e_id:  e4\n",
      "X_id_list:  ['X3']\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (700, 500)\n",
      "C_dict[e].shape:  torch.Size([700, 500])\n",
      "---\n",
      "e_id:  e5\n",
      "X_id_list:  ['X4']\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (600, 300)\n",
      "C_dict[e].shape:  torch.Size([600, 300])\n",
      "---\n",
      "#\n",
      "concatenated-matrix chunking...\n",
      "#\n",
      "e_id:  e0 , min_num_datapoints:  200 , num_chunks:  1\n",
      "e_id:  e3 , min_features:  200 , k:  100\n",
      "#\n",
      "e_id:  e0  C_dict[e_id].shape:  torch.Size([200, 1200])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([200, 1200])\n",
      "---\n",
      "e_id:  e1  C_dict[e_id].shape:  torch.Size([500, 900])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([500, 900])\n",
      "---\n",
      "e_id:  e2  C_dict[e_id].shape:  torch.Size([300, 800])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([300, 800])\n",
      "---\n",
      "e_id:  e3  C_dict[e_id].shape:  torch.Size([400, 200])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([400, 200])\n",
      "---\n",
      "e_id:  e4  C_dict[e_id].shape:  torch.Size([700, 500])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([700, 500])\n",
      "---\n",
      "e_id:  e5  C_dict[e_id].shape:  torch.Size([600, 300])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([600, 300])\n",
      "---\n",
      "#\n",
      "creating pytorch variables of input matrices...\n",
      "#\n",
      "__input_transformation - end\n",
      "__network_construction - start\n",
      "__network_construction - end\n",
      "dcmf - model construction - end\n",
      "#\n",
      "#\n",
      "dcmf.fit - start\n",
      "epoch:  1  total loss L:  8.722249984741211  Took  0.0  secs.\n",
      "epoch:  2  total loss L:  8.588600158691406  Took  0.0  secs.\n",
      "epoch:  3  total loss L:  8.005353927612305  Took  0.0  secs.\n",
      "epoch:  4  total loss L:  7.121304512023926  Took  0.0  secs.\n",
      "epoch:  5  total loss L:  7.063302040100098  Took  0.0  secs.\n",
      "epoch:  6  total loss L:  7.111464977264404  Took  0.0  secs.\n",
      "epoch:  7  total loss L:  6.774438858032227  Took  0.0  secs.\n",
      "epoch:  8  total loss L:  6.3821797370910645  Took  0.0  secs.\n",
      "epoch:  9  total loss L:  6.234618186950684  Took  0.0  secs.\n",
      "epoch:  10  total loss L:  6.249112606048584  Took  0.0  secs.\n",
      "epoch:  11  total loss L:  6.1830291748046875  Took  0.0  secs.\n",
      "epoch:  12  total loss L:  5.979086875915527  Took  0.0  secs.\n",
      "epoch:  13  total loss L:  5.768280982971191  Took  0.0  secs.\n",
      "epoch:  14  total loss L:  5.666277885437012  Took  0.0  secs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  15  total loss L:  5.652462482452393  Took  0.0  secs.\n",
      "epoch:  16  total loss L:  5.6245036125183105  Took  0.0  secs.\n",
      "epoch:  17  total loss L:  5.528800964355469  Took  0.0  secs.\n",
      "epoch:  18  total loss L:  5.404654026031494  Took  0.0  secs.\n",
      "epoch:  19  total loss L:  5.316600799560547  Took  0.0  secs.\n",
      "epoch:  20  total loss L:  5.2819719314575195  Took  0.0  secs.\n",
      "epoch:  21  total loss L:  5.266491889953613  Took  0.0  secs.\n",
      "epoch:  22  total loss L:  5.229125499725342  Took  0.0  secs.\n",
      "epoch:  23  total loss L:  5.162822723388672  Took  0.0  secs.\n",
      "epoch:  24  total loss L:  5.092687129974365  Took  0.0  secs.\n",
      "epoch:  25  total loss L:  5.043733596801758  Took  0.0  secs.\n",
      "epoch:  26  total loss L:  5.01816463470459  Took  0.0  secs.\n",
      "epoch:  27  total loss L:  4.9975128173828125  Took  0.0  secs.\n",
      "epoch:  28  total loss L:  4.962766647338867  Took  0.0  secs.\n",
      "epoch:  29  total loss L:  4.91307258605957  Took  0.0  secs.\n",
      "epoch:  30  total loss L:  4.863916397094727  Took  0.0  secs.\n",
      "epoch:  31  total loss L:  4.829544544219971  Took  0.0  secs.\n",
      "epoch:  32  total loss L:  4.808789253234863  Took  0.0  secs.\n",
      "epoch:  33  total loss L:  4.787867546081543  Took  0.0  secs.\n",
      "epoch:  34  total loss L:  4.756591796875  Took  0.0  secs.\n",
      "epoch:  35  total loss L:  4.718573093414307  Took  0.0  secs.\n",
      "epoch:  36  total loss L:  4.68483829498291  Took  0.0  secs.\n",
      "epoch:  37  total loss L:  4.660989761352539  Took  0.0  secs.\n",
      "epoch:  38  total loss L:  4.642592430114746  Took  0.0  secs.\n",
      "epoch:  39  total loss L:  4.621773719787598  Took  0.0  secs.\n",
      "epoch:  40  total loss L:  4.595603942871094  Took  0.0  secs.\n",
      "epoch:  41  total loss L:  4.567383766174316  Took  0.0  secs.\n",
      "epoch:  42  total loss L:  4.541999816894531  Took  0.0  secs.\n",
      "epoch:  43  total loss L:  4.52129602432251  Took  0.0  secs.\n",
      "epoch:  44  total loss L:  4.503015518188477  Took  0.0  secs.\n",
      "epoch:  45  total loss L:  4.483395576477051  Took  0.0  secs.\n",
      "epoch:  46  total loss L:  4.460939407348633  Took  0.0  secs.\n",
      "epoch:  47  total loss L:  4.437699317932129  Took  0.0  secs.\n",
      "epoch:  48  total loss L:  4.416708469390869  Took  0.0  secs.\n",
      "epoch:  49  total loss L:  4.398592948913574  Took  0.0  secs.\n",
      "epoch:  50  total loss L:  4.381327152252197  Took  0.0  secs.\n",
      "epoch:  51  total loss L:  4.362912178039551  Took  0.0  secs.\n",
      "epoch:  52  total loss L:  4.343308448791504  Took  0.0  secs.\n",
      "epoch:  53  total loss L:  4.323835849761963  Took  0.0  secs.\n",
      "epoch:  54  total loss L:  4.30560302734375  Took  0.0  secs.\n",
      "epoch:  55  total loss L:  4.288674354553223  Took  0.0  secs.\n",
      "epoch:  56  total loss L:  4.272181034088135  Took  0.0  secs.\n",
      "epoch:  57  total loss L:  4.2551164627075195  Took  0.0  secs.\n",
      "epoch:  58  total loss L:  4.237354278564453  Took  0.0  secs.\n",
      "epoch:  59  total loss L:  4.219768524169922  Took  0.0  secs.\n",
      "epoch:  60  total loss L:  4.203137397766113  Took  0.0  secs.\n",
      "epoch:  61  total loss L:  4.187236785888672  Took  0.0  secs.\n",
      "epoch:  62  total loss L:  4.171298980712891  Took  0.0  secs.\n",
      "epoch:  63  total loss L:  4.154962539672852  Took  0.0  secs.\n",
      "epoch:  64  total loss L:  4.138477325439453  Took  0.0  secs.\n",
      "epoch:  65  total loss L:  4.12228536605835  Took  0.0  secs.\n",
      "epoch:  66  total loss L:  4.106622695922852  Took  0.0  secs.\n",
      "epoch:  67  total loss L:  4.091325283050537  Took  0.0  secs.\n",
      "epoch:  68  total loss L:  4.0759663581848145  Took  0.0  secs.\n",
      "epoch:  69  total loss L:  4.06036376953125  Took  0.0  secs.\n",
      "epoch:  70  total loss L:  4.044798851013184  Took  0.0  secs.\n",
      "epoch:  71  total loss L:  4.029578685760498  Took  0.0  secs.\n",
      "epoch:  72  total loss L:  4.0146589279174805  Took  0.0  secs.\n",
      "epoch:  73  total loss L:  3.999817371368408  Took  0.0  secs.\n",
      "epoch:  74  total loss L:  3.984917163848877  Took  0.0  secs.\n",
      "epoch:  75  total loss L:  3.9699764251708984  Took  0.0  secs.\n",
      "epoch:  76  total loss L:  3.955159902572632  Took  0.0  secs.\n",
      "epoch:  77  total loss L:  3.9405996799468994  Took  0.0  secs.\n",
      "epoch:  78  total loss L:  3.9261982440948486  Took  0.0  secs.\n",
      "epoch:  79  total loss L:  3.9117836952209473  Took  0.0  secs.\n",
      "epoch:  80  total loss L:  3.8973472118377686  Took  0.0  secs.\n",
      "epoch:  81  total loss L:  3.882992744445801  Took  0.0  secs.\n",
      "epoch:  82  total loss L:  3.868791341781616  Took  0.0  secs.\n",
      "epoch:  83  total loss L:  3.854731559753418  Took  0.0  secs.\n",
      "epoch:  84  total loss L:  3.840731143951416  Took  0.0  secs.\n",
      "epoch:  85  total loss L:  3.8267273902893066  Took  0.0  secs.\n",
      "epoch:  86  total loss L:  3.8127715587615967  Took  0.0  secs.\n",
      "epoch:  87  total loss L:  3.798941135406494  Took  0.0  secs.\n",
      "epoch:  88  total loss L:  3.7852253913879395  Took  0.0  secs.\n",
      "epoch:  89  total loss L:  3.7715742588043213  Took  0.0  secs.\n",
      "epoch:  90  total loss L:  3.7579588890075684  Took  0.0  secs.\n",
      "epoch:  91  total loss L:  3.74438738822937  Took  0.0  secs.\n",
      "epoch:  92  total loss L:  3.730910062789917  Took  0.0  secs.\n",
      "epoch:  93  total loss L:  3.7175395488739014  Took  0.0  secs.\n",
      "epoch:  94  total loss L:  3.704235792160034  Took  0.0  secs.\n",
      "epoch:  95  total loss L:  3.6909780502319336  Took  0.0  secs.\n",
      "epoch:  96  total loss L:  3.6777799129486084  Took  0.0  secs.\n",
      "epoch:  97  total loss L:  3.664663314819336  Took  0.0  secs.\n",
      "epoch:  98  total loss L:  3.6516404151916504  Took  0.0  secs.\n",
      "epoch:  99  total loss L:  3.638688087463379  Took  0.0  secs.\n",
      "epoch:  100  total loss L:  3.6257882118225098  Took  0.0  secs.\n",
      "#\n",
      "dcmf.fit - end\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in ['dt1', 'ds1', 'ds2', 'ds3', 'dn1', 'dn2', 'dn3']:\n",
    "    print(\"dataset_name: \",dataset_name)\n",
    "    print(\"#\")\n",
    "    #\n",
    "    fname = in_dir + dataset_name + \"/0.csv\"\n",
    "    X0 = pd.read_csv(fname,header=None).values\n",
    "    #\n",
    "    fname = in_dir + dataset_name + \"/1.csv\"\n",
    "    X1 = pd.read_csv(fname,header=None).values\n",
    "    #\n",
    "    fname = in_dir + dataset_name + \"/2.csv\"\n",
    "    X2 = pd.read_csv(fname,header=None).values\n",
    "    #\n",
    "    fname = in_dir + dataset_name + \"/3.csv\"\n",
    "    X3 = pd.read_csv(fname,header=None).values\n",
    "    #\n",
    "    fname = in_dir + dataset_name + \"/4.csv\"\n",
    "    X4 = pd.read_csv(fname,header=None).values\n",
    "    #\n",
    "    out_dir = out_dir_base + dataset_name + \"/\"\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    #\n",
    "    print(\"X0.shape: \",X0.shape)\n",
    "    print(\"X1.shape: \",X1.shape)\n",
    "    print(\"X2.shape: \",X2.shape)\n",
    "    print(\"X3.shape: \",X3.shape)\n",
    "    print(\"X4.shape: \",X4.shape)\n",
    "    #\n",
    "    G = {\"e0\":[\"X0\",\"X1\",\"X2\"],\\\n",
    "         \"e1\":[\"X0\",\"X3\"],\\\n",
    "         \"e2\":[\"X1\",\"X4\"],\\\n",
    "         \"e3\":[\"X2\"],\\\n",
    "         \"e4\":[\"X3\"],\\\n",
    "         \"e5\":[\"X4\"]}\n",
    "    #\n",
    "    X_data = {\n",
    "        \"X0\":X0,\n",
    "        \"X1\":X1,\n",
    "        \"X2\":X2,\n",
    "        \"X3\":X3,\n",
    "        \"X4\":X4}\n",
    "    #\n",
    "    X_meta = {\"X0\":[\"e0\",\"e1\"],\\\n",
    "         \"X1\":[\"e0\",\"e2\"],\\\n",
    "         \"X2\":[\"e0\",\"e3\"],\\\n",
    "         \"X3\":[\"e4\",\"e1\"],\\\n",
    "         \"X4\":[\"e5\",\"e2\"]}\n",
    "    #\n",
    "    X_val = {}\n",
    "    #\n",
    "    kf = 0.0005\n",
    "    k = 100\n",
    "    e_actf = \"tanh\"\n",
    "    d_actf = \"tanh\"\n",
    "    is_linear_last_enc_layer = False\n",
    "    is_linear_last_dec_layer = False\n",
    "    num_chunks = 10\n",
    "    #\n",
    "    learning_rate = 0.0001\n",
    "    weight_decay = 0.01\n",
    "    max_epochs = 100\n",
    "    convg_thres = -0.1\n",
    "    #\n",
    "    is_pretrain=False\n",
    "    pretrain_thres= 0.1\n",
    "    max_pretrain_epochs = 2\n",
    "    #\n",
    "    val_metric = \"auc\"\n",
    "    is_val_transpose = True\n",
    "    at_k = 10\n",
    "    #\n",
    "    is_gpu = True\n",
    "    gpu_ids = \"1\"\n",
    "    #\n",
    "    num_folds = 1\n",
    "    #\n",
    "    dcmf_model = dcmf(G, X_data, X_meta,\\\n",
    "                num_chunks=num_chunks,k=k, kf=kf, e_actf=e_actf, d_actf=d_actf,\\\n",
    "                learning_rate=learning_rate, weight_decay=weight_decay, convg_thres=convg_thres, max_epochs=max_epochs,\\\n",
    "                is_gpu=is_gpu,gpu_ids=gpu_ids,is_pretrain=is_pretrain, pretrain_thres=pretrain_thres,\\\n",
    "                max_pretrain_epochs=max_pretrain_epochs,X_val=X_val,val_metric=val_metric,\\\n",
    "                is_val_transpose=is_val_transpose, at_k=at_k,\\\n",
    "                is_linear_last_enc_layer=is_linear_last_enc_layer,is_linear_last_dec_layer=is_linear_last_dec_layer,num_val_sets=num_folds)\n",
    "    #\n",
    "    dcmf_model.fit()\n",
    "    #\n",
    "    dict_out = dcmf_model.out_dict_X_prime[\"1\"]\n",
    "    dict_out_np = {}\n",
    "    for cur_mat_id in dict_out:\n",
    "        cur_mat_tensor = dict_out[cur_mat_id]\n",
    "        cur_mat_np = cur_mat_tensor.cpu().detach().numpy()\n",
    "        dict_out_np[cur_mat_id] = cur_mat_np\n",
    "    #\n",
    "    fname_out = out_dir + \"dict_out_dcmf.pkl\"\n",
    "    pkl.dump(dict_out_np,open(fname_out,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
