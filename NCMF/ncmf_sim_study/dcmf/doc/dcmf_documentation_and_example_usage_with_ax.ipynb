{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dCMF\n",
    "Example of running the \"dcmf\" module with the best parameters found using the ax framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ax\n",
    "from ax import RangeParameter, ChoiceParameter, FixedParameter\n",
    "from ax import ParameterType, SearchSpace\n",
    "from ax.service.managed_loop import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import time\n",
    "import itertools\n",
    "import os\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dcmf import dcmf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the sample dataset\n",
    "\n",
    "This directory contains a sample synthetic dataset generated for the augmented setting of Fig 1(c) in the [paper](https://arxiv.org/abs/1811.11427).\n",
    "You can download the sample data from [here](https://drive.google.com/open?id=1EFF_kuOIg2aYyOGZY_peX3NziqCSxxP1) and unzip it to the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/sample_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from data_dir:  ../data/sample_data/\n"
     ]
    }
   ],
   "source": [
    "#Loads the dataset into a dict\n",
    "#Note: This dataset contains 5-folds for the matrix X_12 (matrix R below)\n",
    "num_folds = 1\n",
    "#\n",
    "pp = pprint.PrettyPrinter()\n",
    "print(\"Loading data from data_dir: \",data_dir)\n",
    "U1 = pkl.load(open(data_dir+\"X_13.pkl\",'rb'))\n",
    "U2 = pkl.load(open(data_dir+\"X_14.pkl\",'rb'))\n",
    "V1 = pkl.load(open(data_dir+\"X_26.pkl\",'rb'))\n",
    "W1 = pkl.load(open(data_dir+\"X_53.pkl\",'rb'))\n",
    "R_temp_dict = {}\n",
    "for fold_num in np.arange(1,num_folds+1):\n",
    "    Rtrain = pkl.load(open(data_dir+'/X_12_train_fold_'+str(fold_num)+'.pkl','rb'))\n",
    "    Rtrain = Rtrain\n",
    "    Rtrain_idx = pkl.load(open(data_dir+'/X_12_train_idx_'+str(fold_num)+'.pkl','rb')) \n",
    "    Rtest = pkl.load(open(data_dir+'/X_12_test_fold_'+str(fold_num)+'.pkl','rb'))\n",
    "    Rtest_idx = pkl.load(open(data_dir+'/X_12_test_idx_'+str(fold_num)+'.pkl','rb'))\n",
    "    Rdoublets = pkl.load(open(data_dir+'/R_doublets_'+str(fold_num)+'.pkl','rb'))\n",
    "    R_temp_dict[fold_num] = {\"Rtrain\":Rtrain,\"Rtrain_idx\":Rtrain_idx,\"Rtest\":Rtest,\"Rtest_idx\":Rtest_idx,\"Rdoublets\":Rdoublets}\n",
    "#\n",
    "data_dict = {\"U1\":U1,\"U2\":U2,\"V1\":V1,\"W1\":W1,\"R\":R_temp_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U1.shape:  (1000, 20)\n",
      "U2.shape:  (1000, 150)\n",
      "V1.shape:  (2000, 250)\n",
      "W1.shape:  (300, 20)\n",
      "R.shape:  (1000, 2000)\n"
     ]
    }
   ],
   "source": [
    "print(\"U1.shape: \",U1.shape)\n",
    "print(\"U2.shape: \",U2.shape)\n",
    "print(\"V1.shape: \",V1.shape)\n",
    "print(\"W1.shape: \",W1.shape)\n",
    "print(\"R.shape: \",data_dict['R'][1]['Rtrain'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the required data structures\n",
    "\n",
    "Here we construct the data structures required as input to the dcmf API\n",
    "\n",
    "#### *entity matrix relationship graph *\n",
    "\n",
    "- **G**: dict, keys are entity IDs and values are lists of associated matrix IDs\n",
    "\n",
    "#### * training data*\n",
    "- **X_data**: dict, keys are matrix IDs and values are (1) np.array, or (2) dict, (if this matrix is in validation set **X_val**) with validation set IDs as keys & values as np.array\n",
    "- **X_meta**: dict, keys are matrix IDs and values are lists of the 2 associated entity IDs\n",
    "\n",
    "#### *validation data*\n",
    "- **X_val**: dict, keys are IDs of the matrices that are part of validation set and values are dict with validation set IDs as keys and values are (1) scipy.sparse matrix, or (2) list of triplets corresponding to the validation entries (if you would like to perform classification and measure AUC)  \n",
    "**Note**: To perform K folds cross validation, use K validation sets for the corresponsing matrix/matrices. In the example below, we used a single validation set with ID \"1\" for each of the matrices with IDs \"X1\" and \"X2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = {\n",
    "    \"e1\":[\"X1\",\"X2\",\"X3\"],\\\n",
    "    \"e2\":[\"X1\",\"X4\"],\\\n",
    "    \"e3\":[\"X2\",\"X5\"],\\\n",
    "    \"e4\":[\"X3\"],\\\n",
    "    \"e5\":[\"X5\"],\\\n",
    "    \"e6\":[\"X4\"]}\n",
    "    #\"e6\":[\"X4\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = {\n",
    "    \"X1\":{\"1\":data_dict['R'][1][\"Rtrain\"]},\\\n",
    "    \"X2\":{\"1\":U1},\\\n",
    "    \"X3\":U2,\\\n",
    "    \"X4\":V1,\\\n",
    "    \"X5\":W1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_meta = {\n",
    "    \"X1\":[\"e1\",\"e2\"],\\\n",
    "    \"X2\":[\"e1\",\"e3\"],\\\n",
    "    \"X3\":[\"e1\",\"e4\"],\\\n",
    "    \"X4\":[\"e2\",\"e6\"],\\\n",
    "    \"X5\":[\"e5\",\"e3\"]}\n",
    "    #\"X5\":[\"e5\",\"e3\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rtest_triplets1 = [[1,1,1],[2,2,0]]\n",
    "Rtest_triplets2 = [[1,1,1],[3,3,0],[1,2,0],[0,1,0],[0,2,0],[0,3,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = {\n",
    "    \"X1\":{\"1\":Rtest_triplets1},\n",
    "    \"X2\":{\"1\":Rtest_triplets2}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *dCMF network construction - hyperparameters*\n",
    "\n",
    "- **kf**: float, in the range (0,1) \n",
    "- **k**: int, entity representation or encoding size. Refer Appendix A in the [paper](https://arxiv.org/abs/1811.11427) for info about how k and kf are used in the dCMF network construction. \n",
    "- **e_actf**: str, autoencoder's encoding activation function.\n",
    "- **d_actf**: str, autoencoder's decoding activation function. Supported functions are \"tanh\",\"sigma\",\"relu\",\"lrelu\"\n",
    "- **is_linear_last_enc_layer**: bool, True to set linear activation for the bottleneck/encoding generation layer \n",
    "- **is_linear_last_dec_layer**: bool, True to set linear activation for the output/decoding generation layer \n",
    "- **num_chunks**: int, number of training batches to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = 0.5\n",
    "k = 100\n",
    "e_actf = \"tanh\"\n",
    "d_actf = \"tanh\"\n",
    "is_linear_last_enc_layer = False\n",
    "is_linear_last_dec_layer = False\n",
    "num_chunks = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Optimization/training - hyperparamteres*\n",
    "\n",
    "- **learning_rate**: float, Adam optimizer's learning rate\n",
    "- **weight_decay**: float, Adam optimizers's weight decay (L2 penalty)\n",
    "- **max_epochs**: int, maximum number of training epochs at which the training stops \n",
    "- **convg_thres**: float, convergence threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.05\n",
    "max_epochs = 5\n",
    "convg_thres = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Hyperparamteres related to pre-training*\n",
    "\n",
    "- **is_pretrain**: bool, True for pretraining \n",
    "- **pretrain_thres**: bool, pre-training convergence thresholsd\n",
    "- **max_pretrain_epochs**: int, maximum number of pre-training epochs at which the training stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_pretrain=True\n",
    "pretrain_thres= 0.1\n",
    "max_pretrain_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Parameters related to validation*\n",
    "\n",
    "- **val_metric**: str, Validation performance metric. Supported metrics: [\"rmse\",\"r@k\",\"p@k\",\"auc\"]. Where,  \n",
    "     *rmse* - Root [mean square error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)  \n",
    "     *r@k* - Recall@k. Refer section 5.2's sub-section \"Evaluation metric\" in the [paper](https://arxiv.org/abs/1811.11427)      \n",
    "     *p@k* - Probability@k. Refer section 5.3's sub-section \"Evaluation metric\" in the [paper](https://arxiv.org/abs/1811.11427)      \n",
    "     *auc* - [Area under the curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)\n",
    "    \n",
    "- **is_val_transpose**: bool, True if the reconstructed matrix has to be transposed before computing the validation performance\n",
    "- **at_k**: int, the value of k if the **val_metric** is either \"r@k\" or \"p@k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_metric = \"auc\"\n",
    "is_val_transpose = True\n",
    "at_k = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *GPU - parameters *\n",
    "\n",
    "- **is_gpu**: bool, True if pytorch tensors storage and operations has to be done in GPU\n",
    "- **gpu_ids**: str, Comma separated string of CUDA GPU ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_gpu = False\n",
    "gpu_ids = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter selection using the ax framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Installation instruction can be found at: [https://ax.dev/](https://ax.dev/)\n",
    "- The example below is based on the following API:\n",
    "[https://ax.dev/tutorials/gpei_hartmann_loop.html](https://ax.dev/tutorials/gpei_hartmann_loop.html)\n",
    "- And here is a high level intro to the library: \n",
    "[https://www.youtube.com/watch?v=2c8YX0E8Qhw](https://www.youtube.com/watch?v=2c8YX0E8Qhw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wrapper method for DCMF to use with the ax framework\n",
    "# Here we perform the hyper parameter optimization based on the training loss\n",
    "# i.e. finding the optimum hyperparams that results in minimum loss\n",
    "\n",
    "def run_dcmf(parameterization):\n",
    "    #hyper-parameters that are selected using ax\n",
    "    learning_rate = parameterization[\"learning_rate\"]\n",
    "    weight_decay = parameterization[\"weight_decay\"]\n",
    "    #\n",
    "    dcmf_model = dcmf(G, X_data, X_meta,\\\n",
    "                num_chunks=num_chunks,k=k, kf=kf, e_actf=e_actf, d_actf=d_actf,\\\n",
    "                learning_rate=learning_rate, weight_decay=weight_decay, convg_thres=convg_thres, max_epochs=max_epochs,\\\n",
    "                is_gpu=is_gpu,gpu_ids=gpu_ids,is_pretrain=is_pretrain, pretrain_thres=pretrain_thres,\\\n",
    "                max_pretrain_epochs=max_pretrain_epochs,X_val=X_val,val_metric=val_metric,\\\n",
    "                is_val_transpose=is_val_transpose, at_k=at_k,\\\n",
    "                is_linear_last_enc_layer=is_linear_last_enc_layer,is_linear_last_dec_layer=is_linear_last_dec_layer,num_val_sets=num_folds)\n",
    "    #\n",
    "    dcmf_model.fit()\n",
    "    print(\"#\")\n",
    "    print(\"dcmf_model.out_dict_info: \")\n",
    "    pp.pprint(dcmf_model.out_dict_info)\n",
    "    print(\"#\")\n",
    "    #\n",
    "    out_dict = {}\n",
    "    out_dict[\"loss\"] = (dcmf_model.out_dict_info[\"loss_all_folds_avg_sum\"], 0.0)\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 07-16 22:02:56] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+GPEI', steps=[Sobol for 5 trials, GPEI for subsequent trials]). Iterations after 5 will take longer to generate due to  model-fitting.\n",
      "[INFO 07-16 22:02:56] ax.service.managed_loop: Started full optimization with 2 steps.\n",
      "[INFO 07-16 22:02:56] ax.service.managed_loop: Running optimization trial 1...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "#\n",
      "dCMF:\n",
      "---\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  4.171199586614966e-06\n",
      "weight_decay:  0.009874873130339199\n",
      "convg_thres:  0.1\n",
      "max_epochs:  5\n",
      "isPretrain:  True\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  2\n",
      "k:  100\n",
      "kf:  0.5\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  False\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  2\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "## fold_num:  1  ##\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  4.171199586614966e-06\n",
      "weight_decay:  0.009874873130339199\n",
      "convg_thres:  0.1\n",
      "max_epochs:  5\n",
      "isPretrain:  True\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  2\n",
      "k:  100\n",
      "kf:  0.5\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  False\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  2\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "dcmf - model construction - start\n",
      "__input_transformation - start\n",
      "#\n",
      "concatenated-matrix construction...\n",
      "e_id:  e1\n",
      "X_id_list:  ['X1', 'X2', 'X3']\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (1000, 2000)\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (1000, 20)\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (1000, 150)\n",
      "C_dict[e].shape:  torch.Size([1000, 2170])\n",
      "---\n",
      "e_id:  e2\n",
      "X_id_list:  ['X1', 'X4']\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (1000, 2000)\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (2000, 250)\n",
      "C_dict[e].shape:  torch.Size([2000, 1250])\n",
      "---\n",
      "e_id:  e3\n",
      "X_id_list:  ['X2', 'X5']\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (1000, 20)\n",
      "X_id:  X5\n",
      "X[X_id].shape:  (300, 20)\n",
      "C_dict[e].shape:  torch.Size([20, 1300])\n",
      "---\n",
      "e_id:  e4\n",
      "X_id_list:  ['X3']\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (1000, 150)\n",
      "C_dict[e].shape:  torch.Size([150, 1000])\n",
      "---\n",
      "e_id:  e5\n",
      "X_id_list:  ['X5']\n",
      "X_id:  X5\n",
      "X[X_id].shape:  (300, 20)\n",
      "C_dict[e].shape:  torch.Size([300, 20])\n",
      "---\n",
      "e_id:  e6\n",
      "X_id_list:  ['X4']\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (2000, 250)\n",
      "C_dict[e].shape:  torch.Size([250, 2000])\n",
      "---\n",
      "#\n",
      "concatenated-matrix chunking...\n",
      "WARNING: Entity with ID: e5 has minimum feature size 20 in the setting. The encoding length k 100 is larger than minimim entity feature size.\n",
      "#\n",
      "e_id:  e3 , min_num_datapoints:  20 , num_chunks:  2\n",
      "e_id:  e5 , min_features:  20 , k:  100\n",
      "#\n",
      "e_id:  e1  C_dict[e_id].shape:  torch.Size([1000, 2170])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([500, 2170])\n",
      "---\n",
      "e_id:  e2  C_dict[e_id].shape:  torch.Size([2000, 1250])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([1000, 1250])\n",
      "---\n",
      "e_id:  e3  C_dict[e_id].shape:  torch.Size([20, 1300])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([10, 1300])\n",
      "---\n",
      "e_id:  e4  C_dict[e_id].shape:  torch.Size([150, 1000])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([75, 1000])\n",
      "---\n",
      "e_id:  e5  C_dict[e_id].shape:  torch.Size([300, 20])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([150, 20])\n",
      "---\n",
      "e_id:  e6  C_dict[e_id].shape:  torch.Size([250, 2000])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([125, 2000])\n",
      "---\n",
      "#\n",
      "creating pytorch variables of input matrices...\n",
      "#\n",
      "__input_transformation - end\n",
      "__network_construction - start\n",
      "__network_construction - end\n",
      "dcmf - model construction - end\n",
      "#\n",
      "__pretrain - start\n",
      "pretrain epoch:  1  total loss L:  5.896239757537842  Took  0.5  secs.\n",
      "pretrain epoch:  2  total loss L:  5.8941051959991455  Took  0.6  secs.\n",
      "**pretrain converged**\n",
      "__pretrain - end\n",
      "#\n",
      "dcmf.fit - start\n",
      "epoch:  1  total loss L:  9.507555961608887  Took  0.5  secs.\n",
      "epoch:  2  total loss L:  9.10748291015625  Took  0.8  secs.\n",
      "epoch:  3  total loss L:  8.807185649871826  Took  0.6  secs.\n",
      "epoch:  4  total loss L:  8.565542697906494  Took  0.8  secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 07-16 22:03:01] ax.service.managed_loop: Running optimization trial 2...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  5  total loss L:  8.336682319641113  Took  0.5  secs.\n",
      "Computing AUC.\n",
      "Rpred.shape:  (2000, 1000)\n",
      "Rtest_triplets.shape:  (2, 3)\n",
      "Computing AUC.\n",
      "Rpred.shape:  (20, 1000)\n",
      "Rtest_triplets.shape:  (6, 3)\n",
      "#\n",
      "dcmf.fit - end\n",
      "#\n",
      "dcmf_model.out_dict_info: \n",
      "{'E': 6,\n",
      " 'M': 5,\n",
      " 'loss_all_folds': {'1': [0.6903803944587708,\n",
      "                          0.7196261584758759,\n",
      "                          1.0797675251960754,\n",
      "                          0.9683963656425476,\n",
      "                          1.4825401306152344,\n",
      "                          0.9440502226352692,\n",
      "                          0.5293242186307907,\n",
      "                          0.6657675802707672,\n",
      "                          0.7025379836559296,\n",
      "                          0.5187419801950455,\n",
      "                          0.03554987534880638]},\n",
      " 'loss_all_folds_avg_sum': 8.336682435125113,\n",
      " 'loss_all_folds_avg_tuple': [0.6903803944587708,\n",
      "                              0.7196261584758759,\n",
      "                              1.0797675251960754,\n",
      "                              0.9683963656425476,\n",
      "                              1.4825401306152344,\n",
      "                              0.9440502226352692,\n",
      "                              0.5293242186307907,\n",
      "                              0.6657675802707672,\n",
      "                              0.7025379836559296,\n",
      "                              0.5187419801950455,\n",
      "                              0.03554987534880638],\n",
      " 'num_val_sets': 1,\n",
      " 'params': {'convg_thres': 0.1,\n",
      "            'd_actf': 'tanh',\n",
      "            'e_actf': 'tanh',\n",
      "            'is_linear_last_dec_layer': False,\n",
      "            'is_linear_last_enc_layer': False,\n",
      "            'is_pretrain': True,\n",
      "            'k': 100,\n",
      "            'kf': 0.5,\n",
      "            'learning_rate': 4.171199586614966e-06,\n",
      "            'max_epochs': 5,\n",
      "            'max_pretrain_epochs': 2,\n",
      "            'num_chunks': 2,\n",
      "            'pretrain_thres': 0.1,\n",
      "            'weight_decay': 0.009874873130339199},\n",
      " 'val_metric': 'auc',\n",
      " 'val_perf_all_folds': {'1': {'X1': 0.0, 'X2': 0.0}},\n",
      " 'val_perf_all_folds_avg': {'X1': 0.0, 'X2': 0.0},\n",
      " 'val_perf_all_folds_total': {'1': 0.0},\n",
      " 'val_perf_all_folds_total_avg': 0.0}\n",
      "#\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "#\n",
      "dCMF:\n",
      "---\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  4.29610376060009e-07\n",
      "weight_decay:  0.009241838757959194\n",
      "convg_thres:  0.1\n",
      "max_epochs:  5\n",
      "isPretrain:  True\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  2\n",
      "k:  100\n",
      "kf:  0.5\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  False\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  2\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "## fold_num:  1  ##\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  4.29610376060009e-07\n",
      "weight_decay:  0.009241838757959194\n",
      "convg_thres:  0.1\n",
      "max_epochs:  5\n",
      "isPretrain:  True\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  2\n",
      "k:  100\n",
      "kf:  0.5\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  False\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  2\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "dcmf - model construction - start\n",
      "__input_transformation - start\n",
      "#\n",
      "concatenated-matrix construction...\n",
      "e_id:  e1\n",
      "X_id_list:  ['X1', 'X2', 'X3']\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (1000, 2000)\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (1000, 20)\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (1000, 150)\n",
      "C_dict[e].shape:  torch.Size([1000, 2170])\n",
      "---\n",
      "e_id:  e2\n",
      "X_id_list:  ['X1', 'X4']\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (1000, 2000)\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (2000, 250)\n",
      "C_dict[e].shape:  torch.Size([2000, 1250])\n",
      "---\n",
      "e_id:  e3\n",
      "X_id_list:  ['X2', 'X5']\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (1000, 20)\n",
      "X_id:  X5\n",
      "X[X_id].shape:  (300, 20)\n",
      "C_dict[e].shape:  torch.Size([20, 1300])\n",
      "---\n",
      "e_id:  e4\n",
      "X_id_list:  ['X3']\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (1000, 150)\n",
      "C_dict[e].shape:  torch.Size([150, 1000])\n",
      "---\n",
      "e_id:  e5\n",
      "X_id_list:  ['X5']\n",
      "X_id:  X5\n",
      "X[X_id].shape:  (300, 20)\n",
      "C_dict[e].shape:  torch.Size([300, 20])\n",
      "---\n",
      "e_id:  e6\n",
      "X_id_list:  ['X4']\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (2000, 250)\n",
      "C_dict[e].shape:  torch.Size([250, 2000])\n",
      "---\n",
      "#\n",
      "concatenated-matrix chunking...\n",
      "WARNING: Entity with ID: e5 has minimum feature size 20 in the setting. The encoding length k 100 is larger than minimim entity feature size.\n",
      "#\n",
      "e_id:  e3 , min_num_datapoints:  20 , num_chunks:  2\n",
      "e_id:  e5 , min_features:  20 , k:  100\n",
      "#\n",
      "e_id:  e1  C_dict[e_id].shape:  torch.Size([1000, 2170])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([500, 2170])\n",
      "---\n",
      "e_id:  e2  C_dict[e_id].shape:  torch.Size([2000, 1250])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([1000, 1250])\n",
      "---\n",
      "e_id:  e3  C_dict[e_id].shape:  torch.Size([20, 1300])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([10, 1300])\n",
      "---\n",
      "e_id:  e4  C_dict[e_id].shape:  torch.Size([150, 1000])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([75, 1000])\n",
      "---\n",
      "e_id:  e5  C_dict[e_id].shape:  torch.Size([300, 20])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([150, 20])\n",
      "---\n",
      "e_id:  e6  C_dict[e_id].shape:  torch.Size([250, 2000])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([125, 2000])\n",
      "---\n",
      "#\n",
      "creating pytorch variables of input matrices...\n",
      "#\n",
      "__input_transformation - end\n",
      "__network_construction - start\n",
      "__network_construction - end\n",
      "dcmf - model construction - end\n",
      "#\n",
      "__pretrain - start\n",
      "pretrain epoch:  1  total loss L:  6.138975143432617  Took  0.5  secs.\n",
      "pretrain epoch:  2  total loss L:  6.138742685317993  Took  0.6  secs.\n",
      "**pretrain converged**\n",
      "__pretrain - end\n",
      "#\n",
      "dcmf.fit - start\n",
      "epoch:  1  total loss L:  10.296629905700684  Took  0.4  secs.\n",
      "epoch:  2  total loss L:  10.22489881515503  Took  1.1  secs.\n",
      "**train converged**\n",
      "Computing AUC.\n",
      "Rpred.shape:  (2000, 1000)\n",
      "Rtest_triplets.shape:  (2, 3)\n",
      "Computing AUC.\n",
      "Rpred.shape:  (20, 1000)\n",
      "Rtest_triplets.shape:  (6, 3)\n",
      "#\n",
      "dcmf.fit - end\n",
      "#\n",
      "dcmf_model.out_dict_info: \n",
      "{'E': 6,\n",
      " 'M': 5,\n",
      " 'loss_all_folds': {'1': [0.6888581216335297,\n",
      "                          0.7211124300956726,\n",
      "                          1.083496868610382,\n",
      "                          0.9612263143062592,\n",
      "                          1.7381648421287537,\n",
      "                          0.9454910457134247,\n",
      "                          0.6343055367469788,\n",
      "                          0.7395466268062592,\n",
      "                          0.7117992043495178,\n",
      "                          0.8434598743915558,\n",
      "                          1.1574382781982422]},\n",
      " 'loss_all_folds_avg_sum': 10.224899142980576,\n",
      " 'loss_all_folds_avg_tuple': [0.6888581216335297,\n",
      "                              0.7211124300956726,\n",
      "                              1.083496868610382,\n",
      "                              0.9612263143062592,\n",
      "                              1.7381648421287537,\n",
      "                              0.9454910457134247,\n",
      "                              0.6343055367469788,\n",
      "                              0.7395466268062592,\n",
      "                              0.7117992043495178,\n",
      "                              0.8434598743915558,\n",
      "                              1.1574382781982422],\n",
      " 'num_val_sets': 1,\n",
      " 'params': {'convg_thres': 0.1,\n",
      "            'd_actf': 'tanh',\n",
      "            'e_actf': 'tanh',\n",
      "            'is_linear_last_dec_layer': False,\n",
      "            'is_linear_last_enc_layer': False,\n",
      "            'is_pretrain': True,\n",
      "            'k': 100,\n",
      "            'kf': 0.5,\n",
      "            'learning_rate': 4.29610376060009e-07,\n",
      "            'max_epochs': 5,\n",
      "            'max_pretrain_epochs': 2,\n",
      "            'num_chunks': 2,\n",
      "            'pretrain_thres': 0.1,\n",
      "            'weight_decay': 0.009241838757959194},\n",
      " 'val_metric': 'auc',\n",
      " 'val_perf_all_folds': {'1': {'X1': 1.0, 'X2': 0.19999999999999996}},\n",
      " 'val_perf_all_folds_avg': {'X1': 1.0, 'X2': 0.19999999999999996},\n",
      " 'val_perf_all_folds_total': {'1': 1.2},\n",
      " 'val_perf_all_folds_total_avg': 1.2}\n",
      "#\n"
     ]
    }
   ],
   "source": [
    "# The ax method that performs the hyperparams optimization\n",
    "# Here we optimize only 2 hyperparameters: learning_rate and weight_decay. \n",
    "# You can add more hyperparams as in the commented section\n",
    "# The optimization is performed with 2 DCMF executions. \n",
    "# You can change this by setting \"total_trials\" as desired\n",
    "# Tip: Use atleast total_trials=50 for finding near optimum of the two hyperparameters\n",
    "\n",
    "best_parameters, values, experiment, model = optimize(\n",
    "    parameters=[\n",
    "        {\n",
    "            \"name\": \"weight_decay\",\n",
    "            \"type\": \"range\",\n",
    "            \"bounds\": [1e-6, 1e-2],\n",
    "            \"value_type\": \"float\",  # Optional, defaults to inference from type of \"bounds\".\n",
    "            \"log_scale\": False,  # Optional, defaults to False.\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"learning_rate\",\n",
    "            \"type\": \"range\",\n",
    "            \"bounds\": [1e-7, 1e-5], #mortality1y\n",
    "            #\"bounds\": [1e-5, 1e-4], #diag\n",
    "            \"value_type\": \"float\",  # Optional, defaults to inference from type of \"bounds\".\n",
    "            \"log_scale\": False,  # Optional, defaults to False.\n",
    "        }\n",
    "        # {\n",
    "        #     \"name\": \"convg_thres\",\n",
    "        #     \"type\": \"range\",\n",
    "        #     \"bounds\": [1e-5, 1e-3], #diag\n",
    "        #     \"value_type\": \"float\",  # Optional, defaults to inference from type of \"bounds\".\n",
    "        #     \"log_scale\": False,  # Optional, defaults to False.\n",
    "        # },\n",
    "        # {\n",
    "        #     \"name\": \"num_layers\",\n",
    "        #     \"type\": \"choice\",\n",
    "        #     #\"values\": [0, 1, 2],\n",
    "        #     \"values\": [2,2],\n",
    "        #     \"value_type\": \"int\"\n",
    "        # },\n",
    "        # {\n",
    "        #     \"name\": \"k\",\n",
    "        #     \"type\": \"choice\",\n",
    "        #     #\"values\": [50, 100, 150, 200],\n",
    "        #     #\"values\": [50,100,200],\n",
    "        #     \"value_type\": \"int\"\n",
    "        # }\n",
    "        # {\n",
    "        #     \"name\": \"actf\",\n",
    "        #     \"type\": \"choice\",\n",
    "        #     \"values\": [\"tanh\", \"sigma\"],\n",
    "        #     \"value_type\": \"str\"\n",
    "        # }\n",
    "        # {\n",
    "        #     \"name\": \"num_layers\",\n",
    "        #     \"type\": \"choice\",\n",
    "        #     \"values\": [1,2],\n",
    "        #     \"value_type\": \"int\"\n",
    "        # }\n",
    "    ],\n",
    "    experiment_name=\"dcmf_bo\",\n",
    "    objective_name=\"loss\",\n",
    "    evaluation_function=run_dcmf,\n",
    "    minimize=True,  # Optional, defaults to False.\n",
    "    #parameter_constraints=[\"k%2 <= 0\"],  # Optional.\n",
    "    #outcome_constraints=[\"loss >= 0\"],  # Optional.\n",
    "    total_trials=2, # Optional.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment.trials: \n",
      "{0: Trial(experiment_name='dcmf_bo', index=0, status=TrialStatus.COMPLETED, arm=Arm(name='0_0', parameters={'weight_decay': 0.009874873130339199, 'learning_rate': 4.171199586614966e-06})),\n",
      " 1: Trial(experiment_name='dcmf_bo', index=1, status=TrialStatus.COMPLETED, arm=Arm(name='1_0', parameters={'weight_decay': 0.009241838757959194, 'learning_rate': 4.29610376060009e-07}))}\n",
      "#\n"
     ]
    }
   ],
   "source": [
    "#Info about all the ax trails\n",
    "print(\"experiment.trials: \")\n",
    "pprint.pprint(experiment.trials)\n",
    "print(\"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_parameters: \n",
      "{'weight_decay': 0.009874873130339199, 'learning_rate': 4.171199586614966e-06}\n",
      "#\n"
     ]
    }
   ],
   "source": [
    "# The best hyper-parameters found using ax\n",
    "print(\"best_parameters: \")\n",
    "print(best_parameters)\n",
    "print(\"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values[0]: \n",
      "{'loss': 8.336682435125113}\n",
      "#\n"
     ]
    }
   ],
   "source": [
    "#The loss corresponding to the best hyper-parameters\n",
    "print(\"values[0]: \")\n",
    "print(values[0])\n",
    "print(\"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obj:  8.3367  params:  {'weight_decay': 0.009874873130339199, 'learning_rate': 4.171199586614966e-06}\n",
      "obj:  10.2249  params:  {'weight_decay': 0.009241838757959194, 'learning_rate': 4.29610376060009e-07}\n",
      "#\n"
     ]
    }
   ],
   "source": [
    "#The loss corresponding to all the hyper-parameters tried\n",
    "for idx in experiment.trials.keys():\n",
    "    trial =  experiment.trials[idx]\n",
    "    print(\"obj: \",round(trial.objective_mean,4),\" params: \",trial.arm.parameters)\n",
    "print(\"#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerunning the DCMF with the best parameters found using the ax framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Instantiating the dCMF model with the best hyper-parameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best hyper-parameters\n",
    "learning_rate = best_parameters[\"learning_rate\"]\n",
    "weight_decay = best_parameters[\"weight_decay\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "#\n",
      "dCMF:\n",
      "---\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  4.171199586614966e-06\n",
      "weight_decay:  0.009874873130339199\n",
      "convg_thres:  0.1\n",
      "max_epochs:  5\n",
      "isPretrain:  True\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  2\n",
      "k:  100\n",
      "kf:  0.5\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  False\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  2\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n"
     ]
    }
   ],
   "source": [
    "dcmf_model = dcmf(G, X_data, X_meta,\\\n",
    "            num_chunks=num_chunks,k=k, kf=kf, e_actf=e_actf, d_actf=d_actf,\\\n",
    "            learning_rate=learning_rate, weight_decay=weight_decay, convg_thres=convg_thres, max_epochs=max_epochs,\\\n",
    "            is_gpu=is_gpu,gpu_ids=gpu_ids,is_pretrain=is_pretrain, pretrain_thres=pretrain_thres,\\\n",
    "            max_pretrain_epochs=max_pretrain_epochs,X_val=X_val,val_metric=val_metric,\\\n",
    "            is_val_transpose=is_val_transpose, at_k=at_k,\\\n",
    "            is_linear_last_enc_layer=is_linear_last_enc_layer,is_linear_last_dec_layer=is_linear_last_dec_layer,num_val_sets=num_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Fitting... *\n",
    "- Performs the input transformation and network construction\n",
    "- (Pre-trains and) trains the model to obtain the entity representations\n",
    "- Reconstruct the input matrices using the entity representations obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## fold_num:  1  ##\n",
      "dcmf_base.__init__ - start\n",
      "dcmf_base.__init__ - end\n",
      "#\n",
      "dCMF: \n",
      "#\n",
      "learning_rate:  4.171199586614966e-06\n",
      "weight_decay:  0.009874873130339199\n",
      "convg_thres:  0.1\n",
      "max_epochs:  5\n",
      "isPretrain:  True\n",
      "pretrain_thres:  0.1\n",
      "max_pretrain_epochs:  2\n",
      "num_chunks:  2\n",
      "k:  100\n",
      "kf:  0.5\n",
      "e_actf:  tanh\n",
      "d_actf:  tanh\n",
      "is_gpu:  False\n",
      "gpu_ids:  1\n",
      "num entities:  6\n",
      "num matrices:  5\n",
      "num_val_sets:  1\n",
      "X_val #matrices:  2\n",
      "val_metric (used only if X_val #matrices > 0):  auc\n",
      "at_k (used only if X_val #matrices > 0 and val_metric is r@k or p@k):  10\n",
      "is_val_transpose:  True\n",
      "is_linear_last_enc_layer:  False\n",
      "is_linear_last_dec_layer:  False\n",
      "#\n",
      "dcmf - model construction - start\n",
      "__input_transformation - start\n",
      "#\n",
      "concatenated-matrix construction...\n",
      "e_id:  e1\n",
      "X_id_list:  ['X1', 'X2', 'X3']\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (1000, 2000)\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (1000, 20)\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (1000, 150)\n",
      "C_dict[e].shape:  torch.Size([1000, 2170])\n",
      "---\n",
      "e_id:  e2\n",
      "X_id_list:  ['X1', 'X4']\n",
      "X_id:  X1\n",
      "X[X_id].shape:  (1000, 2000)\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (2000, 250)\n",
      "C_dict[e].shape:  torch.Size([2000, 1250])\n",
      "---\n",
      "e_id:  e3\n",
      "X_id_list:  ['X2', 'X5']\n",
      "X_id:  X2\n",
      "X[X_id].shape:  (1000, 20)\n",
      "X_id:  X5\n",
      "X[X_id].shape:  (300, 20)\n",
      "C_dict[e].shape:  torch.Size([20, 1300])\n",
      "---\n",
      "e_id:  e4\n",
      "X_id_list:  ['X3']\n",
      "X_id:  X3\n",
      "X[X_id].shape:  (1000, 150)\n",
      "C_dict[e].shape:  torch.Size([150, 1000])\n",
      "---\n",
      "e_id:  e5\n",
      "X_id_list:  ['X5']\n",
      "X_id:  X5\n",
      "X[X_id].shape:  (300, 20)\n",
      "C_dict[e].shape:  torch.Size([300, 20])\n",
      "---\n",
      "e_id:  e6\n",
      "X_id_list:  ['X4']\n",
      "X_id:  X4\n",
      "X[X_id].shape:  (2000, 250)\n",
      "C_dict[e].shape:  torch.Size([250, 2000])\n",
      "---\n",
      "#\n",
      "concatenated-matrix chunking...\n",
      "WARNING: Entity with ID: e5 has minimum feature size 20 in the setting. The encoding length k 100 is larger than minimim entity feature size.\n",
      "#\n",
      "e_id:  e3 , min_num_datapoints:  20 , num_chunks:  2\n",
      "e_id:  e5 , min_features:  20 , k:  100\n",
      "#\n",
      "e_id:  e1  C_dict[e_id].shape:  torch.Size([1000, 2170])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([500, 2170])\n",
      "---\n",
      "e_id:  e2  C_dict[e_id].shape:  torch.Size([2000, 1250])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([1000, 1250])\n",
      "---\n",
      "e_id:  e3  C_dict[e_id].shape:  torch.Size([20, 1300])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([10, 1300])\n",
      "---\n",
      "e_id:  e4  C_dict[e_id].shape:  torch.Size([150, 1000])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([75, 1000])\n",
      "---\n",
      "e_id:  e5  C_dict[e_id].shape:  torch.Size([300, 20])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([150, 20])\n",
      "---\n",
      "e_id:  e6  C_dict[e_id].shape:  torch.Size([250, 2000])\n",
      "C_temp_chunks_list[0].shape:  torch.Size([125, 2000])\n",
      "---\n",
      "#\n",
      "creating pytorch variables of input matrices...\n",
      "#\n",
      "__input_transformation - end\n",
      "__network_construction - start\n",
      "__network_construction - end\n",
      "dcmf - model construction - end\n",
      "#\n",
      "__pretrain - start\n",
      "pretrain epoch:  1  total loss L:  5.741612672805786  Took  0.9  secs.\n",
      "pretrain epoch:  2  total loss L:  5.739584445953369  Took  1.1  secs.\n",
      "**pretrain converged**\n",
      "__pretrain - end\n",
      "#\n",
      "dcmf.fit - start\n",
      "epoch:  1  total loss L:  10.75200891494751  Took  0.4  secs.\n",
      "epoch:  2  total loss L:  10.00381088256836  Took  0.7  secs.\n",
      "epoch:  3  total loss L:  9.356749534606934  Took  0.6  secs.\n",
      "epoch:  4  total loss L:  8.808760166168213  Took  0.5  secs.\n",
      "epoch:  5  total loss L:  8.354618549346924  Took  0.5  secs.\n",
      "Computing AUC.\n",
      "Rpred.shape:  (2000, 1000)\n",
      "Rtest_triplets.shape:  (2, 3)\n",
      "Computing AUC.\n",
      "Rpred.shape:  (20, 1000)\n",
      "Rtest_triplets.shape:  (6, 3)\n",
      "#\n",
      "dcmf.fit - end\n"
     ]
    }
   ],
   "source": [
    "dcmf_model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Result attributes:*\n",
    "- **out_dict_U**:  dict, keys are validation set IDs and values are dict with entity IDs as keys and np.array of entity representations/encodings as values\n",
    "- **out_dict_X_prime**: dict, keys are matrix IDs and values are matrix reconstructions\n",
    "- **out_dict_info**: dict, keys are loss/validation performance attributes and values are corresponding results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['e1', 'e2', 'e3', 'e4', 'e5', 'e6'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcmf_model.out_dict_U['1'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['X1', 'X2', 'X3', 'X4', 'X5'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcmf_model.out_dict_X_prime['1'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'learning_rate': 4.171199586614966e-06,\n",
       "  'weight_decay': 0.009874873130339199,\n",
       "  'convg_thres': 0.1,\n",
       "  'max_epochs': 5,\n",
       "  'is_pretrain': True,\n",
       "  'pretrain_thres': 0.1,\n",
       "  'max_pretrain_epochs': 2,\n",
       "  'num_chunks': 2,\n",
       "  'k': 100,\n",
       "  'kf': 0.5,\n",
       "  'e_actf': 'tanh',\n",
       "  'd_actf': 'tanh',\n",
       "  'is_linear_last_enc_layer': False,\n",
       "  'is_linear_last_dec_layer': False},\n",
       " 'num_val_sets': 1,\n",
       " 'loss_all_folds': {'1': [0.6874975860118866,\n",
       "   0.7197298407554626,\n",
       "   1.0806135535240173,\n",
       "   0.9703676998615265,\n",
       "   1.3286762833595276,\n",
       "   0.9447701871395111,\n",
       "   0.4794623553752899,\n",
       "   0.7553596496582031,\n",
       "   0.7198514938354492,\n",
       "   0.46184852719306946,\n",
       "   0.20644105970859528]},\n",
       " 'loss_all_folds_avg_tuple': [0.6874975860118866,\n",
       "  0.7197298407554626,\n",
       "  1.0806135535240173,\n",
       "  0.9703676998615265,\n",
       "  1.3286762833595276,\n",
       "  0.9447701871395111,\n",
       "  0.4794623553752899,\n",
       "  0.7553596496582031,\n",
       "  0.7198514938354492,\n",
       "  0.46184852719306946,\n",
       "  0.20644105970859528],\n",
       " 'loss_all_folds_avg_sum': 8.354618236422539,\n",
       " 'val_metric': 'auc',\n",
       " 'val_perf_all_folds': {'1': {'X1': 1.0, 'X2': 0.0}},\n",
       " 'val_perf_all_folds_avg': {'X1': 1.0, 'X2': 0.0},\n",
       " 'val_perf_all_folds_total': {'1': 1.0},\n",
       " 'val_perf_all_folds_total_avg': 1.0,\n",
       " 'E': 6,\n",
       " 'M': 5}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcmf_model.out_dict_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
