{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "#\n",
    "import pprint\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import time\n",
    "import itertools\n",
    "import os\n",
    "import pprint\n",
    "#\n",
    "from src.ncmf import ncmf\n",
    "#\n",
    "import os\n",
    "import torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" \n",
    "pp = pprint.PrettyPrinter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NCMF\n",
    "Example of running the \"NCMF\" module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *User inputs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_no = 1\n",
    "data_dir = \"./ncmf_sim_data\"\n",
    "#list_dataset_names = [\"dt1\"]\n",
    "list_dataset_names = ['dt1', 'ds1', 'ds2', 'ds3', 'dn1', 'dn2', 'dn3']\n",
    "#list_dataset_names = [\"ds1\",\"ds2\",\"ds3\"]\n",
    "#list_dataset_names = [\"dn1\",\"dn2\",\"dn3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "Mapping node ids to matrix indices...\n",
      "Splitting training and validation links...\n",
      "0\n",
      "Loading matrices and masks...\n",
      "Warning: Last batch has 200 rows, while other batch sizes are 2048. \n",
      "Warning: Last batch has 500 rows, while other batch sizes are 2048. \n",
      "Warning: Last batch has 200 rows, while other batch sizes are 2048. \n",
      "Warning: Last batch has 300 rows, while other batch sizes are 2048. \n",
      "Warning: Last batch has 200 rows, while other batch sizes are 2048. \n",
      "Warning: Last batch has 400 rows, while other batch sizes are 2048. \n",
      "Warning: Last batch has 700 rows, while other batch sizes are 2048. \n",
      "Warning: Last batch has 500 rows, while other batch sizes are 2048. \n",
      "Warning: Last batch has 600 rows, while other batch sizes are 2048. \n",
      "Warning: Last batch has 300 rows, while other batch sizes are 2048. \n",
      "To reconstruct X0\n",
      "dim:0; e0\n",
      "X0 e0 row\n",
      "X1 e0 row\n",
      "X2 e0 row\n",
      "dim:1; e1\n",
      "X0 e1 col\n",
      "X3 e1 col\n",
      "To reconstruct X1\n",
      "dim:0; e0\n",
      "X0 e0 row\n",
      "X1 e0 row\n",
      "X2 e0 row\n",
      "dim:1; e2\n",
      "X1 e2 col\n",
      "X4 e2 col\n",
      "To reconstruct X2\n",
      "dim:0; e0\n",
      "X0 e0 row\n",
      "X1 e0 row\n",
      "X2 e0 row\n",
      "dim:1; e3\n",
      "X2 e3 col\n",
      "To reconstruct X3\n",
      "dim:0; e4\n",
      "X3 e4 row\n",
      "dim:1; e1\n",
      "X0 e1 col\n",
      "X3 e1 col\n",
      "To reconstruct X4\n",
      "dim:0; e5\n",
      "X4 e5 row\n",
      "dim:1; e2\n",
      "X1 e2 col\n",
      "X4 e2 col\n",
      "Preparing autoencoders' configurations...\n",
      "Preparing reconstructors' configurations...\n",
      "Preparing fusions' configurations...\n",
      "Initialising autoencoders...\n",
      "Initialising reconstructors...\n",
      "NMF hadamard 50\n",
      "Output activation None\n",
      "NMF hadamard 50\n",
      "Output activation None\n",
      "NMF hadamard 50\n",
      "Output activation None\n",
      "NMF hadamard 50\n",
      "Output activation None\n",
      "NMF hadamard 50\n",
      "Output activation None\n",
      "Initialising fusions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ragu/bioinf/ncmf-main/NCMF/sample_data_NCMF_v2/../src/loss.py:40: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /home/ragu/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755853042/work/aten/src/ATen/native/cuda/jit_utils.cpp:860.)\n",
      "  t1 = torch.lgamma(theta + self.eps) + torch.lgamma(y_true +\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m matrix_types \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreal\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX3\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX4\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     15\u001b[0m }\n\u001b[1;32m     16\u001b[0m ncmf_model \u001b[38;5;241m=\u001b[39m ncmf(sample_no, data_dir, dataset_name,\\\n\u001b[1;32m     17\u001b[0m                   matrix_types, num_epochs, learning_rate, \\\n\u001b[1;32m     18\u001b[0m                   weight_decay, convergence_threshold, batch_size, \\\n\u001b[1;32m     19\u001b[0m                   batch_size, entity_matrices, autoencoder_act_f \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m\"\u001b[39m, reconstructor_act_f \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mncmf_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/data/ragu/bioinf/ncmf-main/NCMF/sample_data_NCMF_v2/../src/ncmf.py:87\u001b[0m, in \u001b[0;36mncmf.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcuda_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     86\u001b[0m net \u001b[38;5;241m=\u001b[39m DCMF( graph, meta, entity_dims, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautoencoder_config, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreconstructor_config, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfusion_config, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatrix_types)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 87\u001b[0m net, losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatrix_types\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m entity_embedding \u001b[38;5;241m=\u001b[39m retrieve_embedding( net, embloaders, norm_params, device)\n\u001b[1;32m     89\u001b[0m save_embedding(node_idx_df, entity_embedding, file_path\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_folder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb_file))\n",
      "File \u001b[0;32m/data/ragu/bioinf/ncmf-main/NCMF/sample_data_NCMF_v2/../src/train.py:38\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[0;34m(net, trainloaders, validloaders, embloaders, norm_params, hyperparams, device, writer, matrix_types)\u001b[0m\n\u001b[1;32m     30\u001b[0m net\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     31\u001b[0m beta \u001b[38;5;241m=\u001b[39m anneal_beta(\n\u001b[1;32m     32\u001b[0m     hyperparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_cycles\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     33\u001b[0m     hyperparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproportion\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     hyperparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manneal\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     37\u001b[0m )\n\u001b[0;32m---> 38\u001b[0m train_loss, train_rmse_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loss_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnorm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmatrix_types\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m     53\u001b[0m net\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/data/ragu/bioinf/ncmf-main/NCMF/sample_data_NCMF_v2/../src/train.py:163\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(net, optimizer, scheduler, scaler, loss_func, trainloaders, norm_params, hyperparams, beta, device, matrix_types)\u001b[0m\n\u001b[1;32m    160\u001b[0m     loss \u001b[38;5;241m=\u001b[39m row_loss \u001b[38;5;241m+\u001b[39m col_loss \u001b[38;5;241m+\u001b[39m rec_loss\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# work around for amp + checkpointing\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    166\u001b[0m     rmse_loss \u001b[38;5;241m=\u001b[39m rmse_loss_func(\n\u001b[1;32m    167\u001b[0m         XP_block[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mM_bar\u001b[39m\u001b[38;5;124m'\u001b[39m], X_block, X_block_mask)\n",
      "File \u001b[0;32m~/anaconda3/envs/env_py38/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/env_py38/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for dataset_name in list_dataset_names:\n",
    "    print(\"#\")\n",
    "    ! mkdir -p {data_dir}/{dataset_name}/{sample_no}\n",
    "    #\n",
    "    # Setting hyperparameters\n",
    "    num_epochs = 300\n",
    "    batch_size = 2048\n",
    "    weight_decay = 1e-3\n",
    "    learning_rate = 1e-5\n",
    "    convergence_threshold = -1e-3\n",
    "    entity_matrices = [\"X0\",\"X1\",\"X2\",\"X3\",\"X4\"]\n",
    "    matrix_types = {\n",
    "        \"real\": [\"X0\",\"X1\",\"X3\"],\n",
    "        \"binary\": [\"X2\",\"X4\"]\n",
    "    }\n",
    "    ncmf_model = ncmf(sample_no, data_dir, dataset_name,\\\n",
    "                      matrix_types, num_epochs, learning_rate, \\\n",
    "                      weight_decay, convergence_threshold, batch_size, \\\n",
    "                      batch_size, entity_matrices, autoencoder_act_f = \"tanh\", reconstructor_act_f = \"tanh\",\\\n",
    "                     reconstructor_output_act_f_real = \"tanh\")\n",
    "    ncmf_model.fit()\n",
    "    print(\"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
