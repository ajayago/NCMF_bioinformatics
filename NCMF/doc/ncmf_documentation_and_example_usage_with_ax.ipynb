{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NCMF - AX\n",
    "Example of running the \"ncmf\" module with the best parameters found using the ax framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_no = 1\n",
    "data_dir = f\"../../datasets/NCMF/\"\n",
    "dataset_name = \"MIMIC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ax\n",
    "from ax import RangeParameter, ChoiceParameter, FixedParameter\n",
    "from ax import ParameterType, SearchSpace\n",
    "from ax.service.managed_loop import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import time\n",
    "import itertools\n",
    "import os\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ncmf import ncmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Optimization/training - hyperparamteres*\n",
    "\n",
    "- **learning_rate**: float, Adam optimizer's learning rate\n",
    "- **weight_decay**: float, Adam optimizers's weight decay (L2 penalty)\n",
    "- **max_epochs**: int, maximum number of training epochs at which the training stops \n",
    "- **convg_thres**: float, convergence threshold "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter selection using the ax framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Installation instruction can be found at: [https://ax.dev/](https://ax.dev/)\n",
    "- The example below is based on the following API:\n",
    "[https://ax.dev/tutorials/gpei_hartmann_loop.html](https://ax.dev/tutorials/gpei_hartmann_loop.html)\n",
    "- And here is a high level intro to the library: \n",
    "[https://www.youtube.com/watch?v=2c8YX0E8Qhw](https://www.youtube.com/watch?v=2c8YX0E8Qhw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wrapper method for DCMF to use with the ax framework\n",
    "# Here we perform the hyper parameter optimization based on the training loss\n",
    "# i.e. finding the optimum hyperparams that results in minimum loss\n",
    "\n",
    "def run_ncmf(parameterization):\n",
    "    #hyper-parameters that are selected using ax\n",
    "    learning_rate = 1e-6\n",
    "    convergence_threshold = 1e-3\n",
    "    num_epochs = 10\n",
    "    batch_size = 2048\n",
    "    entity_matrices = ['X0', 'X1', 'X2']\n",
    "    matrix_types = {\n",
    "    \"real\": [\"X0\", \"X1\", \"X2\"],\n",
    "    \"binary\": []\n",
    "    }\n",
    "    weight_decay = parameterization[\"weight_decay\"]\n",
    "    ncmf_model = ncmf(sample_no, data_dir, dataset_name, matrix_types, num_epochs, learning_rate, weight_decay, convergence_threshold, batch_size, batch_size, entity_matrices)\n",
    "    #\n",
    "    ncmf_model.fit()\n",
    "    ncmf_model.evaluate()\n",
    "    print(\"#\")\n",
    "    print(\"ncmf_model.out_dict_info: \")\n",
    "    pp.pprint(ncmf_model.out_dict_info)\n",
    "    print(\"#\")\n",
    "    #\n",
    "    out_dict = {}\n",
    "    out_dict[\"auc\"] = (ncmf_model.out_dict_info[\"auc\"], 0.0)\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 07-21 15:52:15] ax.modelbridge.dispatch_utils: Using Sobol generation strategy.\n",
      "[INFO 07-21 15:52:15] ax.service.managed_loop: Started full optimization with 2 steps.\n",
      "[INFO 07-21 15:52:15] ax.service.managed_loop: Running optimization trial 1...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping node ids to matrix indices...\n",
      "Splitting training and validation links...\n",
      "Loading matrices and masks...\n",
      "Warning: Last batch has 1795 rows, while other batch sizes are 2048. \n",
      "Warning: Last batch has 1321 rows, while other batch sizes are 2048. \n",
      "Warning: Last batch has 1321 rows, while other batch sizes are 2048. \n",
      "Warning: Last batch has 596 rows, while other batch sizes are 2048. \n",
      "Warning: Last batch has 596 rows, while other batch sizes are 2048. \n",
      "Warning: Last batch has 1795 rows, while other batch sizes are 2048. \n",
      "To reconstruct X0\n",
      "dim:0; e0\n",
      "X0 e0 row\n",
      "X2 e0 col\n",
      "dim:1; e1\n",
      "X0 e1 col\n",
      "X1 e1 row\n",
      "To reconstruct X1\n",
      "dim:0; e1\n",
      "X0 e1 col\n",
      "X1 e1 row\n",
      "dim:1; e2\n",
      "X1 e2 col\n",
      "X2 e2 row\n",
      "To reconstruct X2\n",
      "dim:0; e2\n",
      "X1 e2 col\n",
      "X2 e2 row\n",
      "dim:1; e0\n",
      "X0 e0 row\n",
      "X2 e0 col\n",
      "Preparing autoencoders' configurations...\n",
      "Preparing reconstructors' configurations...\n",
      "Preparing fusions' configurations...\n",
      "Initialising autoencoders...\n",
      "Initialising reconstructors...\n",
      "Initialising fusions...\n",
      "Retreive Embedding\n",
      "====> Epoch 0: Average Train Loss: 21.5427955 | Train RMSE: 5.1967854 | Average Valid Loss: 6.1572056 | Valid RMSE: 10.8265608 | beta: 0.0\n",
      "Retreive Embedding\n",
      "====> Epoch 1: Average Train Loss: 21.5145757 | Train RMSE: 5.1414132 | Average Valid Loss: 6.1444920 | Valid RMSE: 10.7673937 | beta: 0.0\n",
      "Retreive Embedding\n",
      "====> Epoch 2: Average Train Loss: 21.4887160 | Train RMSE: 5.0950060 | Average Valid Loss: 6.1317891 | Valid RMSE: 10.7115093 | beta: 0.0\n",
      "Retreive Embedding\n",
      "====> Epoch 3: Average Train Loss: 21.4666458 | Train RMSE: 5.0514712 | Average Valid Loss: 6.1191047 | Valid RMSE: 10.6586923 | beta: 0.0\n",
      "Retreive Embedding\n",
      "====> Epoch 4: Average Train Loss: 21.4413434 | Train RMSE: 5.0098567 | Average Valid Loss: 6.1065567 | Valid RMSE: 10.6085454 | beta: 0.0\n",
      "Retreive Embedding\n",
      "====> Epoch 5: Average Train Loss: 21.4193303 | Train RMSE: 4.9698834 | Average Valid Loss: 6.0941928 | Valid RMSE: 10.5608037 | beta: 0.0\n",
      "Retreive Embedding\n",
      "====> Epoch 6: Average Train Loss: 21.3962625 | Train RMSE: 4.9316306 | Average Valid Loss: 6.0821013 | Valid RMSE: 10.5155260 | beta: 0.0\n",
      "Retreive Embedding\n",
      "====> Epoch 7: Average Train Loss: 21.3715798 | Train RMSE: 4.8949327 | Average Valid Loss: 6.0701518 | Valid RMSE: 10.4722989 | beta: 0.0\n",
      "Retreive Embedding\n",
      "====> Epoch 8: Average Train Loss: 21.3502916 | Train RMSE: 4.8597031 | Average Valid Loss: 6.0584821 | Valid RMSE: 10.4308933 | beta: 0.0\n",
      "Retreive Embedding\n",
      "====> Epoch 9: Average Train Loss: 21.3267066 | Train RMSE: 4.8258033 | Average Valid Loss: 6.0470405 | Valid RMSE: 10.3911749 | beta: 0.0\n",
      "Finished Training\n",
      "\n",
      "Retreive Embedding\n",
      "Reconstruct\n",
      "0/3 | 0/1\n",
      "1/3 | 0/1\n",
      "2/3 | 0/1\n",
      "0/1 | 0/1\n",
      "0/1 | 0/3\n",
      "0/1 | 1/3\n",
      "0/1 | 2/3\n",
      "Start eval\n",
      "Starting evaluation func\n",
      "check1\n",
      "check 2\n",
      "check 3\n",
      "Just before cross val\n",
      "Start Evaluation Fold 0!\n",
      "Start Evaluation Fold 1!\n",
      "Start Evaluation Fold 2!\n",
      "Start Evaluation Fold 3!\n",
      "Start Evaluation Fold 4!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 07-21 15:53:19] ax.service.managed_loop: Running optimization trial 2...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record Results!\n",
      "DCMF++ eval done\n",
      "#\n",
      "ncmf_model.out_dict_info: \n",
      "{'F1': 0.7824033229481305,\n",
      " 'auc': 0.808919435719484,\n",
      " 'mrr': 0.8523750340489284,\n",
      " 'params': {'autoencoder_config': {'activation_function': 'tanh',\n",
      "                                   'hidden_dim': 1024,\n",
      "                                   'k': 50,\n",
      "                                   'k_factor': 0},\n",
      "            'fusion_config': {'activation_function': 'tanh'},\n",
      "            'hyperparameter_config': {'anneal': 'cosine',\n",
      "                                      'convergence_threshold': 0.001,\n",
      "                                      'lamda': 0.001,\n",
      "                                      'learning_rate': 1e-06,\n",
      "                                      'max_norm': 1,\n",
      "                                      'ntrain_neg': 5,\n",
      "                                      'num_cycles': 10,\n",
      "                                      'num_epochs': 10,\n",
      "                                      'nvalid_neg': 5,\n",
      "                                      'pretrain': False,\n",
      "                                      'proportion': 0.8,\n",
      "                                      'train_batch_size': 2048,\n",
      "                                      'valid_batch_size': 2048,\n",
      "                                      'weight_decay': 0.001},\n",
      "            'reconstructor_config': {'activation_function': 'tanh'}},\n",
      " 'precision': 0.8001721433549023,\n",
      " 'recall': 0.7655924292699865}\n",
      "#\n",
      "Mapping node ids to matrix indices...\n",
      "Splitting training and validation links...\n",
      "Loading matrices and masks...\n",
      "Warning: Last batch has 1795 rows, while other batch sizes are 2048. \n",
      "Warning: Last batch has 1321 rows, while other batch sizes are 2048. \n",
      "Warning: Last batch has 1321 rows, while other batch sizes are 2048. \n",
      "Warning: Last batch has 596 rows, while other batch sizes are 2048. \n",
      "Warning: Last batch has 596 rows, while other batch sizes are 2048. \n",
      "Warning: Last batch has 1795 rows, while other batch sizes are 2048. \n",
      "To reconstruct X0\n",
      "dim:0; e0\n",
      "X0 e0 row\n",
      "X2 e0 col\n",
      "dim:1; e1\n",
      "X0 e1 col\n",
      "X1 e1 row\n",
      "To reconstruct X1\n",
      "dim:0; e1\n",
      "X0 e1 col\n",
      "X1 e1 row\n",
      "dim:1; e2\n",
      "X1 e2 col\n",
      "X2 e2 row\n",
      "To reconstruct X2\n",
      "dim:0; e2\n",
      "X1 e2 col\n",
      "X2 e2 row\n",
      "dim:1; e0\n",
      "X0 e0 row\n",
      "X2 e0 col\n",
      "Preparing autoencoders' configurations...\n",
      "Preparing reconstructors' configurations...\n",
      "Preparing fusions' configurations...\n",
      "Initialising autoencoders...\n",
      "Initialising reconstructors...\n",
      "Initialising fusions...\n",
      "Retreive Embedding\n",
      "====> Epoch 0: Average Train Loss: 21.5427951 | Train RMSE: 5.1967897 | Average Valid Loss: 6.1572080 | Valid RMSE: 10.8265613 | beta: 0.0\n",
      "Retreive Embedding\n",
      "====> Epoch 1: Average Train Loss: 21.5145743 | Train RMSE: 5.1414218 | Average Valid Loss: 6.1444849 | Valid RMSE: 10.7673585 | beta: 0.0\n",
      "Retreive Embedding\n",
      "====> Epoch 2: Average Train Loss: 21.4887137 | Train RMSE: 5.0950298 | Average Valid Loss: 6.1317754 | Valid RMSE: 10.7114755 | beta: 0.0\n",
      "Retreive Embedding\n",
      "====> Epoch 3: Average Train Loss: 21.4666495 | Train RMSE: 5.0514784 | Average Valid Loss: 6.1191100 | Valid RMSE: 10.6586978 | beta: 0.0\n",
      "Retreive Embedding\n",
      "====> Epoch 4: Average Train Loss: 21.4413401 | Train RMSE: 5.0098763 | Average Valid Loss: 6.1065392 | Valid RMSE: 10.6084975 | beta: 0.0\n",
      "Retreive Embedding\n",
      "====> Epoch 5: Average Train Loss: 21.4193388 | Train RMSE: 4.9699244 | Average Valid Loss: 6.0942008 | Valid RMSE: 10.5608729 | beta: 0.0\n",
      "Retreive Embedding\n",
      "====> Epoch 6: Average Train Loss: 21.3962545 | Train RMSE: 4.9316421 | Average Valid Loss: 6.0820685 | Valid RMSE: 10.5154428 | beta: 0.0\n",
      "Retreive Embedding\n",
      "====> Epoch 7: Average Train Loss: 21.3715806 | Train RMSE: 4.8949194 | Average Valid Loss: 6.0701612 | Valid RMSE: 10.4723398 | beta: 0.0\n",
      "Retreive Embedding\n",
      "====> Epoch 8: Average Train Loss: 21.3502953 | Train RMSE: 4.8597412 | Average Valid Loss: 6.0584843 | Valid RMSE: 10.4309213 | beta: 0.0\n",
      "Retreive Embedding\n",
      "====> Epoch 9: Average Train Loss: 21.3267101 | Train RMSE: 4.8258109 | Average Valid Loss: 6.0470320 | Valid RMSE: 10.3911871 | beta: 0.0\n",
      "Finished Training\n",
      "\n",
      "Retreive Embedding\n",
      "Reconstruct\n",
      "0/3 | 0/1\n",
      "1/3 | 0/1\n",
      "2/3 | 0/1\n",
      "0/1 | 0/1\n",
      "0/1 | 0/3\n",
      "0/1 | 1/3\n",
      "0/1 | 2/3\n",
      "Start eval\n",
      "Starting evaluation func\n",
      "check1\n",
      "check 2\n",
      "check 3\n",
      "Just before cross val\n",
      "Start Evaluation Fold 0!\n",
      "Start Evaluation Fold 1!\n",
      "Start Evaluation Fold 2!\n",
      "Start Evaluation Fold 3!\n",
      "Start Evaluation Fold 4!\n",
      "Record Results!\n",
      "DCMF++ eval done\n",
      "#\n",
      "ncmf_model.out_dict_info: \n",
      "{'F1': 0.782391107517144,\n",
      " 'auc': 0.8089099894185781,\n",
      " 'mrr': 0.8523750340489284,\n",
      " 'params': {'autoencoder_config': {'activation_function': 'tanh',\n",
      "                                   'hidden_dim': 1024,\n",
      "                                   'k': 50,\n",
      "                                   'k_factor': 0},\n",
      "            'fusion_config': {'activation_function': 'tanh'},\n",
      "            'hyperparameter_config': {'anneal': 'cosine',\n",
      "                                      'convergence_threshold': 0.001,\n",
      "                                      'lamda': 0.001,\n",
      "                                      'learning_rate': 1e-06,\n",
      "                                      'max_norm': 1,\n",
      "                                      'ntrain_neg': 5,\n",
      "                                      'num_cycles': 10,\n",
      "                                      'num_epochs': 10,\n",
      "                                      'nvalid_neg': 5,\n",
      "                                      'pretrain': False,\n",
      "                                      'proportion': 0.8,\n",
      "                                      'train_batch_size': 2048,\n",
      "                                      'valid_batch_size': 2048,\n",
      "                                      'weight_decay': 0.5},\n",
      "            'reconstructor_config': {'activation_function': 'tanh'}},\n",
      " 'precision': 0.8001782337452829,\n",
      " 'recall': 0.7655631387509705}\n",
      "#\n"
     ]
    }
   ],
   "source": [
    "# The ax method that performs the hyperparams optimization\n",
    "# Here we optimize only 2 hyperparameters: learning_rate and weight_decay. \n",
    "# You can add more hyperparams as in the commented section\n",
    "# The optimization is performed with 2 DCMF executions. \n",
    "# You can change this by setting \"total_trials\" as desired\n",
    "# Tip: Use atleast total_trials=50 for finding near optimum of the two hyperparameters\n",
    "num_chunks = 2\n",
    "is_gpu = False\n",
    "gpu_ids = \"1\"\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "best_parameters, values, experiment, model = optimize(\n",
    "    parameters=[\n",
    "        {\n",
    "            \"name\": \"weight_decay\",\n",
    "            \"type\": \"choice\",\n",
    "            \"values\": [0.5, 1e-3],\n",
    "            \"value_type\": \"float\"  # Optional, defaults to inference from type of \"bounds\".\n",
    "            #\"log_scale\": False,  # Optional, defaults to False.\n",
    "        }\n",
    "#         {\n",
    "#             \"name\": \"batch_size\",\n",
    "#             \"type\": \"choice\",\n",
    "#             \"values\": [256, 512],\n",
    "#             \"value_type\": \"int\"\n",
    "#         }\n",
    "#         {\n",
    "#             \"name\": \"num_epochs\",\n",
    "#             \"type\": \"choice\",\n",
    "#             \"values\": [1000, 2000],\n",
    "#             \"value_type\": \"int\"\n",
    "#         }\n",
    "#         {\n",
    "#             \"name\": \"learning_rate\",\n",
    "#             \"type\": \"range\",\n",
    "#             \"bounds\": [1e-5, 1e-3], #mortality1y\n",
    "#             #\"bounds\": [1e-5, 1e-4], #diag\n",
    "#             \"value_type\": \"float\",  # Optional, defaults to inference from type of \"bounds\".\n",
    "#             \"log_scale\": False,  # Optional, defaults to False.\n",
    "#         },\n",
    "#         {\n",
    "#             \"name\": \"convg_thres\",\n",
    "#             \"type\": \"range\",\n",
    "#             \"bounds\": [1e-5, 1e-3], #diag\n",
    "#             \"value_type\": \"float\",  # Optional, defaults to inference from type of \"bounds\".\n",
    "#             \"log_scale\": False,  # Optional, defaults to False.\n",
    "#         }\n",
    "        # {\n",
    "        #     \"name\": \"num_layers\",\n",
    "        #     \"type\": \"choice\",\n",
    "        #     #\"values\": [0, 1, 2],\n",
    "        #     \"values\": [2,2],\n",
    "        #     \"value_type\": \"int\"\n",
    "        # },\n",
    "        # {\n",
    "        #     \"name\": \"k\",\n",
    "        #     \"type\": \"choice\",\n",
    "        #     #\"values\": [50, 100, 150, 200],\n",
    "        #     #\"values\": [50,100,200],\n",
    "        #     \"value_type\": \"int\"\n",
    "        # }\n",
    "        # {\n",
    "        #     \"name\": \"actf\",\n",
    "        #     \"type\": \"choice\",\n",
    "        #     \"values\": [\"tanh\", \"sigma\"],\n",
    "        #     \"value_type\": \"str\"\n",
    "        # }\n",
    "        # {\n",
    "        #     \"name\": \"num_layers\",\n",
    "        #     \"type\": \"choice\",\n",
    "        #     \"values\": [1,2],\n",
    "        #     \"value_type\": \"int\"\n",
    "        # }\n",
    "    ],\n",
    "    experiment_name=\"ncmf\",\n",
    "    objective_name=\"auc\",\n",
    "    evaluation_function=run_ncmf,\n",
    "    minimize=False,  # Optional, defaults to False.\n",
    "    #parameter_constraints=[\"k%2 <= 0\"],  # Optional.\n",
    "    #outcome_constraints=[\"loss >= 0\"],  # Optional.\n",
    "    total_trials=2, # Optional.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment.trials: \n",
      "{0: Trial(experiment_name='ncmf', index=0, status=TrialStatus.COMPLETED, arm=Arm(name='0_0', parameters={'weight_decay': 0.001})),\n",
      " 1: Trial(experiment_name='ncmf', index=1, status=TrialStatus.COMPLETED, arm=Arm(name='1_0', parameters={'weight_decay': 0.5}))}\n",
      "#\n"
     ]
    }
   ],
   "source": [
    "#Info about all the ax trails\n",
    "print(\"experiment.trials: \")\n",
    "pprint.pprint(experiment.trials)\n",
    "print(\"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_parameters: \n",
      "{'weight_decay': 0.001}\n",
      "#\n"
     ]
    }
   ],
   "source": [
    "# The best hyper-parameters found using ax\n",
    "print(\"best_parameters: \")\n",
    "print(best_parameters)\n",
    "print(\"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values[0]: \n",
      "{'auc': 0.808919435719484}\n",
      "#\n"
     ]
    }
   ],
   "source": [
    "#The loss corresponding to the best hyper-parameters\n",
    "print(\"values[0]: \")\n",
    "print(values[0])\n",
    "print(\"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obj:  0.8089  params:  {'weight_decay': 0.001}\n",
      "obj:  0.8089  params:  {'weight_decay': 0.5}\n",
      "#\n"
     ]
    }
   ],
   "source": [
    "#The loss corresponding to all the hyper-parameters tried\n",
    "for idx in experiment.trials.keys():\n",
    "    trial =  experiment.trials[idx]\n",
    "    print(\"obj: \",round(trial.objective_mean,4),\" params: \",trial.arm.parameters)\n",
    "print(\"#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerunning NCMF with the best parameters found using the ax framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Instantiating the NCMF model with the best hyper-parameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best hyper-parameters\n",
    "weight_decay = best_parameters[\"weight_decay\"]\n",
    "learning_rate = 1e-6\n",
    "convergence_threshold = 1e-3\n",
    "num_epochs = 10\n",
    "batch_size = 2048\n",
    "entity_matrices = ['X0', 'X1', 'X2']\n",
    "matrix_types = {\n",
    "    \"real\": [\"X0\", \"X1\", \"X2\"],\n",
    "    \"binary\": []\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncmf_model = ncmf(sample_no, data_dir, dataset_name, matrix_types, num_epochs, learning_rate, weight_decay, convergence_threshold, batch_size, batch_size, entity_matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Fitting... *\n",
    "- Performs the input transformation and network construction\n",
    "- (Pre-trains and) trains the model to obtain the entity representations\n",
    "- Reconstruct the input matrices using the entity representations obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping node ids to matrix indices...\n",
      "Splitting training and validation links...\n",
      "Loading matrices and masks...\n",
      "Warning: Last batch has 1795 rows, while other batch sizes are 2048. \n",
      "Warning: Last batch has 1321 rows, while other batch sizes are 2048. \n",
      "Warning: Last batch has 1321 rows, while other batch sizes are 2048. \n",
      "Warning: Last batch has 596 rows, while other batch sizes are 2048. \n",
      "Warning: Last batch has 596 rows, while other batch sizes are 2048. \n",
      "Warning: Last batch has 1795 rows, while other batch sizes are 2048. \n",
      "To reconstruct X0\n",
      "dim:0; e0\n",
      "X0 e0 row\n",
      "X2 e0 col\n",
      "dim:1; e1\n",
      "X0 e1 col\n",
      "X1 e1 row\n",
      "To reconstruct X1\n",
      "dim:0; e1\n",
      "X0 e1 col\n",
      "X1 e1 row\n",
      "dim:1; e2\n",
      "X1 e2 col\n",
      "X2 e2 row\n",
      "To reconstruct X2\n",
      "dim:0; e2\n",
      "X1 e2 col\n",
      "X2 e2 row\n",
      "dim:1; e0\n",
      "X0 e0 row\n",
      "X2 e0 col\n",
      "Preparing autoencoders' configurations...\n",
      "Preparing reconstructors' configurations...\n",
      "Preparing fusions' configurations...\n",
      "Initialising autoencoders...\n",
      "Initialising reconstructors...\n",
      "Initialising fusions...\n",
      "Retreive Embedding\n",
      "====> Epoch 0: Average Train Loss: 21.5427955 | Train RMSE: 5.1967864 | Average Valid Loss: 6.1572085 | Valid RMSE: 10.8265661 | beta: 0.0\n",
      "Retreive Embedding\n",
      "====> Epoch 1: Average Train Loss: 21.5145743 | Train RMSE: 5.1414151 | Average Valid Loss: 6.1444913 | Valid RMSE: 10.7673998 | beta: 0.0\n",
      "Retreive Embedding\n",
      "====> Epoch 2: Average Train Loss: 21.4887170 | Train RMSE: 5.0950103 | Average Valid Loss: 6.1317898 | Valid RMSE: 10.7115088 | beta: 0.0\n",
      "Retreive Embedding\n",
      "====> Epoch 3: Average Train Loss: 21.4666453 | Train RMSE: 5.0514712 | Average Valid Loss: 6.1191036 | Valid RMSE: 10.6586866 | beta: 0.0\n",
      "Retreive Embedding\n",
      "====> Epoch 4: Average Train Loss: 21.4413432 | Train RMSE: 5.0098572 | Average Valid Loss: 6.1065546 | Valid RMSE: 10.6085340 | beta: 0.0\n",
      "Retreive Embedding\n",
      "====> Epoch 5: Average Train Loss: 21.4193296 | Train RMSE: 4.9698811 | Average Valid Loss: 6.0941902 | Valid RMSE: 10.5607894 | beta: 0.0\n",
      "Retreive Embedding\n",
      "====> Epoch 6: Average Train Loss: 21.3962628 | Train RMSE: 4.9316273 | Average Valid Loss: 6.0821040 | Valid RMSE: 10.5155110 | beta: 0.0\n",
      "Retreive Embedding\n",
      "====> Epoch 7: Average Train Loss: 21.3715818 | Train RMSE: 4.8949318 | Average Valid Loss: 6.0701533 | Valid RMSE: 10.4722844 | beta: 0.0\n",
      "Retreive Embedding\n",
      "====> Epoch 8: Average Train Loss: 21.3502922 | Train RMSE: 4.8597050 | Average Valid Loss: 6.0584896 | Valid RMSE: 10.4308779 | beta: 0.0\n",
      "Retreive Embedding\n",
      "====> Epoch 9: Average Train Loss: 21.3267071 | Train RMSE: 4.8258018 | Average Valid Loss: 6.0470415 | Valid RMSE: 10.3911520 | beta: 0.0\n",
      "Finished Training\n",
      "\n",
      "Retreive Embedding\n",
      "Reconstruct\n",
      "0/3 | 0/1\n",
      "1/3 | 0/1\n",
      "2/3 | 0/1\n",
      "0/1 | 0/1\n",
      "0/1 | 0/3\n",
      "0/1 | 1/3\n",
      "0/1 | 2/3\n"
     ]
    }
   ],
   "source": [
    "ncmf_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start eval\n",
      "Starting evaluation func\n",
      "check1\n",
      "check 2\n",
      "check 3\n",
      "Just before cross val\n",
      "Start Evaluation Fold 0!\n",
      "Start Evaluation Fold 1!\n",
      "Start Evaluation Fold 2!\n",
      "Start Evaluation Fold 3!\n",
      "Start Evaluation Fold 4!\n",
      "Record Results!\n",
      "DCMF++ eval done\n"
     ]
    }
   ],
   "source": [
    "ncmf_model.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Result attributes:*\n",
    "- **out_dict_info**: dict, keys are loss/validation performance attributes and values are corresponding results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'hyperparameter_config': {'num_epochs': 10,\n",
       "   'learning_rate': 1e-06,\n",
       "   'weight_decay': 0.001,\n",
       "   'convergence_threshold': 0.001,\n",
       "   'train_batch_size': 2048,\n",
       "   'valid_batch_size': 2048,\n",
       "   'pretrain': False,\n",
       "   'max_norm': 1,\n",
       "   'lamda': 0.001,\n",
       "   'anneal': 'cosine',\n",
       "   'num_cycles': 10,\n",
       "   'proportion': 0.8,\n",
       "   'ntrain_neg': 5,\n",
       "   'nvalid_neg': 5},\n",
       "  'autoencoder_config': {'k': 50,\n",
       "   'k_factor': 0,\n",
       "   'hidden_dim': 1024,\n",
       "   'activation_function': 'tanh'},\n",
       "  'reconstructor_config': {'activation_function': 'tanh'},\n",
       "  'fusion_config': {'activation_function': 'tanh'}},\n",
       " 'auc': 0.8089306144330044,\n",
       " 'mrr': 0.8523750340489284,\n",
       " 'recall': 0.7655927201753346,\n",
       " 'precision': 0.8001961415667695,\n",
       " 'F1': 0.7824153092173912}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncmf_model.out_dict_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
